<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>automl_alex.base API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>automl_alex.base</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pandas as pd
import numpy as np
import sys
import time
import optuna
from tqdm import tqdm
import joblib

from .logger import *
from .automl_alex import BestSingleModel
import automl_alex
from .optimizer import *


import sklearn
from sklearn.model_selection import RepeatedKFold, RepeatedStratifiedKFold

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style(style=&#34;darkgrid&#34;)

# disable chained assignments
pd.options.mode.chained_assignment = None 

predict_proba_metrics = [&#39;roc_auc_score&#39;, &#39;log_loss&#39;, &#39;brier_score_loss&#39;]

class ModelBase(object):
    &#34;&#34;&#34;
    Base class for a specific ML algorithm implementation factory,
    i.e. it defines algorithm-specific hyperparameter space and generic methods for model training &amp; inference
    &#34;&#34;&#34;
    pbar = 0
    model = None
    study = None
    history_trials = []
    history_trials_dataframe = pd.DataFrame()
    best_model_param = None

    def __init__(self,  
                    model_param=None, 
                    type_of_estimator=None, # classifier or regression
                    gpu=False, 
                    verbose=0,
                    random_state=42
                    ):
        self._gpu = gpu
        self._random_state = random_state
        logger_print_lvl(verbose)

        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        self.model_param = self._init_default_model_param()
        if model_param is not None:
            self.model_param = self.model_param.update(model_param)
    

    def _init_default_model_param(self,):
        &#34;&#34;&#34;
        Default model_param
        &#34;&#34;&#34;
        model_param = {}
        return(model_param)


    def fit(self, X_train=None, y_train=None, X_test=None, y_test=None, verbose=False):
        &#34;&#34;&#34;
        Args:
            X (pd.DataFrame, shape (n_samples, n_features)): the input data
            y (pd.DataFrame, shape (n_samples, ) or (n_samples, n_outputs)): the target data
        Return:
            model (Class)
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def predict(self, X=None):
        &#34;&#34;&#34;
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def is_possible_predict_proba(self):
        &#34;&#34;&#34;
        Return:
            bool, whether model can predict proba
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def predict_proba(self, X):
        &#34;&#34;&#34;
        Args:
            dataset (np.array, shape (n_samples, n_features)): the input data

        Return:
            np.array, shape (n_samples, n_classes): predicted probabilities
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def predict_or_predict_proba(self, X):
        &#34;&#34;&#34;
        Ð¡heck and if it is possible get predict_proba
        &#34;&#34;&#34;
        if (self.is_possible_predict_proba()) and \
                (self._type_of_estimator == &#39;classifier&#39;):
            predicts = self.predict_proba(X)
        else:
            predicts = self.predict(X)
        return(predicts)


    def _is_possible_feature_importance(self):
        &#34;&#34;&#34;
        Return:
            bool, whether model can predict proba
        &#34;&#34;&#34;
        return False


    def get_feature_importance(self, train_x, importance_type=&#39;gain&#39;,):
        &#34;&#34;&#34;
        Return:
            list feature_importance
        &#34;&#34;&#34;
        if not self._is_possible_feature_importance(): 
            raise Exception(&#34;Model cannot get feature_importance&#34;)
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    @logger.catch
    def score(self, 
            X_test, 
            y_test,
            metric=None,
            print_metric=False, 
            metric_round=4, 
            ):
        if self.model is None:
            raise Exception(&#34;No fit models&#34;)

        if metric is None:
            if self._type_of_estimator == &#39;classifier&#39;:
                metric = sklearn.metrics.roc_auc_score
            elif self._type_of_estimator == &#39;regression&#39;:
                metric = sklearn.metrics.mean_squared_error

        # Predict
        if (metric.__name__ in predict_proba_metrics):
            y_pred_test = self.predict_or_predict_proba(X_test)
        else:
            y_pred_test = self.predict(X_test)
        score = round(metric(y_test, y_pred_test),metric_round)

        if print_metric:
            logger_print_lvl(3)
            logger.info(f&#39;{metric.__name__}: {score}&#39;)
        return(score)


    @logger.catch
    def fit_score(self, 
            X_train, 
            y_train, 
            X_test, 
            y_test,
            metric=None,
            print_metric=False, 
            metric_round=4, 
            ):
        start = time.time()
        # Fit
        self.fit(X_train, y_train,)

        total_time_fit = round((time.time() - start),2)
        if print_metric:
            logger_print_lvl(3)
            logger.info(f&#39;fit time: {total_time_fit} sec&#39;)

        # Score
        score = self.score(X_test, y_test, 
            metric=metric,
            print_metric=print_metric, 
            metric_round=metric_round,
            )
        return(score)


    def y_format(self, y):
        if isinstance(y, pd.DataFrame):
            y = np.array(y[y.columns[0]].values)
        return y


    def get_model_opt_params(self, ):
        &#34;&#34;&#34;
        Return:
            dict from parameter name to hyperopt distribution: default
            parameter space
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def __metric_direction_detected__(self, metric, y):
        zero_y = np.zeros(len(y))
        zero_score = metric(y, zero_y)
        best_score = metric(y, y)

        if best_score &gt; zero_score:
            direction = &#39;maximize&#39;
        else:
            direction = &#39;minimize&#39;
        return(direction)



    def opt(self,X,y,
            timeout=200, # optimization time in seconds
            metric=None,
            metric_round=4,
            combined_score_opt=False,
            cold_start=30,
            auto_parameters=True,
            folds=7,
            score_folds=2,
            opt_lvl=2,
            early_stoping=100,
            verbose=1,):
        &#34;&#34;&#34;
        Description of opt:
        in progress... 

        Args:
            timeout=100 (int):
            folds=None (None or int):
            cold_start=None (None or int):
            score_cv_folds=None (None or int):
            opt_lvl=None (None or int):
            direction=None (None or str):
            early_stoping=100 (int):
            feature_selection=True (bool):
            verbose=1 (int):
        
        Returns:
            history_trials (pd.DataFrame)
        &#34;&#34;&#34;

        if metric is not None:
            self.metric = metric
            self.direction = self.__metric_direction_detected__(self.metric, y)
        else:
            if self._type_of_estimator == &#39;classifier&#39;:
                self.metric = sklearn.metrics.roc_auc_score
                self.direction = &#39;maximize&#39;
            elif self._type_of_estimator == &#39;regression&#39;:
                self.metric = sklearn.metrics.mean_squared_error
                self.direction = &#39;minimize&#39;

        logger.info(f&#39;{self._type_of_estimator} optimize: {self.direction}&#39;)

        self.optimizer = BestSingleModel(
            type_of_estimator=self._type_of_estimator,
            models_names = [self.__name__,],
            feature_selection=False,
            auto_parameters=auto_parameters,
            folds=folds,
            score_folds=score_folds,
            metric=self.metric,
            metric_round=metric_round, 
            cold_start=cold_start,
            opt_lvl=opt_lvl,
            early_stoping=early_stoping,
            gpu=self._gpu,
            random_state=self._random_state)

        history = self.optimizer.opt(X, y, 
            timeout, 
            verbose=verbose, 
            )

        self.model_param = self.optimizer.cv_model.estimator.model_param
        self.fit(X,y)
        return(history)


    def plot_opt_param_importances(self,):
        &#39;&#39;&#39;
        Plot hyperparameter importances.
        &#39;&#39;&#39;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_param_importances(self.optimizer.study))


    def plot_opt_history(self,):
        &#39;&#39;&#39;
        Plot optimization history of all trials in a study.
        &#39;&#39;&#39;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_optimization_history(self.optimizer.study))


    def plot_parallel_coordinate(self,):
        &#34;&#34;&#34;
        Plot the high-dimentional parameter relationships in a study.
        Note that, If a parameter contains missing values, a trial with missing values is not plotted.
        &#34;&#34;&#34;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_parallel_coordinate(self.optimizer.study))


    def plot_slice(self, params=None):
        &#34;&#34;&#34;
        Plot the parameter relationship as slice plot in a study.
        Note that, If a parameter contains missing values, a trial with missing values is not plotted.
        &#34;&#34;&#34;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_slice(self.optimizer.study, params=params))

    
    def plot_contour(self, params=None):
        &#34;&#34;&#34;
        Plot the parameter relationship as contour plot in a study.
        Note that, If a parameter contains missing values, a trial with missing values is not plotted.
        &#34;&#34;&#34;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_contour(self.optimizer.study, params=params))


    def _is_model_start_opt_params(self,):
        return(False)
    

    @logger.catch
    def save(self, name, verbose=1):
        joblib.dump(self, name+&#39;.pkl&#39;)
        logger.info(&#39;Save Model&#39;)


    @logger.catch
    def load(self, name,verbose=1):
        model = joblib.load(name+&#39;.pkl&#39;)
        logger.info(&#39;Load Model&#39;)
        return(model)
    


class ModelClassifier(ModelBase):
    _type_of_estimator=&#39;classifier&#39;


class ModelRegressor(ModelBase):
    _type_of_estimator=&#39;regression&#39;



class EarlyStoppingExceeded(optuna.exceptions.OptunaError):
    &#39;&#39;&#39;
    Custom EarlyStop for Optuna
    &#39;&#39;&#39;
    def __init__(self, early_stop=100, best_score = None):
        self.early_stop = early_stop
        self.early_stop_count = 0
        self.best_score = None

    def early_stopping_opt_maximize(self, study, trial):
        if self.best_score is None:
            self.best_score = study.best_value

        if study.best_value &gt; self.best_score:
            self.best_score = study.best_value
            self.early_stop_count = 0
        else:
            if self.early_stop_count &lt; self.early_stop:
                self.early_stop_count=self.early_stop_count+1
            else:
                self.early_stop_count = 0
                self.best_score = None
                raise EarlyStoppingExceeded()
    
    def early_stopping_opt_minimize(self, study, trial):
        if self.best_score is None:
            self.best_score = study.best_value

        if study.best_value &lt; self.best_score:
            self.best_score = study.best_value
            self.early_stop_count = 0
        else:
            if self.early_stop_count &lt; self.early_stop:
                self.early_stop_count=self.early_stop_count+1
            else:
                self.early_stop_count = 0
                self.best_score = None
                raise EarlyStoppingExceeded()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="automl_alex.base.EarlyStoppingExceeded"><code class="flex name class">
<span>class <span class="ident">EarlyStoppingExceeded</span></span>
<span>(</span><span>early_stop=100, best_score=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom EarlyStop for Optuna</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EarlyStoppingExceeded(optuna.exceptions.OptunaError):
    &#39;&#39;&#39;
    Custom EarlyStop for Optuna
    &#39;&#39;&#39;
    def __init__(self, early_stop=100, best_score = None):
        self.early_stop = early_stop
        self.early_stop_count = 0
        self.best_score = None

    def early_stopping_opt_maximize(self, study, trial):
        if self.best_score is None:
            self.best_score = study.best_value

        if study.best_value &gt; self.best_score:
            self.best_score = study.best_value
            self.early_stop_count = 0
        else:
            if self.early_stop_count &lt; self.early_stop:
                self.early_stop_count=self.early_stop_count+1
            else:
                self.early_stop_count = 0
                self.best_score = None
                raise EarlyStoppingExceeded()
    
    def early_stopping_opt_minimize(self, study, trial):
        if self.best_score is None:
            self.best_score = study.best_value

        if study.best_value &lt; self.best_score:
            self.best_score = study.best_value
            self.early_stop_count = 0
        else:
            if self.early_stop_count &lt; self.early_stop:
                self.early_stop_count=self.early_stop_count+1
            else:
                self.early_stop_count = 0
                self.best_score = None
                raise EarlyStoppingExceeded()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>optuna.exceptions.OptunaError</li>
<li>builtins.Exception</li>
<li>builtins.BaseException</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.base.EarlyStoppingExceeded.early_stopping_opt_maximize"><code class="name flex">
<span>def <span class="ident">early_stopping_opt_maximize</span></span>(<span>self, study, trial)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def early_stopping_opt_maximize(self, study, trial):
    if self.best_score is None:
        self.best_score = study.best_value

    if study.best_value &gt; self.best_score:
        self.best_score = study.best_value
        self.early_stop_count = 0
    else:
        if self.early_stop_count &lt; self.early_stop:
            self.early_stop_count=self.early_stop_count+1
        else:
            self.early_stop_count = 0
            self.best_score = None
            raise EarlyStoppingExceeded()</code></pre>
</details>
</dd>
<dt id="automl_alex.base.EarlyStoppingExceeded.early_stopping_opt_minimize"><code class="name flex">
<span>def <span class="ident">early_stopping_opt_minimize</span></span>(<span>self, study, trial)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def early_stopping_opt_minimize(self, study, trial):
    if self.best_score is None:
        self.best_score = study.best_value

    if study.best_value &lt; self.best_score:
        self.best_score = study.best_value
        self.early_stop_count = 0
    else:
        if self.early_stop_count &lt; self.early_stop:
            self.early_stop_count=self.early_stop_count+1
        else:
            self.early_stop_count = 0
            self.best_score = None
            raise EarlyStoppingExceeded()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.base.ModelBase"><code class="flex name class">
<span>class <span class="ident">ModelBase</span></span>
<span>(</span><span>model_param=None, type_of_estimator=None, gpu=False, verbose=0, random_state=42)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for a specific ML algorithm implementation factory,
i.e. it defines algorithm-specific hyperparameter space and generic methods for model training &amp; inference</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelBase(object):
    &#34;&#34;&#34;
    Base class for a specific ML algorithm implementation factory,
    i.e. it defines algorithm-specific hyperparameter space and generic methods for model training &amp; inference
    &#34;&#34;&#34;
    pbar = 0
    model = None
    study = None
    history_trials = []
    history_trials_dataframe = pd.DataFrame()
    best_model_param = None

    def __init__(self,  
                    model_param=None, 
                    type_of_estimator=None, # classifier or regression
                    gpu=False, 
                    verbose=0,
                    random_state=42
                    ):
        self._gpu = gpu
        self._random_state = random_state
        logger_print_lvl(verbose)

        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        self.model_param = self._init_default_model_param()
        if model_param is not None:
            self.model_param = self.model_param.update(model_param)
    

    def _init_default_model_param(self,):
        &#34;&#34;&#34;
        Default model_param
        &#34;&#34;&#34;
        model_param = {}
        return(model_param)


    def fit(self, X_train=None, y_train=None, X_test=None, y_test=None, verbose=False):
        &#34;&#34;&#34;
        Args:
            X (pd.DataFrame, shape (n_samples, n_features)): the input data
            y (pd.DataFrame, shape (n_samples, ) or (n_samples, n_outputs)): the target data
        Return:
            model (Class)
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def predict(self, X=None):
        &#34;&#34;&#34;
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def is_possible_predict_proba(self):
        &#34;&#34;&#34;
        Return:
            bool, whether model can predict proba
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def predict_proba(self, X):
        &#34;&#34;&#34;
        Args:
            dataset (np.array, shape (n_samples, n_features)): the input data

        Return:
            np.array, shape (n_samples, n_classes): predicted probabilities
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def predict_or_predict_proba(self, X):
        &#34;&#34;&#34;
        Ð¡heck and if it is possible get predict_proba
        &#34;&#34;&#34;
        if (self.is_possible_predict_proba()) and \
                (self._type_of_estimator == &#39;classifier&#39;):
            predicts = self.predict_proba(X)
        else:
            predicts = self.predict(X)
        return(predicts)


    def _is_possible_feature_importance(self):
        &#34;&#34;&#34;
        Return:
            bool, whether model can predict proba
        &#34;&#34;&#34;
        return False


    def get_feature_importance(self, train_x, importance_type=&#39;gain&#39;,):
        &#34;&#34;&#34;
        Return:
            list feature_importance
        &#34;&#34;&#34;
        if not self._is_possible_feature_importance(): 
            raise Exception(&#34;Model cannot get feature_importance&#34;)
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    @logger.catch
    def score(self, 
            X_test, 
            y_test,
            metric=None,
            print_metric=False, 
            metric_round=4, 
            ):
        if self.model is None:
            raise Exception(&#34;No fit models&#34;)

        if metric is None:
            if self._type_of_estimator == &#39;classifier&#39;:
                metric = sklearn.metrics.roc_auc_score
            elif self._type_of_estimator == &#39;regression&#39;:
                metric = sklearn.metrics.mean_squared_error

        # Predict
        if (metric.__name__ in predict_proba_metrics):
            y_pred_test = self.predict_or_predict_proba(X_test)
        else:
            y_pred_test = self.predict(X_test)
        score = round(metric(y_test, y_pred_test),metric_round)

        if print_metric:
            logger_print_lvl(3)
            logger.info(f&#39;{metric.__name__}: {score}&#39;)
        return(score)


    @logger.catch
    def fit_score(self, 
            X_train, 
            y_train, 
            X_test, 
            y_test,
            metric=None,
            print_metric=False, 
            metric_round=4, 
            ):
        start = time.time()
        # Fit
        self.fit(X_train, y_train,)

        total_time_fit = round((time.time() - start),2)
        if print_metric:
            logger_print_lvl(3)
            logger.info(f&#39;fit time: {total_time_fit} sec&#39;)

        # Score
        score = self.score(X_test, y_test, 
            metric=metric,
            print_metric=print_metric, 
            metric_round=metric_round,
            )
        return(score)


    def y_format(self, y):
        if isinstance(y, pd.DataFrame):
            y = np.array(y[y.columns[0]].values)
        return y


    def get_model_opt_params(self, ):
        &#34;&#34;&#34;
        Return:
            dict from parameter name to hyperopt distribution: default
            parameter space
        &#34;&#34;&#34;
        raise NotImplementedError(&#34;Pure virtual class.&#34;)


    def __metric_direction_detected__(self, metric, y):
        zero_y = np.zeros(len(y))
        zero_score = metric(y, zero_y)
        best_score = metric(y, y)

        if best_score &gt; zero_score:
            direction = &#39;maximize&#39;
        else:
            direction = &#39;minimize&#39;
        return(direction)



    def opt(self,X,y,
            timeout=200, # optimization time in seconds
            metric=None,
            metric_round=4,
            combined_score_opt=False,
            cold_start=30,
            auto_parameters=True,
            folds=7,
            score_folds=2,
            opt_lvl=2,
            early_stoping=100,
            verbose=1,):
        &#34;&#34;&#34;
        Description of opt:
        in progress... 

        Args:
            timeout=100 (int):
            folds=None (None or int):
            cold_start=None (None or int):
            score_cv_folds=None (None or int):
            opt_lvl=None (None or int):
            direction=None (None or str):
            early_stoping=100 (int):
            feature_selection=True (bool):
            verbose=1 (int):
        
        Returns:
            history_trials (pd.DataFrame)
        &#34;&#34;&#34;

        if metric is not None:
            self.metric = metric
            self.direction = self.__metric_direction_detected__(self.metric, y)
        else:
            if self._type_of_estimator == &#39;classifier&#39;:
                self.metric = sklearn.metrics.roc_auc_score
                self.direction = &#39;maximize&#39;
            elif self._type_of_estimator == &#39;regression&#39;:
                self.metric = sklearn.metrics.mean_squared_error
                self.direction = &#39;minimize&#39;

        logger.info(f&#39;{self._type_of_estimator} optimize: {self.direction}&#39;)

        self.optimizer = BestSingleModel(
            type_of_estimator=self._type_of_estimator,
            models_names = [self.__name__,],
            feature_selection=False,
            auto_parameters=auto_parameters,
            folds=folds,
            score_folds=score_folds,
            metric=self.metric,
            metric_round=metric_round, 
            cold_start=cold_start,
            opt_lvl=opt_lvl,
            early_stoping=early_stoping,
            gpu=self._gpu,
            random_state=self._random_state)

        history = self.optimizer.opt(X, y, 
            timeout, 
            verbose=verbose, 
            )

        self.model_param = self.optimizer.cv_model.estimator.model_param
        self.fit(X,y)
        return(history)


    def plot_opt_param_importances(self,):
        &#39;&#39;&#39;
        Plot hyperparameter importances.
        &#39;&#39;&#39;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_param_importances(self.optimizer.study))


    def plot_opt_history(self,):
        &#39;&#39;&#39;
        Plot optimization history of all trials in a study.
        &#39;&#39;&#39;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_optimization_history(self.optimizer.study))


    def plot_parallel_coordinate(self,):
        &#34;&#34;&#34;
        Plot the high-dimentional parameter relationships in a study.
        Note that, If a parameter contains missing values, a trial with missing values is not plotted.
        &#34;&#34;&#34;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_parallel_coordinate(self.optimizer.study))


    def plot_slice(self, params=None):
        &#34;&#34;&#34;
        Plot the parameter relationship as slice plot in a study.
        Note that, If a parameter contains missing values, a trial with missing values is not plotted.
        &#34;&#34;&#34;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_slice(self.optimizer.study, params=params))

    
    def plot_contour(self, params=None):
        &#34;&#34;&#34;
        Plot the parameter relationship as contour plot in a study.
        Note that, If a parameter contains missing values, a trial with missing values is not plotted.
        &#34;&#34;&#34;
        if self.optimizer.study is None:
            raise Exception(&#39;No history to visualize!&#39;)
        return(optuna.visualization.plot_contour(self.optimizer.study, params=params))


    def _is_model_start_opt_params(self,):
        return(False)
    

    @logger.catch
    def save(self, name, verbose=1):
        joblib.dump(self, name+&#39;.pkl&#39;)
        logger.info(&#39;Save Model&#39;)


    @logger.catch
    def load(self, name,verbose=1):
        model = joblib.load(name+&#39;.pkl&#39;)
        logger.info(&#39;Load Model&#39;)
        return(model)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="automl_alex.base.ModelClassifier" href="#automl_alex.base.ModelClassifier">ModelClassifier</a></li>
<li><a title="automl_alex.base.ModelRegressor" href="#automl_alex.base.ModelRegressor">ModelRegressor</a></li>
<li><a title="automl_alex.models.model_catboost.CatBoost" href="models/model_catboost.html#automl_alex.models.model_catboost.CatBoost">CatBoost</a></li>
<li><a title="automl_alex.models.model_lightgbm.LightGBM" href="models/model_lightgbm.html#automl_alex.models.model_lightgbm.LightGBM">LightGBM</a></li>
<li><a title="automl_alex.models.model_xgboost.XGBoost" href="models/model_xgboost.html#automl_alex.models.model_xgboost.XGBoost">XGBoost</a></li>
<li><a title="automl_alex.models.sklearn_models.LinearModel" href="models/sklearn_models.html#automl_alex.models.sklearn_models.LinearModel">LinearModel</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="automl_alex.base.ModelBase.best_model_param"><code class="name">var <span class="ident">best_model_param</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="automl_alex.base.ModelBase.history_trials"><code class="name">var <span class="ident">history_trials</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="automl_alex.base.ModelBase.history_trials_dataframe"><code class="name">var <span class="ident">history_trials_dataframe</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="automl_alex.base.ModelBase.model"><code class="name">var <span class="ident">model</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="automl_alex.base.ModelBase.pbar"><code class="name">var <span class="ident">pbar</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="automl_alex.base.ModelBase.study"><code class="name">var <span class="ident">study</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.base.ModelBase.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X_train=None, y_train=None, X_test=None, y_test=None, verbose=False)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<p>X (pd.DataFrame, shape (n_samples, n_features)): the input data
y (pd.DataFrame, shape (n_samples, ) or (n_samples, n_outputs)): the target data</p>
<h2 id="return">Return</h2>
<p>model (Class)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, X_train=None, y_train=None, X_test=None, y_test=None, verbose=False):
    &#34;&#34;&#34;
    Args:
        X (pd.DataFrame, shape (n_samples, n_features)): the input data
        y (pd.DataFrame, shape (n_samples, ) or (n_samples, n_outputs)): the target data
    Return:
        model (Class)
    &#34;&#34;&#34;
    raise NotImplementedError(&#34;Pure virtual class.&#34;)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.fit_score"><code class="name flex">
<span>def <span class="ident">fit_score</span></span>(<span>self, X_train, y_train, X_test, y_test, metric=None, print_metric=False, metric_round=4)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit_score(self, 
        X_train, 
        y_train, 
        X_test, 
        y_test,
        metric=None,
        print_metric=False, 
        metric_round=4, 
        ):
    start = time.time()
    # Fit
    self.fit(X_train, y_train,)

    total_time_fit = round((time.time() - start),2)
    if print_metric:
        logger_print_lvl(3)
        logger.info(f&#39;fit time: {total_time_fit} sec&#39;)

    # Score
    score = self.score(X_test, y_test, 
        metric=metric,
        print_metric=print_metric, 
        metric_round=metric_round,
        )
    return(score)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.get_feature_importance"><code class="name flex">
<span>def <span class="ident">get_feature_importance</span></span>(<span>self, train_x, importance_type='gain')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="return">Return</h2>
<p>list feature_importance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_feature_importance(self, train_x, importance_type=&#39;gain&#39;,):
    &#34;&#34;&#34;
    Return:
        list feature_importance
    &#34;&#34;&#34;
    if not self._is_possible_feature_importance(): 
        raise Exception(&#34;Model cannot get feature_importance&#34;)
    raise NotImplementedError(&#34;Pure virtual class.&#34;)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.get_model_opt_params"><code class="name flex">
<span>def <span class="ident">get_model_opt_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="return">Return</h2>
<p>dict from parameter name to hyperopt distribution: default
parameter space</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_opt_params(self, ):
    &#34;&#34;&#34;
    Return:
        dict from parameter name to hyperopt distribution: default
        parameter space
    &#34;&#34;&#34;
    raise NotImplementedError(&#34;Pure virtual class.&#34;)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.is_possible_predict_proba"><code class="name flex">
<span>def <span class="ident">is_possible_predict_proba</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="return">Return</h2>
<p>bool, whether model can predict proba</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_possible_predict_proba(self):
    &#34;&#34;&#34;
    Return:
        bool, whether model can predict proba
    &#34;&#34;&#34;
    raise NotImplementedError(&#34;Pure virtual class.&#34;)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, name, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def load(self, name,verbose=1):
    model = joblib.load(name+&#39;.pkl&#39;)
    logger.info(&#39;Load Model&#39;)
    return(model)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.opt"><code class="name flex">
<span>def <span class="ident">opt</span></span>(<span>self, X, y, timeout=200, metric=None, metric_round=4, combined_score_opt=False, cold_start=30, auto_parameters=True, folds=7, score_folds=2, opt_lvl=2, early_stoping=100, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Description of opt:
in progress&hellip; </p>
<h2 id="args">Args</h2>
<p>timeout=100 (int):
folds=None (None or int):
cold_start=None (None or int):
score_cv_folds=None (None or int):
opt_lvl=None (None or int):
direction=None (None or str):
early_stoping=100 (int):
feature_selection=True (bool):
verbose=1 (int):</p>
<h2 id="returns">Returns</h2>
<p>history_trials (pd.DataFrame)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def opt(self,X,y,
        timeout=200, # optimization time in seconds
        metric=None,
        metric_round=4,
        combined_score_opt=False,
        cold_start=30,
        auto_parameters=True,
        folds=7,
        score_folds=2,
        opt_lvl=2,
        early_stoping=100,
        verbose=1,):
    &#34;&#34;&#34;
    Description of opt:
    in progress... 

    Args:
        timeout=100 (int):
        folds=None (None or int):
        cold_start=None (None or int):
        score_cv_folds=None (None or int):
        opt_lvl=None (None or int):
        direction=None (None or str):
        early_stoping=100 (int):
        feature_selection=True (bool):
        verbose=1 (int):
    
    Returns:
        history_trials (pd.DataFrame)
    &#34;&#34;&#34;

    if metric is not None:
        self.metric = metric
        self.direction = self.__metric_direction_detected__(self.metric, y)
    else:
        if self._type_of_estimator == &#39;classifier&#39;:
            self.metric = sklearn.metrics.roc_auc_score
            self.direction = &#39;maximize&#39;
        elif self._type_of_estimator == &#39;regression&#39;:
            self.metric = sklearn.metrics.mean_squared_error
            self.direction = &#39;minimize&#39;

    logger.info(f&#39;{self._type_of_estimator} optimize: {self.direction}&#39;)

    self.optimizer = BestSingleModel(
        type_of_estimator=self._type_of_estimator,
        models_names = [self.__name__,],
        feature_selection=False,
        auto_parameters=auto_parameters,
        folds=folds,
        score_folds=score_folds,
        metric=self.metric,
        metric_round=metric_round, 
        cold_start=cold_start,
        opt_lvl=opt_lvl,
        early_stoping=early_stoping,
        gpu=self._gpu,
        random_state=self._random_state)

    history = self.optimizer.opt(X, y, 
        timeout, 
        verbose=verbose, 
        )

    self.model_param = self.optimizer.cv_model.estimator.model_param
    self.fit(X,y)
    return(history)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.plot_contour"><code class="name flex">
<span>def <span class="ident">plot_contour</span></span>(<span>self, params=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the parameter relationship as contour plot in a study.
Note that, If a parameter contains missing values, a trial with missing values is not plotted.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_contour(self, params=None):
    &#34;&#34;&#34;
    Plot the parameter relationship as contour plot in a study.
    Note that, If a parameter contains missing values, a trial with missing values is not plotted.
    &#34;&#34;&#34;
    if self.optimizer.study is None:
        raise Exception(&#39;No history to visualize!&#39;)
    return(optuna.visualization.plot_contour(self.optimizer.study, params=params))</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.plot_opt_history"><code class="name flex">
<span>def <span class="ident">plot_opt_history</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot optimization history of all trials in a study.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_opt_history(self,):
    &#39;&#39;&#39;
    Plot optimization history of all trials in a study.
    &#39;&#39;&#39;
    if self.optimizer.study is None:
        raise Exception(&#39;No history to visualize!&#39;)
    return(optuna.visualization.plot_optimization_history(self.optimizer.study))</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.plot_opt_param_importances"><code class="name flex">
<span>def <span class="ident">plot_opt_param_importances</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot hyperparameter importances.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_opt_param_importances(self,):
    &#39;&#39;&#39;
    Plot hyperparameter importances.
    &#39;&#39;&#39;
    if self.optimizer.study is None:
        raise Exception(&#39;No history to visualize!&#39;)
    return(optuna.visualization.plot_param_importances(self.optimizer.study))</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.plot_parallel_coordinate"><code class="name flex">
<span>def <span class="ident">plot_parallel_coordinate</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the high-dimentional parameter relationships in a study.
Note that, If a parameter contains missing values, a trial with missing values is not plotted.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_parallel_coordinate(self,):
    &#34;&#34;&#34;
    Plot the high-dimentional parameter relationships in a study.
    Note that, If a parameter contains missing values, a trial with missing values is not plotted.
    &#34;&#34;&#34;
    if self.optimizer.study is None:
        raise Exception(&#39;No history to visualize!&#39;)
    return(optuna.visualization.plot_parallel_coordinate(self.optimizer.study))</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.plot_slice"><code class="name flex">
<span>def <span class="ident">plot_slice</span></span>(<span>self, params=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the parameter relationship as slice plot in a study.
Note that, If a parameter contains missing values, a trial with missing values is not plotted.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_slice(self, params=None):
    &#34;&#34;&#34;
    Plot the parameter relationship as slice plot in a study.
    Note that, If a parameter contains missing values, a trial with missing values is not plotted.
    &#34;&#34;&#34;
    if self.optimizer.study is None:
        raise Exception(&#39;No history to visualize!&#39;)
    return(optuna.visualization.plot_slice(self.optimizer.study, params=params))</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X=None)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<p>X (np.array, shape (n_samples, n_features)): the input data</p>
<h2 id="return">Return</h2>
<p>np.array, shape (n_samples, n_classes)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict(self, X=None):
    &#34;&#34;&#34;
    Args:
        X (np.array, shape (n_samples, n_features)): the input data
    Return:
        np.array, shape (n_samples, n_classes)
    &#34;&#34;&#34;
    raise NotImplementedError(&#34;Pure virtual class.&#34;)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.predict_or_predict_proba"><code class="name flex">
<span>def <span class="ident">predict_or_predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Ð¡heck and if it is possible get predict_proba</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_or_predict_proba(self, X):
    &#34;&#34;&#34;
    Ð¡heck and if it is possible get predict_proba
    &#34;&#34;&#34;
    if (self.is_possible_predict_proba()) and \
            (self._type_of_estimator == &#39;classifier&#39;):
        predicts = self.predict_proba(X)
    else:
        predicts = self.predict(X)
    return(predicts)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.predict_proba"><code class="name flex">
<span>def <span class="ident">predict_proba</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<p>dataset (np.array, shape (n_samples, n_features)): the input data</p>
<h2 id="return">Return</h2>
<p>np.array, shape (n_samples, n_classes): predicted probabilities</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_proba(self, X):
    &#34;&#34;&#34;
    Args:
        dataset (np.array, shape (n_samples, n_features)): the input data

    Return:
        np.array, shape (n_samples, n_classes): predicted probabilities
    &#34;&#34;&#34;
    raise NotImplementedError(&#34;Pure virtual class.&#34;)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, name, verbose=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def save(self, name, verbose=1):
    joblib.dump(self, name+&#39;.pkl&#39;)
    logger.info(&#39;Save Model&#39;)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.score"><code class="name flex">
<span>def <span class="ident">score</span></span>(<span>self, X_test, y_test, metric=None, print_metric=False, metric_round=4)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def score(self, 
        X_test, 
        y_test,
        metric=None,
        print_metric=False, 
        metric_round=4, 
        ):
    if self.model is None:
        raise Exception(&#34;No fit models&#34;)

    if metric is None:
        if self._type_of_estimator == &#39;classifier&#39;:
            metric = sklearn.metrics.roc_auc_score
        elif self._type_of_estimator == &#39;regression&#39;:
            metric = sklearn.metrics.mean_squared_error

    # Predict
    if (metric.__name__ in predict_proba_metrics):
        y_pred_test = self.predict_or_predict_proba(X_test)
    else:
        y_pred_test = self.predict(X_test)
    score = round(metric(y_test, y_pred_test),metric_round)

    if print_metric:
        logger_print_lvl(3)
        logger.info(f&#39;{metric.__name__}: {score}&#39;)
    return(score)</code></pre>
</details>
</dd>
<dt id="automl_alex.base.ModelBase.y_format"><code class="name flex">
<span>def <span class="ident">y_format</span></span>(<span>self, y)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def y_format(self, y):
    if isinstance(y, pd.DataFrame):
        y = np.array(y[y.columns[0]].values)
    return y</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.base.ModelClassifier"><code class="flex name class">
<span>class <span class="ident">ModelClassifier</span></span>
<span>(</span><span>model_param=None, type_of_estimator=None, gpu=False, verbose=0, random_state=42)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for a specific ML algorithm implementation factory,
i.e. it defines algorithm-specific hyperparameter space and generic methods for model training &amp; inference</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelClassifier(ModelBase):
    _type_of_estimator=&#39;classifier&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.base.ModelBase" href="#automl_alex.base.ModelBase">ModelBase</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.base.ModelBase" href="#automl_alex.base.ModelBase">ModelBase</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.base.ModelBase.fit" href="#automl_alex.base.ModelBase.fit">fit</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.get_feature_importance" href="#automl_alex.base.ModelBase.get_feature_importance">get_feature_importance</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.get_model_opt_params" href="#automl_alex.base.ModelBase.get_model_opt_params">get_model_opt_params</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.is_possible_predict_proba" href="#automl_alex.base.ModelBase.is_possible_predict_proba">is_possible_predict_proba</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.opt" href="#automl_alex.base.ModelBase.opt">opt</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_contour" href="#automl_alex.base.ModelBase.plot_contour">plot_contour</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_opt_history" href="#automl_alex.base.ModelBase.plot_opt_history">plot_opt_history</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_opt_param_importances" href="#automl_alex.base.ModelBase.plot_opt_param_importances">plot_opt_param_importances</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_parallel_coordinate" href="#automl_alex.base.ModelBase.plot_parallel_coordinate">plot_parallel_coordinate</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_slice" href="#automl_alex.base.ModelBase.plot_slice">plot_slice</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict" href="#automl_alex.base.ModelBase.predict">predict</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict_or_predict_proba" href="#automl_alex.base.ModelBase.predict_or_predict_proba">predict_or_predict_proba</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict_proba" href="#automl_alex.base.ModelBase.predict_proba">predict_proba</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="automl_alex.base.ModelRegressor"><code class="flex name class">
<span>class <span class="ident">ModelRegressor</span></span>
<span>(</span><span>model_param=None, type_of_estimator=None, gpu=False, verbose=0, random_state=42)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for a specific ML algorithm implementation factory,
i.e. it defines algorithm-specific hyperparameter space and generic methods for model training &amp; inference</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelRegressor(ModelBase):
    _type_of_estimator=&#39;regression&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.base.ModelBase" href="#automl_alex.base.ModelBase">ModelBase</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.base.ModelBase" href="#automl_alex.base.ModelBase">ModelBase</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.base.ModelBase.fit" href="#automl_alex.base.ModelBase.fit">fit</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.get_feature_importance" href="#automl_alex.base.ModelBase.get_feature_importance">get_feature_importance</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.get_model_opt_params" href="#automl_alex.base.ModelBase.get_model_opt_params">get_model_opt_params</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.is_possible_predict_proba" href="#automl_alex.base.ModelBase.is_possible_predict_proba">is_possible_predict_proba</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.opt" href="#automl_alex.base.ModelBase.opt">opt</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_contour" href="#automl_alex.base.ModelBase.plot_contour">plot_contour</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_opt_history" href="#automl_alex.base.ModelBase.plot_opt_history">plot_opt_history</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_opt_param_importances" href="#automl_alex.base.ModelBase.plot_opt_param_importances">plot_opt_param_importances</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_parallel_coordinate" href="#automl_alex.base.ModelBase.plot_parallel_coordinate">plot_parallel_coordinate</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_slice" href="#automl_alex.base.ModelBase.plot_slice">plot_slice</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict" href="#automl_alex.base.ModelBase.predict">predict</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict_or_predict_proba" href="#automl_alex.base.ModelBase.predict_or_predict_proba">predict_or_predict_proba</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict_proba" href="#automl_alex.base.ModelBase.predict_proba">predict_proba</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="automl_alex" href="index.html">automl_alex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="automl_alex.base.EarlyStoppingExceeded" href="#automl_alex.base.EarlyStoppingExceeded">EarlyStoppingExceeded</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.base.EarlyStoppingExceeded.early_stopping_opt_maximize" href="#automl_alex.base.EarlyStoppingExceeded.early_stopping_opt_maximize">early_stopping_opt_maximize</a></code></li>
<li><code><a title="automl_alex.base.EarlyStoppingExceeded.early_stopping_opt_minimize" href="#automl_alex.base.EarlyStoppingExceeded.early_stopping_opt_minimize">early_stopping_opt_minimize</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.base.ModelBase" href="#automl_alex.base.ModelBase">ModelBase</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.base.ModelBase.best_model_param" href="#automl_alex.base.ModelBase.best_model_param">best_model_param</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.fit" href="#automl_alex.base.ModelBase.fit">fit</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.fit_score" href="#automl_alex.base.ModelBase.fit_score">fit_score</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.get_feature_importance" href="#automl_alex.base.ModelBase.get_feature_importance">get_feature_importance</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.get_model_opt_params" href="#automl_alex.base.ModelBase.get_model_opt_params">get_model_opt_params</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.history_trials" href="#automl_alex.base.ModelBase.history_trials">history_trials</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.history_trials_dataframe" href="#automl_alex.base.ModelBase.history_trials_dataframe">history_trials_dataframe</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.is_possible_predict_proba" href="#automl_alex.base.ModelBase.is_possible_predict_proba">is_possible_predict_proba</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.load" href="#automl_alex.base.ModelBase.load">load</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.model" href="#automl_alex.base.ModelBase.model">model</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.opt" href="#automl_alex.base.ModelBase.opt">opt</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.pbar" href="#automl_alex.base.ModelBase.pbar">pbar</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_contour" href="#automl_alex.base.ModelBase.plot_contour">plot_contour</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_opt_history" href="#automl_alex.base.ModelBase.plot_opt_history">plot_opt_history</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_opt_param_importances" href="#automl_alex.base.ModelBase.plot_opt_param_importances">plot_opt_param_importances</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_parallel_coordinate" href="#automl_alex.base.ModelBase.plot_parallel_coordinate">plot_parallel_coordinate</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.plot_slice" href="#automl_alex.base.ModelBase.plot_slice">plot_slice</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict" href="#automl_alex.base.ModelBase.predict">predict</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict_or_predict_proba" href="#automl_alex.base.ModelBase.predict_or_predict_proba">predict_or_predict_proba</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.predict_proba" href="#automl_alex.base.ModelBase.predict_proba">predict_proba</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.save" href="#automl_alex.base.ModelBase.save">save</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.score" href="#automl_alex.base.ModelBase.score">score</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.study" href="#automl_alex.base.ModelBase.study">study</a></code></li>
<li><code><a title="automl_alex.base.ModelBase.y_format" href="#automl_alex.base.ModelBase.y_format">y_format</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.base.ModelClassifier" href="#automl_alex.base.ModelClassifier">ModelClassifier</a></code></h4>
</li>
<li>
<h4><code><a title="automl_alex.base.ModelRegressor" href="#automl_alex.base.ModelRegressor">ModelRegressor</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>