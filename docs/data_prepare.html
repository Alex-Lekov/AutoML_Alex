<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>automl_alex.data_prepare API documentation</title>
<meta name="description" content="Data processing, cleaning, and encoding" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>automl_alex.data_prepare</code></h1>
</header>
<section id="section-intro">
<p>Data processing, cleaning, and encoding</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Data processing, cleaning, and encoding
&#39;&#39;&#39;

import pandas as pd
import numpy as np
import random
from itertools import combinations
import joblib
import sys
import gc
from pathlib import Path
import shutil

from ._encoders import *
from ._logger import *
from sklearn.preprocessing import StandardScaler

# disable chained assignments
#pd.options.mode.chained_assignment = None 

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)



class CleanNans(object):
    &#34;&#34;&#34;
    Сlass for cleaning Nans
    &#34;&#34;&#34;

    def __init__(self, method=&#39;median&#39;, verbose=0):
        &#34;&#34;&#34;
        Fill Nans and add column, that there were nans in this column
        
        Args:
            method : [&#39;median&#39;, &#39;mean&#39;,]
        &#34;&#34;&#34;
        self.method = method
        self.verbose = verbose


    @logger.catch
    def fit(self, data, cols=None):
        &#34;&#34;&#34;
        Fit fillna.

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            self
        &#34;&#34;&#34;
        if cols is not None:
            data = data[cols]
        
        data = data._get_numeric_data()

        if self.verbose:
            for col in data.columns:
                pct_missing = np.mean(data[col].isnull())
                if pct_missing &gt; 0.25:
                    logger.warning(&#39;! Attention {} - {}% Nans!&#39;.format(col, round(pct_missing*100)))
        
        self.nan_columns = list(data.columns[data.isnull().sum() &gt; 0])
        if not self.nan_columns:     
            logger.info(&#39;No nans features&#39;)

        if self.method == &#39;median&#39;:
            self.fill_value = data.median()
        elif self.method == &#39;mean&#39;:
            self.fill_value = data.mean()
        else:
            raise ValueError(&#39;Wrong fill method&#39;)

        return self


    @logger.catch
    def transform(self, data, cols=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The train dataset with no missing values.
        &#34;&#34;&#34;
        if cols is not None:
            data = data[cols]

        if self.nan_columns:
            for nan_column in self.nan_columns:
                data[nan_column+&#39;_isNAN&#39;] = pd.isna(data[nan_column]).astype(&#39;uint8&#39;)
            
            data.fillna(self.fill_value, inplace=True)
        else:
            raise ValueError(&#39;No nans features&#39;)

        return data


    @logger.catch
    def fit_transform(self, data, cols=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Fit and transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The train dataset with no missing values.
        &#34;&#34;&#34;
        self.fit(data, cols)

        return self.transform(data)


class NumericInteractionFeatures(object):
    &#34;&#34;&#34;
    Сlass for  Numerical interaction generator features: A/B, A*B, A-B,
    &#34;&#34;&#34;
    _cols_combinations = None


    def __init__(self, operations=[&#39;/&#39;,&#39;*&#39;,&#39;-&#39;,&#39;+&#39;], verbose=0):
        &#34;&#34;&#34;
        Fill Nans and add column, that there were nans in this column
        
        Args:
            method : [&#39;median&#39;, &#39;mean&#39;,]
        &#34;&#34;&#34;
        self.operations = operations
        self.verbose = verbose
        self.columns = None


    def fit(self, columns,):
        &#34;&#34;&#34;
        Fit.

        Args:
            columns (list): num columns names
        Returns:
            self
        &#34;&#34;&#34;
        self.columns = columns
        self._cols_combinations = list(combinations(columns,2))
        return self


    def transform(self, df) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transforms the dataset.
        Args:
            df (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                Dataset with new features.
        &#34;&#34;&#34;
        if self._cols_combinations is None:
            raise Exception(&#34;No fit cols_combinations&#34;)

        fe_df = pd.DataFrame()

        for col1 in self.columns:
            for col2 in self.columns:
                if col1 == col2:
                    continue
                else:
                    if &#39;/&#39; in self.operations:
                        fe_df[&#39;{}_/_{}&#39;.format(col1, col2) ] = (df[col1]*1.) / df[col2]
                    if &#39;-&#39; in self.operations:
                        fe_df[&#39;{}_-_{}&#39;.format(col1, col2) ] = df[col1] - df[col2]

        for c in self._cols_combinations:
            if &#39;*&#39; in self.operations:
                fe_df[&#39;{}_*_{}&#39;.format(c[0], c[1]) ] = df[c[0]] * df[c[1]]
            if &#39;+&#39; in self.operations:
                fe_df[&#39;{}_+_{}&#39;.format(c[0], c[1]) ] = df[c[0]] + df[c[1]]
        return(fe_df)


    def fit_transform(self, data, cols) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Fit and transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
        &#34;&#34;&#34;
        self.fit(cols)

        return self.transform(data)


class CleanOutliers(object):
    &#34;&#34;&#34;
    Сlass for detect and remove outliers from your data. 
    I would like to provide two methods solution based on &#34;z score&#34; and solution based on &#34;IQR&#34;.

    Something important when dealing with outliers is that one should try to use estimators as robust as possible. 
    try different values threshold and method
    &#34;&#34;&#34;
    _weight = {}

    def __init__(self, method=&#39;IQR&#39;, threshold=2, verbose=0):
        &#34;&#34;&#34;
        Fill Nans and add column, that there were nans in this column
        
        Args:
            method : [&#39;IQR&#39;, &#39;z_score&#39;,]
        &#34;&#34;&#34;
        self.method = method
        self.threshold = threshold
        self.verbose = verbose


    def _IQR(self, data, col, threshold=1.5):
        &#39;&#39;&#39;
        outlier detection by Interquartile Ranges Rule, also known as Tukey&#39;s test. 
        calculate the IQR ( 75th quantile - 25th quantile) 
        and the 25th 75th quantile. 
        Any value beyond:
            upper bound = 75th quantile + （IQR * threshold）
            lower bound = 25th quantile - （IQR * threshold）   
        are regarded as outliers. Default threshold is 1.5.
        &#39;&#39;&#39;
        
        quantile1,quantile3 = np.percentile(data[col],[25,75])
        iqr_val = quantile3 - quantile1
        lower_bound = quantile1 - (threshold*iqr_val)
        upper_bound = quantile3 + (threshold*iqr_val)
        return(lower_bound, upper_bound)


    def _fit_z_score(self, data, col,):
        &#39;&#39;&#39;
        Z score is an important measurement or score that tells how many Standard deviation above or below a number is from the mean of the dataset
        Any positive Z score means the no. of standard deviation above the mean and a negative score means no. of standard deviation below the mean
        Z score is calculate by subtracting each value with the mean of data and dividing it by standard deviation
        &#39;&#39;&#39;
        median_y = data[col].median()
        median_absolute_deviation_y = (np.abs(data[col]-median_y)).median()
        return(median_y, median_absolute_deviation_y)


    def _get_z_score(self, median, mad, data, col, threshold=3):
        &#39;&#39;&#39;
        Its Modified z_score

        The goal of taking Z-scores is to remove the effects of the location and scale of the data, 
        allowing different datasets to be compared directly. 
        The intuition behind the Z-score method of outlier detection is that, once we’ve centred and rescaled the data, 
        anything that is too far from zero (the threshold is usually a Z-score of 3 or -3) should be considered an outlier.

        The Z-score method relies on the mean and standard deviation of a group of data to measure central tendency and dispersion. 
        This is troublesome, because the mean and standard deviation are highly affected by outliers – they are not robust. 
        In fact, the skewing that outliers bring is one of the biggest reasons for finding and removing outliers from a dataset!

        Another drawback of the Z-score method is that it behaves strangely in small datasets – in fact, the Z-score method will never detect an outlier if the dataset has fewer than 12 items in it. 
        This motivated the development of a modified Z-score method, which does not suffer from the same limitation

        A further benefit of the modified Z-score method is that it uses the median and MAD rather than the mean and standard deviation. 
        The median and MAD are robust measures of central tendency and dispersion, respectively.

        Default threshold is 3.
        &#39;&#39;&#39;
        modified_z_scores = 0.7413 *((data[col] - median)/mad)
        abs_z_scores = np.abs(modified_z_scores)
        filtered_entries = (abs_z_scores &gt; threshold)
        return(filtered_entries)


    @logger.catch
    def fit(self, data, cols=None):
        &#34;&#34;&#34;
        Fit CleanOutliers.

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            self
        &#34;&#34;&#34;
        if cols is not None:
            chek_columns = cols
        else:
            chek_columns = data._get_numeric_data().columns

        self._weight = {}

        for column in chek_columns:
            if self.method == &#39;IQR&#39;:
                lower_bound, upper_bound = self._IQR(data,col=column,threshold=self.threshold)
                self._weight[column] = [lower_bound, upper_bound]
                #logger.info(self._weight)
                if self.verbose:
                    total_outliers = len(data[column][(data[column] &lt; lower_bound) | (data[column] &gt; upper_bound)])

            elif self.method == &#39;z_score&#39;:
                median, mad = self._fit_z_score(data, col=column)
                self._weight[column] = [median, mad]
                if self.verbose:
                    filtered_entries = self._get_z_score(median, mad, data, column, threshold=self.threshold)
                    total_outliers = filtered_entries.sum()
            else:
                raise ValueError(&#39;Wrong method&#39;)

            if self.verbose:
                if total_outliers &gt; 0:
                    logger.info(f&#39;Num of outlier detected: {total_outliers} in Feature {column}&#39;)
                    logger.info(f&#39;Proportion of outlier detected: {round((100/(len(data)/total_outliers)),1)} %&#39;)

        return self


    @logger.catch
    def transform(self, data,) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The dataset.
        &#34;&#34;&#34;
        #logger.info(self._weight)
        for weight_values in self._weight:
            if self.method == &#39;IQR&#39;:
                data.loc[data[weight_values] &lt; self._weight[weight_values][0], weight_values] = self._weight[weight_values][0]
                data.loc[data[weight_values] &gt; self._weight[weight_values][1], weight_values] = self._weight[weight_values][1]

                feature_name = weight_values+&#39;_Is_Outliers_&#39;+self.method
                data[feature_name] = 0
                data.loc[
                    (data[weight_values] &lt; self._weight[weight_values][0]) | (data[weight_values] &gt; self._weight[weight_values][1]), \
                    feature_name
                    ] = 1

            elif self.method == &#39;z_score&#39;:
                filtered_entries = self._get_z_score(
                    self._weight[weight_values][0], 
                    self._weight[weight_values][1], 
                    data, 
                    weight_values, 
                    threshold=self.threshold
                    )
                data.loc[filtered_entries, weight_values] = data[weight_values].median()

                feature_name = weight_values+&#39;_Is_Outliers_&#39;+self.method
                data[feature_name] = 0
                data.loc[filtered_entries, feature_name] = 1

        return data


    def fit_transform(self, data, cols=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Fit and transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The dataset.
        &#34;&#34;&#34;
        self.fit(data, cols)

        return self.transform(data)


class DataPrepare(object):
    &#34;&#34;&#34;
    Сlass for cleaning, encoding and processing your dataset
    &#34;&#34;&#34;
    _clean_outliers_enc = None
    _binary_encoder = None
    _clean_nan_encoder = None
    _cat_clean_ord_encoder = None
    _fit_cat_encoders={}


    def __init__(self, 
                cat_features=None,
                clean_and_encod_data=True,
                cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;],
                clean_nan=True,
                clean_outliers=True,
                outliers_method=&#39;IQR&#39;, # method : [&#39;IQR&#39;, &#39;z_score&#39;,]
                outliers_threshold=2,
                drop_invariant=True,
                num_generator_features=True,
                operations_num_generator=[&#39;/&#39;,&#39;*&#39;,&#39;-&#39;,],
                #group_generator_features=False,
                #frequency_enc_num_features=False,
                normalization=True,
                reduce_memory=False,
                random_state=42,
                verbose=3):
        &#34;&#34;&#34;
        Description of __init__

        Args:
            cat_features=None (list or None): 
            clean_and_encod_data=True (undefined):
            cat_encoder_names=None (list or None):
            clean_nan=True (undefined):
            drop_invariant=True (bool): boolean for whether or not to drop columns with 0 variance.
            num_generator_features=True (undefined):
            random_state=42 (undefined):
        &#34;&#34;&#34;
        self.random_state = random_state
        self.cat_encoder_names = cat_encoder_names

        self.verbose = verbose
        logger_print_lvl(self.verbose)

        self._clean_and_encod_data = clean_and_encod_data
        self._clean_nan = clean_nan
        self._clean_outliers = clean_outliers
        self._outliers_threshold = outliers_threshold
        self._drop_invariant = drop_invariant
        self._outliers_method = outliers_method
        self._num_generator_features = num_generator_features
        self._operations_num_generator = operations_num_generator
        self._normalization = normalization
        self._reduce_memory = reduce_memory
        self.cat_features = cat_features


    def _check_data_format(self, data):
        &#34;&#34;&#34;
        Check that data is not pd.DataFrame or empty

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
        Return:
            True or Exception
        &#34;&#34;&#34;
        if (not isinstance(data, pd.DataFrame)) or data.empty:
            raise Exception(&#34;data is not pd.DataFrame or empty&#34;)


    def _check_num_nans(self, data):
        &#34;&#34;&#34;
        Check Nans in numeric features in data 

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
        Return:
            bool: True or False
        &#34;&#34;&#34;
        data = data._get_numeric_data()
        return(len(list(data.columns[data.isnull().sum() &gt; 0])) &gt; 0)


    def auto_detect_cat_features(self, data):
        &#34;&#34;&#34;
        Description of _auto_detect_cat_features:
            Auto-detection categorical_features by simple rule:
            categorical feature == if feature nunique low 1% of data

        Args:
            data (pd.DataFrame): dataset
            
        Returns:
            cat_features (list): columns names cat features
        
        &#34;&#34;&#34;
        #object_features = list(data.columns[data.dtypes == &#39;object&#39;])
        cat_features = data.columns[(data.nunique(dropna=False) &lt; len(data)//100) &amp; \
            (data.nunique(dropna=False) &gt;2)]
        if len(cat_features) &lt; 1:
            cat_features = None
        #cat_features = list(set([*object_features, *cat_features]))
        return(cat_features)


    @logger.catch
    def fit_transform(self, data, verbose=None):
        &#34;&#34;&#34;
        Fit and transforms the dataset.

        Args:
            data (pd.DataFrame, shape = (n_samples, n_features)): 
                the input data
        Returns:
            data (pd.Dataframe, shape = (n_train, n_features)):
                The dataset with clean numerical and encoded categorical features.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose =  verbose
        logger_print_lvl(self.verbose)
        ########### check_data_format ######################
        self._check_data_format(data)

        start_columns = len(data.columns)
        logger.info(f&#39;Source data shape: {data.shape}&#39;)
        logger.info(&#39;#&#39;*50)
        logger.info(&#39;! START preprocessing Data&#39;)

        data = data.reset_index(drop=True)
        data.replace([np.inf, -np.inf], np.nan, inplace=True)

        ########### Drop invariant  features ######################
        if self._drop_invariant:
            self._drop_invariant_features = \
                data.columns[data.nunique(dropna=False) &lt; 2]
            if len(self._drop_invariant_features) &gt; 0:
                data.drop(self._drop_invariant_features, axis=1, inplace=True)

        ########### Detect type of features ######################

        if self.cat_features is None:
            self.cat_features = self.auto_detect_cat_features(data)
            if self.cat_features is not None:
                logger.info(f&#39;- Auto detect cat features: {len(self.cat_features)}&#39;)

        self.binary_features = data.columns[data.nunique(dropna=False) &lt;= 2]
        self.num_features = list(set(data.select_dtypes(&#39;number&#39;).columns) - set(self.binary_features))
        self.object_features = list(set(data.columns[(data.dtypes == &#39;object&#39;) | (data.dtypes == &#39;category&#39;)]) - set(self.binary_features))


        ########### Binary Features ######################
        if len(self.binary_features) &gt; 0:
            logger.info(&#39;&gt; Binary Features&#39;)

            self._binary_encoder = OrdinalEncoder()
            self._binary_encoder = self._binary_encoder.fit(data[self.binary_features])
            data[self.binary_features] = self._binary_encoder.transform(data[self.binary_features]).replace(2,0)
            

        ########### Categorical Features ######################
        #if self.cat_features is not None:
        # Clean Categorical Features
        if self.object_features is not None:
            logger.info(&#39;&gt; Clean Categorical Features&#39;)
            self._cat_clean_ord_encoder = OrdinalEncoder()
            self._cat_clean_ord_encoder = self._cat_clean_ord_encoder.fit(data[self.object_features])
            data[self.object_features] = self._cat_clean_ord_encoder.transform(data[self.object_features])


        if self.cat_features is not None:
            # Encode Categorical Features
            logger.info(&#39;&gt; Transform Categorical Features.&#39;)

            for cat_encoder_name in self.cat_encoder_names:

                if cat_encoder_name not in cat_encoders_names.keys():
                    raise Exception(f&#34;{cat_encoder_name} not support!&#34;)

                self._fit_cat_encoders[cat_encoder_name] = cat_encoders_names[cat_encoder_name](cols=self.cat_features, drop_invariant=True)
                if cat_encoder_name == &#39;HashingEncoder&#39;:
                    self._fit_cat_encoders[cat_encoder_name] = cat_encoders_names[cat_encoder_name](
                            n_components=int(np.log(len(data.columns))*1000), 
                            drop_invariant=True)
                
                self._fit_cat_encoders[cat_encoder_name] = \
                    self._fit_cat_encoders[cat_encoder_name].fit(data[self.cat_features])

                data_encodet = self._fit_cat_encoders[cat_encoder_name].transform(data[self.cat_features])
                data_encodet = data_encodet.add_prefix(cat_encoder_name + &#39;_&#39;)
                if self._reduce_memory: 
                    data_encodet = reduce_mem_usage(data_encodet)
                logger.info(f&#39; - Encoder: {cat_encoder_name} ADD features: {len(data_encodet.columns)}&#39;)
                data = data.join(data_encodet.reset_index(drop=True))

        ########### Numerical Features ######################
        # CleanOutliers
        if self._clean_outliers:
            logger.info(&#39;&gt; CleanOutliers&#39;,)
            self._clean_outliers_enc = CleanOutliers(
                threshold=self._outliers_threshold, 
                method=self._outliers_method,
                verbose=self.verbose)
            self._clean_outliers_enc = self._clean_outliers_enc.fit(data, cols=self.num_features)
            data = self._clean_outliers_enc.transform(data)

        # CleanNans
        if self._clean_nan:
            if self._check_num_nans(data):
                self._clean_nan_encoder = CleanNans(verbose=self.verbose)
                self._clean_nan_encoder = self._clean_nan_encoder.fit(data[self.num_features])
                data = self._clean_nan_encoder.transform(data)
                logger.info(f&#39;&gt; CleanNans, total nans columns: {len(self._clean_nan_encoder.nan_columns)}&#39;)
            else:
                logger.info(&#39;  No nans features&#39;)

        # Generator interaction Num Features
        if self._num_generator_features:
            if len(self.num_features) &gt; 1:
                logger.info(&#39;&gt; Generate interaction Num Features&#39;)
                self.num_generator = NumericInteractionFeatures(operations=self._operations_num_generator,)
                self.num_generator = self.num_generator.fit(list(self.num_features))
                fe_df = self.num_generator.transform(data[self.num_features])
                
                if self._reduce_memory:
                    fe_df = reduce_mem_usage(fe_df)
                data = pd.concat([
                        data.reset_index(drop=True), 
                        fe_df.reset_index(drop=True)], 
                        axis=1,)
                #data = data.join(fe_df.reset_index(drop=True))
                logger.info(f&#39; ADD features: {fe_df.shape[1]}&#39;)
        
        data.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.data_median_dict = data.median()
        data.fillna(self.data_median_dict, inplace=True)

        ########### Normalization ######################
        if self._normalization:
            logger.info(&#39;&gt; Normalization Features&#39;)
            self.normalization_features = data.columns[data.nunique(dropna=False) &gt; 2].values
            self.scaler = StandardScaler().fit(data[self.normalization_features])
            data_tmp = self.scaler.transform(data[self.normalization_features])
            data_tmp = pd.DataFrame(data_tmp, columns=self.normalization_features)
            data.drop(self.normalization_features, axis=1, inplace=True)
            data = pd.concat([
                        data.reset_index(drop=True), 
                        data_tmp.reset_index(drop=True)], 
                        axis=1,)
            #data[self.normalization_features] = data_tmp[self.normalization_features]
            data_tmp = None

        ########### reduce_mem_usage ######################
        if self._reduce_memory:
            logger.info(&#39;&gt; Reduce_Memory&#39;)
            data = reduce_mem_usage(data, verbose=self.verbose)
        data.fillna(0, inplace=True)

        ########### Final ######################
        end_columns = len(data.columns)
        logger.info(&#39;#&#39;*50)
        logger.info(f&#39;Final data shape: {data.shape}&#39;)
        logger.info(f&#39;Total ADD columns: {end_columns-start_columns}&#39;)
        logger.info(&#39;#&#39;*50)
        return data


    @logger.catch
    def transform(self, data, verbose=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transform dataset.
        Args:
            data (pd.DataFrame, shape = (n_samples, n_features)): 
                the input data
        Returns:
            data (pd.Dataframe, shape = (n_train, n_features)):
                The dataset with clean numerical and encoded categorical features.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose

        logger_print_lvl(self.verbose)

        ########### check_data_format ######################
        self._check_data_format(data)

        start_columns = len(data.columns)
        logger.info(&#39;#&#39;*50)
        logger.info(&#39;! Start Transform Data&#39;)

        data = data.reset_index(drop=True)
        data.replace([np.inf, -np.inf], np.nan, inplace=True)

        ########### Drop invariant  features ######################
        if self._drop_invariant:
            if len(self._drop_invariant_features) &gt; 0:
                data.drop(self._drop_invariant_features, axis=1, inplace=True)

        ########### Binary Features ######################
        
        if self._binary_encoder:
            data[self.binary_features] = self._binary_encoder.transform(data[self.binary_features]).replace(2,0)
            logger.info(&#39;&gt; Clean Binary Features&#39;)

        ########### Categorical Features ######################
        # Clean Categorical Features
        if self.object_features is not None:
            logger.info(&#39;&gt; Clean Categorical Features&#39;)
            data[self.object_features] = self._cat_clean_ord_encoder.transform(data[self.object_features])
        
        if self.cat_features is not None:
            # Encode Categorical Features
            logger.info(&#39;&gt; Transform Categorical Features.&#39;)
            for cat_encoder_name in self.cat_encoder_names:
                data_encodet = self._fit_cat_encoders[cat_encoder_name].transform(data[self.cat_features])
                data_encodet = data_encodet.add_prefix(cat_encoder_name + &#39;_&#39;)
                if self._reduce_memory:
                    data_encodet = reduce_mem_usage(data_encodet)
                if self.verbose &gt; 0:
                    logger.info(f&#39; - Encoder: {cat_encoder_name} ADD features: {len(data_encodet.columns)}&#39;)
                data = data.join(data_encodet.reset_index(drop=True))

        ########### Numerical Features ######################
        # CleanOutliers
        if self._clean_outliers:
            data = self._clean_outliers_enc.transform(data)

        # CleanNans
        if self._clean_nan_encoder:
            data = self._clean_nan_encoder.transform(data)
            logger.info(&#39;&gt; Clean Nans&#39;)

        # Generator interaction Num Features
        if self._num_generator_features:
            if len(self.num_features) &gt; 1:
                logger.info(&#39;&gt; Generate interaction Num Features&#39;)
                fe_df = self.num_generator.transform(data[self.num_features])
                
                if self._reduce_memory:
                    fe_df = reduce_mem_usage(fe_df)
                data = pd.concat([
                        data.reset_index(drop=True), 
                        fe_df.reset_index(drop=True)
                        ], axis=1,)
                #data = data.join(fe_df.reset_index(drop=True))
                logger.info(f&#39; ADD features: {fe_df.shape[1]}&#39;)
        
        data.replace([np.inf, -np.inf], np.nan, inplace=True)
        data.fillna(self.data_median_dict, inplace=True)

        ########### Normalization ######################
        if self._normalization:
            logger.info(&#39;&gt; Normalization Features&#39;)
            data_tmp = self.scaler.transform(data[self.normalization_features])
            data_tmp = pd.DataFrame(data_tmp, columns=self.normalization_features)
            data.drop(self.normalization_features,axis=1,inplace=True)
            data = pd.concat([
                        data.reset_index(drop=True), 
                        data_tmp.reset_index(drop=True)], 
                        axis=1,)
            #data[self.normalization_features] = data_tmp[self.normalization_features]
            data_tmp=None

        ########### reduce_mem_usage ######################
        if self._reduce_memory:
            logger.info(&#39;&gt; Reduce_Memory&#39;)
            data = reduce_mem_usage(data, verbose=self.verbose)
        data.fillna(0, inplace=True)

        ########### Final ######################

        end_columns = len(data.columns)
        logger.info(&#39;#&#39;*50)
        logger.info(f&#39;Final data shape: {data.shape}&#39;)
        logger.info(f&#39;Total ADD columns: {end_columns-start_columns}&#39;)
        logger.info(&#39;#&#39;*50)
        return data


    @logger.catch
    def save(self, name=&#39;DataPrepare_dump&#39;, folder=&#39;./&#39;):
        dir_tmp = &#34;./DataPrepare_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        for cat_encoder_name in self.cat_encoder_names:
            joblib.dump(self._fit_cat_encoders[cat_encoder_name], \
                dir_tmp+cat_encoder_name+&#39;.pkl&#39;)

        joblib.dump(self, dir_tmp+&#39;DataPrepare&#39;+&#39;.pkl&#39;)

        shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)

        shutil.rmtree(dir_tmp)
        logger.info(&#39;Save DataPrepare&#39;)


    @logger.catch
    def load(self, name=&#39;DataPrepare_dump&#39;, folder=&#39;./&#39;):
        dir_tmp = &#34;./DataPrepare_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)

        shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)

        de = joblib.load(dir_tmp+&#39;DataPrepare&#39;+&#39;.pkl&#39;)

        for cat_encoder_name in de.cat_encoder_names:
            de._fit_cat_encoders[cat_encoder_name] = joblib.load(dir_tmp+cat_encoder_name+&#39;.pkl&#39;)

        shutil.rmtree(dir_tmp)
        logger.info(&#39;Load DataPrepare&#39;)
        return(de)


@logger.catch
def reduce_mem_usage(df, verbose=0):
    &#34;&#34;&#34; iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    &#34;&#34;&#34;
    if verbose &gt; 0:
        start_mem = df.memory_usage().sum() / 1024**2
        logger.info(&#39;Memory usage of dataframe is {:.2f} MB&#39;.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if (str(col_type)[:3] == &#39;int&#39;) or (str(col_type)[:5] == &#39;float&#39;):
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == &#39;int&#39;:
                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                #elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:
                #    df[col] = df[col].astype(np.int16)
                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                #if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max:
                #    df[col] = df[col].astype(np.float16)
                if c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)

    if verbose &gt; 0:
        end_mem = df.memory_usage().sum() / 1024**2
        logger.info(&#39;Memory usage after optimization is: {:.2f} MB&#39;.format(end_mem))
        logger.info(&#39;Decreased by {:.1f}%&#39;.format(100 * (start_mem - end_mem) / start_mem))
    
    return df</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="automl_alex.data_prepare.reduce_mem_usage"><code class="name flex">
<span>def <span class="ident">reduce_mem_usage</span></span>(<span>df, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>iterate through all the columns of a dataframe and modify the data type
to reduce memory usage.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def reduce_mem_usage(df, verbose=0):
    &#34;&#34;&#34; iterate through all the columns of a dataframe and modify the data type
        to reduce memory usage.        
    &#34;&#34;&#34;
    if verbose &gt; 0:
        start_mem = df.memory_usage().sum() / 1024**2
        logger.info(&#39;Memory usage of dataframe is {:.2f} MB&#39;.format(start_mem))
    
    for col in df.columns:
        col_type = df[col].dtype
        
        if (str(col_type)[:3] == &#39;int&#39;) or (str(col_type)[:5] == &#39;float&#39;):
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == &#39;int&#39;:
                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                #elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:
                #    df[col] = df[col].astype(np.int16)
                elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                #if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max:
                #    df[col] = df[col].astype(np.float16)
                if c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)

    if verbose &gt; 0:
        end_mem = df.memory_usage().sum() / 1024**2
        logger.info(&#39;Memory usage after optimization is: {:.2f} MB&#39;.format(end_mem))
        logger.info(&#39;Decreased by {:.1f}%&#39;.format(100 * (start_mem - end_mem) / start_mem))
    
    return df</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="automl_alex.data_prepare.CleanNans"><code class="flex name class">
<span>class <span class="ident">CleanNans</span></span>
<span>(</span><span>method='median', verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Сlass for cleaning Nans</p>
<p>Fill Nans and add column, that there were nans in this column</p>
<h2 id="args">Args</h2>
<p>method : ['median', 'mean',]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CleanNans(object):
    &#34;&#34;&#34;
    Сlass for cleaning Nans
    &#34;&#34;&#34;

    def __init__(self, method=&#39;median&#39;, verbose=0):
        &#34;&#34;&#34;
        Fill Nans and add column, that there were nans in this column
        
        Args:
            method : [&#39;median&#39;, &#39;mean&#39;,]
        &#34;&#34;&#34;
        self.method = method
        self.verbose = verbose


    @logger.catch
    def fit(self, data, cols=None):
        &#34;&#34;&#34;
        Fit fillna.

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            self
        &#34;&#34;&#34;
        if cols is not None:
            data = data[cols]
        
        data = data._get_numeric_data()

        if self.verbose:
            for col in data.columns:
                pct_missing = np.mean(data[col].isnull())
                if pct_missing &gt; 0.25:
                    logger.warning(&#39;! Attention {} - {}% Nans!&#39;.format(col, round(pct_missing*100)))
        
        self.nan_columns = list(data.columns[data.isnull().sum() &gt; 0])
        if not self.nan_columns:     
            logger.info(&#39;No nans features&#39;)

        if self.method == &#39;median&#39;:
            self.fill_value = data.median()
        elif self.method == &#39;mean&#39;:
            self.fill_value = data.mean()
        else:
            raise ValueError(&#39;Wrong fill method&#39;)

        return self


    @logger.catch
    def transform(self, data, cols=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The train dataset with no missing values.
        &#34;&#34;&#34;
        if cols is not None:
            data = data[cols]

        if self.nan_columns:
            for nan_column in self.nan_columns:
                data[nan_column+&#39;_isNAN&#39;] = pd.isna(data[nan_column]).astype(&#39;uint8&#39;)
            
            data.fillna(self.fill_value, inplace=True)
        else:
            raise ValueError(&#39;No nans features&#39;)

        return data


    @logger.catch
    def fit_transform(self, data, cols=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Fit and transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The train dataset with no missing values.
        &#34;&#34;&#34;
        self.fit(data, cols)

        return self.transform(data)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.data_prepare.CleanNans.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, data, cols=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit fillna.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit(self, data, cols=None):
    &#34;&#34;&#34;
    Fit fillna.

    Args:
        data (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        self
    &#34;&#34;&#34;
    if cols is not None:
        data = data[cols]
    
    data = data._get_numeric_data()

    if self.verbose:
        for col in data.columns:
            pct_missing = np.mean(data[col].isnull())
            if pct_missing &gt; 0.25:
                logger.warning(&#39;! Attention {} - {}% Nans!&#39;.format(col, round(pct_missing*100)))
    
    self.nan_columns = list(data.columns[data.isnull().sum() &gt; 0])
    if not self.nan_columns:     
        logger.info(&#39;No nans features&#39;)

    if self.method == &#39;median&#39;:
        self.fill_value = data.median()
    elif self.method == &#39;mean&#39;:
        self.fill_value = data.mean()
    else:
        raise ValueError(&#39;Wrong fill method&#39;)

    return self</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.CleanNans.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, data, cols=None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Fit and transforms the dataset.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>pandas.Dataframe of shape = (n_train, n_features)
The train dataset with no missing values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit_transform(self, data, cols=None) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Fit and transforms the dataset.
    Args:
        data (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        pandas.Dataframe of shape = (n_train, n_features)
            The train dataset with no missing values.
    &#34;&#34;&#34;
    self.fit(data, cols)

    return self.transform(data)</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.CleanNans.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, data, cols=None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms the dataset.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>pandas.Dataframe of shape = (n_train, n_features)
The train dataset with no missing values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def transform(self, data, cols=None) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Transforms the dataset.
    Args:
        data (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        pandas.Dataframe of shape = (n_train, n_features)
            The train dataset with no missing values.
    &#34;&#34;&#34;
    if cols is not None:
        data = data[cols]

    if self.nan_columns:
        for nan_column in self.nan_columns:
            data[nan_column+&#39;_isNAN&#39;] = pd.isna(data[nan_column]).astype(&#39;uint8&#39;)
        
        data.fillna(self.fill_value, inplace=True)
    else:
        raise ValueError(&#39;No nans features&#39;)

    return data</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.data_prepare.CleanOutliers"><code class="flex name class">
<span>class <span class="ident">CleanOutliers</span></span>
<span>(</span><span>method='IQR', threshold=2, verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Сlass for detect and remove outliers from your data.
I would like to provide two methods solution based on "z score" and solution based on "IQR".</p>
<p>Something important when dealing with outliers is that one should try to use estimators as robust as possible.
try different values threshold and method</p>
<p>Fill Nans and add column, that there were nans in this column</p>
<h2 id="args">Args</h2>
<p>method : ['IQR', 'z_score',]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CleanOutliers(object):
    &#34;&#34;&#34;
    Сlass for detect and remove outliers from your data. 
    I would like to provide two methods solution based on &#34;z score&#34; and solution based on &#34;IQR&#34;.

    Something important when dealing with outliers is that one should try to use estimators as robust as possible. 
    try different values threshold and method
    &#34;&#34;&#34;
    _weight = {}

    def __init__(self, method=&#39;IQR&#39;, threshold=2, verbose=0):
        &#34;&#34;&#34;
        Fill Nans and add column, that there were nans in this column
        
        Args:
            method : [&#39;IQR&#39;, &#39;z_score&#39;,]
        &#34;&#34;&#34;
        self.method = method
        self.threshold = threshold
        self.verbose = verbose


    def _IQR(self, data, col, threshold=1.5):
        &#39;&#39;&#39;
        outlier detection by Interquartile Ranges Rule, also known as Tukey&#39;s test. 
        calculate the IQR ( 75th quantile - 25th quantile) 
        and the 25th 75th quantile. 
        Any value beyond:
            upper bound = 75th quantile + （IQR * threshold）
            lower bound = 25th quantile - （IQR * threshold）   
        are regarded as outliers. Default threshold is 1.5.
        &#39;&#39;&#39;
        
        quantile1,quantile3 = np.percentile(data[col],[25,75])
        iqr_val = quantile3 - quantile1
        lower_bound = quantile1 - (threshold*iqr_val)
        upper_bound = quantile3 + (threshold*iqr_val)
        return(lower_bound, upper_bound)


    def _fit_z_score(self, data, col,):
        &#39;&#39;&#39;
        Z score is an important measurement or score that tells how many Standard deviation above or below a number is from the mean of the dataset
        Any positive Z score means the no. of standard deviation above the mean and a negative score means no. of standard deviation below the mean
        Z score is calculate by subtracting each value with the mean of data and dividing it by standard deviation
        &#39;&#39;&#39;
        median_y = data[col].median()
        median_absolute_deviation_y = (np.abs(data[col]-median_y)).median()
        return(median_y, median_absolute_deviation_y)


    def _get_z_score(self, median, mad, data, col, threshold=3):
        &#39;&#39;&#39;
        Its Modified z_score

        The goal of taking Z-scores is to remove the effects of the location and scale of the data, 
        allowing different datasets to be compared directly. 
        The intuition behind the Z-score method of outlier detection is that, once we’ve centred and rescaled the data, 
        anything that is too far from zero (the threshold is usually a Z-score of 3 or -3) should be considered an outlier.

        The Z-score method relies on the mean and standard deviation of a group of data to measure central tendency and dispersion. 
        This is troublesome, because the mean and standard deviation are highly affected by outliers – they are not robust. 
        In fact, the skewing that outliers bring is one of the biggest reasons for finding and removing outliers from a dataset!

        Another drawback of the Z-score method is that it behaves strangely in small datasets – in fact, the Z-score method will never detect an outlier if the dataset has fewer than 12 items in it. 
        This motivated the development of a modified Z-score method, which does not suffer from the same limitation

        A further benefit of the modified Z-score method is that it uses the median and MAD rather than the mean and standard deviation. 
        The median and MAD are robust measures of central tendency and dispersion, respectively.

        Default threshold is 3.
        &#39;&#39;&#39;
        modified_z_scores = 0.7413 *((data[col] - median)/mad)
        abs_z_scores = np.abs(modified_z_scores)
        filtered_entries = (abs_z_scores &gt; threshold)
        return(filtered_entries)


    @logger.catch
    def fit(self, data, cols=None):
        &#34;&#34;&#34;
        Fit CleanOutliers.

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            self
        &#34;&#34;&#34;
        if cols is not None:
            chek_columns = cols
        else:
            chek_columns = data._get_numeric_data().columns

        self._weight = {}

        for column in chek_columns:
            if self.method == &#39;IQR&#39;:
                lower_bound, upper_bound = self._IQR(data,col=column,threshold=self.threshold)
                self._weight[column] = [lower_bound, upper_bound]
                #logger.info(self._weight)
                if self.verbose:
                    total_outliers = len(data[column][(data[column] &lt; lower_bound) | (data[column] &gt; upper_bound)])

            elif self.method == &#39;z_score&#39;:
                median, mad = self._fit_z_score(data, col=column)
                self._weight[column] = [median, mad]
                if self.verbose:
                    filtered_entries = self._get_z_score(median, mad, data, column, threshold=self.threshold)
                    total_outliers = filtered_entries.sum()
            else:
                raise ValueError(&#39;Wrong method&#39;)

            if self.verbose:
                if total_outliers &gt; 0:
                    logger.info(f&#39;Num of outlier detected: {total_outliers} in Feature {column}&#39;)
                    logger.info(f&#39;Proportion of outlier detected: {round((100/(len(data)/total_outliers)),1)} %&#39;)

        return self


    @logger.catch
    def transform(self, data,) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The dataset.
        &#34;&#34;&#34;
        #logger.info(self._weight)
        for weight_values in self._weight:
            if self.method == &#39;IQR&#39;:
                data.loc[data[weight_values] &lt; self._weight[weight_values][0], weight_values] = self._weight[weight_values][0]
                data.loc[data[weight_values] &gt; self._weight[weight_values][1], weight_values] = self._weight[weight_values][1]

                feature_name = weight_values+&#39;_Is_Outliers_&#39;+self.method
                data[feature_name] = 0
                data.loc[
                    (data[weight_values] &lt; self._weight[weight_values][0]) | (data[weight_values] &gt; self._weight[weight_values][1]), \
                    feature_name
                    ] = 1

            elif self.method == &#39;z_score&#39;:
                filtered_entries = self._get_z_score(
                    self._weight[weight_values][0], 
                    self._weight[weight_values][1], 
                    data, 
                    weight_values, 
                    threshold=self.threshold
                    )
                data.loc[filtered_entries, weight_values] = data[weight_values].median()

                feature_name = weight_values+&#39;_Is_Outliers_&#39;+self.method
                data[feature_name] = 0
                data.loc[filtered_entries, feature_name] = 1

        return data


    def fit_transform(self, data, cols=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Fit and transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                The dataset.
        &#34;&#34;&#34;
        self.fit(data, cols)

        return self.transform(data)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.data_prepare.CleanOutliers.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, data, cols=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit CleanOutliers.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit(self, data, cols=None):
    &#34;&#34;&#34;
    Fit CleanOutliers.

    Args:
        data (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        self
    &#34;&#34;&#34;
    if cols is not None:
        chek_columns = cols
    else:
        chek_columns = data._get_numeric_data().columns

    self._weight = {}

    for column in chek_columns:
        if self.method == &#39;IQR&#39;:
            lower_bound, upper_bound = self._IQR(data,col=column,threshold=self.threshold)
            self._weight[column] = [lower_bound, upper_bound]
            #logger.info(self._weight)
            if self.verbose:
                total_outliers = len(data[column][(data[column] &lt; lower_bound) | (data[column] &gt; upper_bound)])

        elif self.method == &#39;z_score&#39;:
            median, mad = self._fit_z_score(data, col=column)
            self._weight[column] = [median, mad]
            if self.verbose:
                filtered_entries = self._get_z_score(median, mad, data, column, threshold=self.threshold)
                total_outliers = filtered_entries.sum()
        else:
            raise ValueError(&#39;Wrong method&#39;)

        if self.verbose:
            if total_outliers &gt; 0:
                logger.info(f&#39;Num of outlier detected: {total_outliers} in Feature {column}&#39;)
                logger.info(f&#39;Proportion of outlier detected: {round((100/(len(data)/total_outliers)),1)} %&#39;)

    return self</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.CleanOutliers.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, data, cols=None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Fit and transforms the dataset.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>pandas.Dataframe of shape = (n_train, n_features)
The dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_transform(self, data, cols=None) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Fit and transforms the dataset.
    Args:
        data (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        pandas.Dataframe of shape = (n_train, n_features)
            The dataset.
    &#34;&#34;&#34;
    self.fit(data, cols)

    return self.transform(data)</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.CleanOutliers.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, data) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms the dataset.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>pandas.Dataframe of shape = (n_train, n_features)
The dataset.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def transform(self, data,) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Transforms the dataset.
    Args:
        data (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        pandas.Dataframe of shape = (n_train, n_features)
            The dataset.
    &#34;&#34;&#34;
    #logger.info(self._weight)
    for weight_values in self._weight:
        if self.method == &#39;IQR&#39;:
            data.loc[data[weight_values] &lt; self._weight[weight_values][0], weight_values] = self._weight[weight_values][0]
            data.loc[data[weight_values] &gt; self._weight[weight_values][1], weight_values] = self._weight[weight_values][1]

            feature_name = weight_values+&#39;_Is_Outliers_&#39;+self.method
            data[feature_name] = 0
            data.loc[
                (data[weight_values] &lt; self._weight[weight_values][0]) | (data[weight_values] &gt; self._weight[weight_values][1]), \
                feature_name
                ] = 1

        elif self.method == &#39;z_score&#39;:
            filtered_entries = self._get_z_score(
                self._weight[weight_values][0], 
                self._weight[weight_values][1], 
                data, 
                weight_values, 
                threshold=self.threshold
                )
            data.loc[filtered_entries, weight_values] = data[weight_values].median()

            feature_name = weight_values+&#39;_Is_Outliers_&#39;+self.method
            data[feature_name] = 0
            data.loc[filtered_entries, feature_name] = 1

    return data</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.data_prepare.DataPrepare"><code class="flex name class">
<span>class <span class="ident">DataPrepare</span></span>
<span>(</span><span>cat_features=None, clean_and_encod_data=True, cat_encoder_names=['HelmertEncoder', 'CountEncoder'], clean_nan=True, clean_outliers=True, outliers_method='IQR', outliers_threshold=2, drop_invariant=True, num_generator_features=True, operations_num_generator=['/', '*', '-'], normalization=True, reduce_memory=False, random_state=42, verbose=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Сlass for cleaning, encoding and processing your dataset</p>
<p>Description of <strong>init</strong></p>
<h2 id="args">Args</h2>
<p>cat_features=None (list or None):
clean_and_encod_data=True (undefined):
cat_encoder_names=None (list or None):
clean_nan=True (undefined):
drop_invariant=True (bool): boolean for whether or not to drop columns with 0 variance.
num_generator_features=True (undefined):
random_state=42 (undefined):</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DataPrepare(object):
    &#34;&#34;&#34;
    Сlass for cleaning, encoding and processing your dataset
    &#34;&#34;&#34;
    _clean_outliers_enc = None
    _binary_encoder = None
    _clean_nan_encoder = None
    _cat_clean_ord_encoder = None
    _fit_cat_encoders={}


    def __init__(self, 
                cat_features=None,
                clean_and_encod_data=True,
                cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;],
                clean_nan=True,
                clean_outliers=True,
                outliers_method=&#39;IQR&#39;, # method : [&#39;IQR&#39;, &#39;z_score&#39;,]
                outliers_threshold=2,
                drop_invariant=True,
                num_generator_features=True,
                operations_num_generator=[&#39;/&#39;,&#39;*&#39;,&#39;-&#39;,],
                #group_generator_features=False,
                #frequency_enc_num_features=False,
                normalization=True,
                reduce_memory=False,
                random_state=42,
                verbose=3):
        &#34;&#34;&#34;
        Description of __init__

        Args:
            cat_features=None (list or None): 
            clean_and_encod_data=True (undefined):
            cat_encoder_names=None (list or None):
            clean_nan=True (undefined):
            drop_invariant=True (bool): boolean for whether or not to drop columns with 0 variance.
            num_generator_features=True (undefined):
            random_state=42 (undefined):
        &#34;&#34;&#34;
        self.random_state = random_state
        self.cat_encoder_names = cat_encoder_names

        self.verbose = verbose
        logger_print_lvl(self.verbose)

        self._clean_and_encod_data = clean_and_encod_data
        self._clean_nan = clean_nan
        self._clean_outliers = clean_outliers
        self._outliers_threshold = outliers_threshold
        self._drop_invariant = drop_invariant
        self._outliers_method = outliers_method
        self._num_generator_features = num_generator_features
        self._operations_num_generator = operations_num_generator
        self._normalization = normalization
        self._reduce_memory = reduce_memory
        self.cat_features = cat_features


    def _check_data_format(self, data):
        &#34;&#34;&#34;
        Check that data is not pd.DataFrame or empty

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
        Return:
            True or Exception
        &#34;&#34;&#34;
        if (not isinstance(data, pd.DataFrame)) or data.empty:
            raise Exception(&#34;data is not pd.DataFrame or empty&#34;)


    def _check_num_nans(self, data):
        &#34;&#34;&#34;
        Check Nans in numeric features in data 

        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
        Return:
            bool: True or False
        &#34;&#34;&#34;
        data = data._get_numeric_data()
        return(len(list(data.columns[data.isnull().sum() &gt; 0])) &gt; 0)


    def auto_detect_cat_features(self, data):
        &#34;&#34;&#34;
        Description of _auto_detect_cat_features:
            Auto-detection categorical_features by simple rule:
            categorical feature == if feature nunique low 1% of data

        Args:
            data (pd.DataFrame): dataset
            
        Returns:
            cat_features (list): columns names cat features
        
        &#34;&#34;&#34;
        #object_features = list(data.columns[data.dtypes == &#39;object&#39;])
        cat_features = data.columns[(data.nunique(dropna=False) &lt; len(data)//100) &amp; \
            (data.nunique(dropna=False) &gt;2)]
        if len(cat_features) &lt; 1:
            cat_features = None
        #cat_features = list(set([*object_features, *cat_features]))
        return(cat_features)


    @logger.catch
    def fit_transform(self, data, verbose=None):
        &#34;&#34;&#34;
        Fit and transforms the dataset.

        Args:
            data (pd.DataFrame, shape = (n_samples, n_features)): 
                the input data
        Returns:
            data (pd.Dataframe, shape = (n_train, n_features)):
                The dataset with clean numerical and encoded categorical features.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose =  verbose
        logger_print_lvl(self.verbose)
        ########### check_data_format ######################
        self._check_data_format(data)

        start_columns = len(data.columns)
        logger.info(f&#39;Source data shape: {data.shape}&#39;)
        logger.info(&#39;#&#39;*50)
        logger.info(&#39;! START preprocessing Data&#39;)

        data = data.reset_index(drop=True)
        data.replace([np.inf, -np.inf], np.nan, inplace=True)

        ########### Drop invariant  features ######################
        if self._drop_invariant:
            self._drop_invariant_features = \
                data.columns[data.nunique(dropna=False) &lt; 2]
            if len(self._drop_invariant_features) &gt; 0:
                data.drop(self._drop_invariant_features, axis=1, inplace=True)

        ########### Detect type of features ######################

        if self.cat_features is None:
            self.cat_features = self.auto_detect_cat_features(data)
            if self.cat_features is not None:
                logger.info(f&#39;- Auto detect cat features: {len(self.cat_features)}&#39;)

        self.binary_features = data.columns[data.nunique(dropna=False) &lt;= 2]
        self.num_features = list(set(data.select_dtypes(&#39;number&#39;).columns) - set(self.binary_features))
        self.object_features = list(set(data.columns[(data.dtypes == &#39;object&#39;) | (data.dtypes == &#39;category&#39;)]) - set(self.binary_features))


        ########### Binary Features ######################
        if len(self.binary_features) &gt; 0:
            logger.info(&#39;&gt; Binary Features&#39;)

            self._binary_encoder = OrdinalEncoder()
            self._binary_encoder = self._binary_encoder.fit(data[self.binary_features])
            data[self.binary_features] = self._binary_encoder.transform(data[self.binary_features]).replace(2,0)
            

        ########### Categorical Features ######################
        #if self.cat_features is not None:
        # Clean Categorical Features
        if self.object_features is not None:
            logger.info(&#39;&gt; Clean Categorical Features&#39;)
            self._cat_clean_ord_encoder = OrdinalEncoder()
            self._cat_clean_ord_encoder = self._cat_clean_ord_encoder.fit(data[self.object_features])
            data[self.object_features] = self._cat_clean_ord_encoder.transform(data[self.object_features])


        if self.cat_features is not None:
            # Encode Categorical Features
            logger.info(&#39;&gt; Transform Categorical Features.&#39;)

            for cat_encoder_name in self.cat_encoder_names:

                if cat_encoder_name not in cat_encoders_names.keys():
                    raise Exception(f&#34;{cat_encoder_name} not support!&#34;)

                self._fit_cat_encoders[cat_encoder_name] = cat_encoders_names[cat_encoder_name](cols=self.cat_features, drop_invariant=True)
                if cat_encoder_name == &#39;HashingEncoder&#39;:
                    self._fit_cat_encoders[cat_encoder_name] = cat_encoders_names[cat_encoder_name](
                            n_components=int(np.log(len(data.columns))*1000), 
                            drop_invariant=True)
                
                self._fit_cat_encoders[cat_encoder_name] = \
                    self._fit_cat_encoders[cat_encoder_name].fit(data[self.cat_features])

                data_encodet = self._fit_cat_encoders[cat_encoder_name].transform(data[self.cat_features])
                data_encodet = data_encodet.add_prefix(cat_encoder_name + &#39;_&#39;)
                if self._reduce_memory: 
                    data_encodet = reduce_mem_usage(data_encodet)
                logger.info(f&#39; - Encoder: {cat_encoder_name} ADD features: {len(data_encodet.columns)}&#39;)
                data = data.join(data_encodet.reset_index(drop=True))

        ########### Numerical Features ######################
        # CleanOutliers
        if self._clean_outliers:
            logger.info(&#39;&gt; CleanOutliers&#39;,)
            self._clean_outliers_enc = CleanOutliers(
                threshold=self._outliers_threshold, 
                method=self._outliers_method,
                verbose=self.verbose)
            self._clean_outliers_enc = self._clean_outliers_enc.fit(data, cols=self.num_features)
            data = self._clean_outliers_enc.transform(data)

        # CleanNans
        if self._clean_nan:
            if self._check_num_nans(data):
                self._clean_nan_encoder = CleanNans(verbose=self.verbose)
                self._clean_nan_encoder = self._clean_nan_encoder.fit(data[self.num_features])
                data = self._clean_nan_encoder.transform(data)
                logger.info(f&#39;&gt; CleanNans, total nans columns: {len(self._clean_nan_encoder.nan_columns)}&#39;)
            else:
                logger.info(&#39;  No nans features&#39;)

        # Generator interaction Num Features
        if self._num_generator_features:
            if len(self.num_features) &gt; 1:
                logger.info(&#39;&gt; Generate interaction Num Features&#39;)
                self.num_generator = NumericInteractionFeatures(operations=self._operations_num_generator,)
                self.num_generator = self.num_generator.fit(list(self.num_features))
                fe_df = self.num_generator.transform(data[self.num_features])
                
                if self._reduce_memory:
                    fe_df = reduce_mem_usage(fe_df)
                data = pd.concat([
                        data.reset_index(drop=True), 
                        fe_df.reset_index(drop=True)], 
                        axis=1,)
                #data = data.join(fe_df.reset_index(drop=True))
                logger.info(f&#39; ADD features: {fe_df.shape[1]}&#39;)
        
        data.replace([np.inf, -np.inf], np.nan, inplace=True)
        self.data_median_dict = data.median()
        data.fillna(self.data_median_dict, inplace=True)

        ########### Normalization ######################
        if self._normalization:
            logger.info(&#39;&gt; Normalization Features&#39;)
            self.normalization_features = data.columns[data.nunique(dropna=False) &gt; 2].values
            self.scaler = StandardScaler().fit(data[self.normalization_features])
            data_tmp = self.scaler.transform(data[self.normalization_features])
            data_tmp = pd.DataFrame(data_tmp, columns=self.normalization_features)
            data.drop(self.normalization_features, axis=1, inplace=True)
            data = pd.concat([
                        data.reset_index(drop=True), 
                        data_tmp.reset_index(drop=True)], 
                        axis=1,)
            #data[self.normalization_features] = data_tmp[self.normalization_features]
            data_tmp = None

        ########### reduce_mem_usage ######################
        if self._reduce_memory:
            logger.info(&#39;&gt; Reduce_Memory&#39;)
            data = reduce_mem_usage(data, verbose=self.verbose)
        data.fillna(0, inplace=True)

        ########### Final ######################
        end_columns = len(data.columns)
        logger.info(&#39;#&#39;*50)
        logger.info(f&#39;Final data shape: {data.shape}&#39;)
        logger.info(f&#39;Total ADD columns: {end_columns-start_columns}&#39;)
        logger.info(&#39;#&#39;*50)
        return data


    @logger.catch
    def transform(self, data, verbose=None) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transform dataset.
        Args:
            data (pd.DataFrame, shape = (n_samples, n_features)): 
                the input data
        Returns:
            data (pd.Dataframe, shape = (n_train, n_features)):
                The dataset with clean numerical and encoded categorical features.
        &#34;&#34;&#34;
        if verbose is not None:
            self.verbose = verbose

        logger_print_lvl(self.verbose)

        ########### check_data_format ######################
        self._check_data_format(data)

        start_columns = len(data.columns)
        logger.info(&#39;#&#39;*50)
        logger.info(&#39;! Start Transform Data&#39;)

        data = data.reset_index(drop=True)
        data.replace([np.inf, -np.inf], np.nan, inplace=True)

        ########### Drop invariant  features ######################
        if self._drop_invariant:
            if len(self._drop_invariant_features) &gt; 0:
                data.drop(self._drop_invariant_features, axis=1, inplace=True)

        ########### Binary Features ######################
        
        if self._binary_encoder:
            data[self.binary_features] = self._binary_encoder.transform(data[self.binary_features]).replace(2,0)
            logger.info(&#39;&gt; Clean Binary Features&#39;)

        ########### Categorical Features ######################
        # Clean Categorical Features
        if self.object_features is not None:
            logger.info(&#39;&gt; Clean Categorical Features&#39;)
            data[self.object_features] = self._cat_clean_ord_encoder.transform(data[self.object_features])
        
        if self.cat_features is not None:
            # Encode Categorical Features
            logger.info(&#39;&gt; Transform Categorical Features.&#39;)
            for cat_encoder_name in self.cat_encoder_names:
                data_encodet = self._fit_cat_encoders[cat_encoder_name].transform(data[self.cat_features])
                data_encodet = data_encodet.add_prefix(cat_encoder_name + &#39;_&#39;)
                if self._reduce_memory:
                    data_encodet = reduce_mem_usage(data_encodet)
                if self.verbose &gt; 0:
                    logger.info(f&#39; - Encoder: {cat_encoder_name} ADD features: {len(data_encodet.columns)}&#39;)
                data = data.join(data_encodet.reset_index(drop=True))

        ########### Numerical Features ######################
        # CleanOutliers
        if self._clean_outliers:
            data = self._clean_outliers_enc.transform(data)

        # CleanNans
        if self._clean_nan_encoder:
            data = self._clean_nan_encoder.transform(data)
            logger.info(&#39;&gt; Clean Nans&#39;)

        # Generator interaction Num Features
        if self._num_generator_features:
            if len(self.num_features) &gt; 1:
                logger.info(&#39;&gt; Generate interaction Num Features&#39;)
                fe_df = self.num_generator.transform(data[self.num_features])
                
                if self._reduce_memory:
                    fe_df = reduce_mem_usage(fe_df)
                data = pd.concat([
                        data.reset_index(drop=True), 
                        fe_df.reset_index(drop=True)
                        ], axis=1,)
                #data = data.join(fe_df.reset_index(drop=True))
                logger.info(f&#39; ADD features: {fe_df.shape[1]}&#39;)
        
        data.replace([np.inf, -np.inf], np.nan, inplace=True)
        data.fillna(self.data_median_dict, inplace=True)

        ########### Normalization ######################
        if self._normalization:
            logger.info(&#39;&gt; Normalization Features&#39;)
            data_tmp = self.scaler.transform(data[self.normalization_features])
            data_tmp = pd.DataFrame(data_tmp, columns=self.normalization_features)
            data.drop(self.normalization_features,axis=1,inplace=True)
            data = pd.concat([
                        data.reset_index(drop=True), 
                        data_tmp.reset_index(drop=True)], 
                        axis=1,)
            #data[self.normalization_features] = data_tmp[self.normalization_features]
            data_tmp=None

        ########### reduce_mem_usage ######################
        if self._reduce_memory:
            logger.info(&#39;&gt; Reduce_Memory&#39;)
            data = reduce_mem_usage(data, verbose=self.verbose)
        data.fillna(0, inplace=True)

        ########### Final ######################

        end_columns = len(data.columns)
        logger.info(&#39;#&#39;*50)
        logger.info(f&#39;Final data shape: {data.shape}&#39;)
        logger.info(f&#39;Total ADD columns: {end_columns-start_columns}&#39;)
        logger.info(&#39;#&#39;*50)
        return data


    @logger.catch
    def save(self, name=&#39;DataPrepare_dump&#39;, folder=&#39;./&#39;):
        dir_tmp = &#34;./DataPrepare_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        for cat_encoder_name in self.cat_encoder_names:
            joblib.dump(self._fit_cat_encoders[cat_encoder_name], \
                dir_tmp+cat_encoder_name+&#39;.pkl&#39;)

        joblib.dump(self, dir_tmp+&#39;DataPrepare&#39;+&#39;.pkl&#39;)

        shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)

        shutil.rmtree(dir_tmp)
        logger.info(&#39;Save DataPrepare&#39;)


    @logger.catch
    def load(self, name=&#39;DataPrepare_dump&#39;, folder=&#39;./&#39;):
        dir_tmp = &#34;./DataPrepare_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)

        shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)

        de = joblib.load(dir_tmp+&#39;DataPrepare&#39;+&#39;.pkl&#39;)

        for cat_encoder_name in de.cat_encoder_names:
            de._fit_cat_encoders[cat_encoder_name] = joblib.load(dir_tmp+cat_encoder_name+&#39;.pkl&#39;)

        shutil.rmtree(dir_tmp)
        logger.info(&#39;Load DataPrepare&#39;)
        return(de)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.data_prepare.DataPrepare.auto_detect_cat_features"><code class="name flex">
<span>def <span class="ident">auto_detect_cat_features</span></span>(<span>self, data)</span>
</code></dt>
<dd>
<div class="desc"><p>Description of _auto_detect_cat_features:
Auto-detection categorical_features by simple rule:
categorical feature == if feature nunique low 1% of data</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>cat_features (list): columns names cat features</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def auto_detect_cat_features(self, data):
    &#34;&#34;&#34;
    Description of _auto_detect_cat_features:
        Auto-detection categorical_features by simple rule:
        categorical feature == if feature nunique low 1% of data

    Args:
        data (pd.DataFrame): dataset
        
    Returns:
        cat_features (list): columns names cat features
    
    &#34;&#34;&#34;
    #object_features = list(data.columns[data.dtypes == &#39;object&#39;])
    cat_features = data.columns[(data.nunique(dropna=False) &lt; len(data)//100) &amp; \
        (data.nunique(dropna=False) &gt;2)]
    if len(cat_features) &lt; 1:
        cat_features = None
    #cat_features = list(set([*object_features, *cat_features]))
    return(cat_features)</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.DataPrepare.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, data, verbose=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit and transforms the dataset.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape = (n_samples, n_features)):
the input data</p>
<h2 id="returns">Returns</h2>
<p>data (pd.Dataframe, shape = (n_train, n_features)):
The dataset with clean numerical and encoded categorical features.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit_transform(self, data, verbose=None):
    &#34;&#34;&#34;
    Fit and transforms the dataset.

    Args:
        data (pd.DataFrame, shape = (n_samples, n_features)): 
            the input data
    Returns:
        data (pd.Dataframe, shape = (n_train, n_features)):
            The dataset with clean numerical and encoded categorical features.
    &#34;&#34;&#34;
    if verbose is not None:
        self.verbose =  verbose
    logger_print_lvl(self.verbose)
    ########### check_data_format ######################
    self._check_data_format(data)

    start_columns = len(data.columns)
    logger.info(f&#39;Source data shape: {data.shape}&#39;)
    logger.info(&#39;#&#39;*50)
    logger.info(&#39;! START preprocessing Data&#39;)

    data = data.reset_index(drop=True)
    data.replace([np.inf, -np.inf], np.nan, inplace=True)

    ########### Drop invariant  features ######################
    if self._drop_invariant:
        self._drop_invariant_features = \
            data.columns[data.nunique(dropna=False) &lt; 2]
        if len(self._drop_invariant_features) &gt; 0:
            data.drop(self._drop_invariant_features, axis=1, inplace=True)

    ########### Detect type of features ######################

    if self.cat_features is None:
        self.cat_features = self.auto_detect_cat_features(data)
        if self.cat_features is not None:
            logger.info(f&#39;- Auto detect cat features: {len(self.cat_features)}&#39;)

    self.binary_features = data.columns[data.nunique(dropna=False) &lt;= 2]
    self.num_features = list(set(data.select_dtypes(&#39;number&#39;).columns) - set(self.binary_features))
    self.object_features = list(set(data.columns[(data.dtypes == &#39;object&#39;) | (data.dtypes == &#39;category&#39;)]) - set(self.binary_features))


    ########### Binary Features ######################
    if len(self.binary_features) &gt; 0:
        logger.info(&#39;&gt; Binary Features&#39;)

        self._binary_encoder = OrdinalEncoder()
        self._binary_encoder = self._binary_encoder.fit(data[self.binary_features])
        data[self.binary_features] = self._binary_encoder.transform(data[self.binary_features]).replace(2,0)
        

    ########### Categorical Features ######################
    #if self.cat_features is not None:
    # Clean Categorical Features
    if self.object_features is not None:
        logger.info(&#39;&gt; Clean Categorical Features&#39;)
        self._cat_clean_ord_encoder = OrdinalEncoder()
        self._cat_clean_ord_encoder = self._cat_clean_ord_encoder.fit(data[self.object_features])
        data[self.object_features] = self._cat_clean_ord_encoder.transform(data[self.object_features])


    if self.cat_features is not None:
        # Encode Categorical Features
        logger.info(&#39;&gt; Transform Categorical Features.&#39;)

        for cat_encoder_name in self.cat_encoder_names:

            if cat_encoder_name not in cat_encoders_names.keys():
                raise Exception(f&#34;{cat_encoder_name} not support!&#34;)

            self._fit_cat_encoders[cat_encoder_name] = cat_encoders_names[cat_encoder_name](cols=self.cat_features, drop_invariant=True)
            if cat_encoder_name == &#39;HashingEncoder&#39;:
                self._fit_cat_encoders[cat_encoder_name] = cat_encoders_names[cat_encoder_name](
                        n_components=int(np.log(len(data.columns))*1000), 
                        drop_invariant=True)
            
            self._fit_cat_encoders[cat_encoder_name] = \
                self._fit_cat_encoders[cat_encoder_name].fit(data[self.cat_features])

            data_encodet = self._fit_cat_encoders[cat_encoder_name].transform(data[self.cat_features])
            data_encodet = data_encodet.add_prefix(cat_encoder_name + &#39;_&#39;)
            if self._reduce_memory: 
                data_encodet = reduce_mem_usage(data_encodet)
            logger.info(f&#39; - Encoder: {cat_encoder_name} ADD features: {len(data_encodet.columns)}&#39;)
            data = data.join(data_encodet.reset_index(drop=True))

    ########### Numerical Features ######################
    # CleanOutliers
    if self._clean_outliers:
        logger.info(&#39;&gt; CleanOutliers&#39;,)
        self._clean_outliers_enc = CleanOutliers(
            threshold=self._outliers_threshold, 
            method=self._outliers_method,
            verbose=self.verbose)
        self._clean_outliers_enc = self._clean_outliers_enc.fit(data, cols=self.num_features)
        data = self._clean_outliers_enc.transform(data)

    # CleanNans
    if self._clean_nan:
        if self._check_num_nans(data):
            self._clean_nan_encoder = CleanNans(verbose=self.verbose)
            self._clean_nan_encoder = self._clean_nan_encoder.fit(data[self.num_features])
            data = self._clean_nan_encoder.transform(data)
            logger.info(f&#39;&gt; CleanNans, total nans columns: {len(self._clean_nan_encoder.nan_columns)}&#39;)
        else:
            logger.info(&#39;  No nans features&#39;)

    # Generator interaction Num Features
    if self._num_generator_features:
        if len(self.num_features) &gt; 1:
            logger.info(&#39;&gt; Generate interaction Num Features&#39;)
            self.num_generator = NumericInteractionFeatures(operations=self._operations_num_generator,)
            self.num_generator = self.num_generator.fit(list(self.num_features))
            fe_df = self.num_generator.transform(data[self.num_features])
            
            if self._reduce_memory:
                fe_df = reduce_mem_usage(fe_df)
            data = pd.concat([
                    data.reset_index(drop=True), 
                    fe_df.reset_index(drop=True)], 
                    axis=1,)
            #data = data.join(fe_df.reset_index(drop=True))
            logger.info(f&#39; ADD features: {fe_df.shape[1]}&#39;)
    
    data.replace([np.inf, -np.inf], np.nan, inplace=True)
    self.data_median_dict = data.median()
    data.fillna(self.data_median_dict, inplace=True)

    ########### Normalization ######################
    if self._normalization:
        logger.info(&#39;&gt; Normalization Features&#39;)
        self.normalization_features = data.columns[data.nunique(dropna=False) &gt; 2].values
        self.scaler = StandardScaler().fit(data[self.normalization_features])
        data_tmp = self.scaler.transform(data[self.normalization_features])
        data_tmp = pd.DataFrame(data_tmp, columns=self.normalization_features)
        data.drop(self.normalization_features, axis=1, inplace=True)
        data = pd.concat([
                    data.reset_index(drop=True), 
                    data_tmp.reset_index(drop=True)], 
                    axis=1,)
        #data[self.normalization_features] = data_tmp[self.normalization_features]
        data_tmp = None

    ########### reduce_mem_usage ######################
    if self._reduce_memory:
        logger.info(&#39;&gt; Reduce_Memory&#39;)
        data = reduce_mem_usage(data, verbose=self.verbose)
    data.fillna(0, inplace=True)

    ########### Final ######################
    end_columns = len(data.columns)
    logger.info(&#39;#&#39;*50)
    logger.info(f&#39;Final data shape: {data.shape}&#39;)
    logger.info(f&#39;Total ADD columns: {end_columns-start_columns}&#39;)
    logger.info(&#39;#&#39;*50)
    return data</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.DataPrepare.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, name='DataPrepare_dump', folder='./')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def load(self, name=&#39;DataPrepare_dump&#39;, folder=&#39;./&#39;):
    dir_tmp = &#34;./DataPrepare_tmp/&#34;
    Path(dir_tmp).mkdir(parents=True, exist_ok=True)

    shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)

    de = joblib.load(dir_tmp+&#39;DataPrepare&#39;+&#39;.pkl&#39;)

    for cat_encoder_name in de.cat_encoder_names:
        de._fit_cat_encoders[cat_encoder_name] = joblib.load(dir_tmp+cat_encoder_name+&#39;.pkl&#39;)

    shutil.rmtree(dir_tmp)
    logger.info(&#39;Load DataPrepare&#39;)
    return(de)</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.DataPrepare.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, name='DataPrepare_dump', folder='./')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def save(self, name=&#39;DataPrepare_dump&#39;, folder=&#39;./&#39;):
    dir_tmp = &#34;./DataPrepare_tmp/&#34;
    Path(dir_tmp).mkdir(parents=True, exist_ok=True)
    for cat_encoder_name in self.cat_encoder_names:
        joblib.dump(self._fit_cat_encoders[cat_encoder_name], \
            dir_tmp+cat_encoder_name+&#39;.pkl&#39;)

    joblib.dump(self, dir_tmp+&#39;DataPrepare&#39;+&#39;.pkl&#39;)

    shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)

    shutil.rmtree(dir_tmp)
    logger.info(&#39;Save DataPrepare&#39;)</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.DataPrepare.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, data, verbose=None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Transform dataset.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape = (n_samples, n_features)):
the input data</p>
<h2 id="returns">Returns</h2>
<p>data (pd.Dataframe, shape = (n_train, n_features)):
The dataset with clean numerical and encoded categorical features.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def transform(self, data, verbose=None) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Transform dataset.
    Args:
        data (pd.DataFrame, shape = (n_samples, n_features)): 
            the input data
    Returns:
        data (pd.Dataframe, shape = (n_train, n_features)):
            The dataset with clean numerical and encoded categorical features.
    &#34;&#34;&#34;
    if verbose is not None:
        self.verbose = verbose

    logger_print_lvl(self.verbose)

    ########### check_data_format ######################
    self._check_data_format(data)

    start_columns = len(data.columns)
    logger.info(&#39;#&#39;*50)
    logger.info(&#39;! Start Transform Data&#39;)

    data = data.reset_index(drop=True)
    data.replace([np.inf, -np.inf], np.nan, inplace=True)

    ########### Drop invariant  features ######################
    if self._drop_invariant:
        if len(self._drop_invariant_features) &gt; 0:
            data.drop(self._drop_invariant_features, axis=1, inplace=True)

    ########### Binary Features ######################
    
    if self._binary_encoder:
        data[self.binary_features] = self._binary_encoder.transform(data[self.binary_features]).replace(2,0)
        logger.info(&#39;&gt; Clean Binary Features&#39;)

    ########### Categorical Features ######################
    # Clean Categorical Features
    if self.object_features is not None:
        logger.info(&#39;&gt; Clean Categorical Features&#39;)
        data[self.object_features] = self._cat_clean_ord_encoder.transform(data[self.object_features])
    
    if self.cat_features is not None:
        # Encode Categorical Features
        logger.info(&#39;&gt; Transform Categorical Features.&#39;)
        for cat_encoder_name in self.cat_encoder_names:
            data_encodet = self._fit_cat_encoders[cat_encoder_name].transform(data[self.cat_features])
            data_encodet = data_encodet.add_prefix(cat_encoder_name + &#39;_&#39;)
            if self._reduce_memory:
                data_encodet = reduce_mem_usage(data_encodet)
            if self.verbose &gt; 0:
                logger.info(f&#39; - Encoder: {cat_encoder_name} ADD features: {len(data_encodet.columns)}&#39;)
            data = data.join(data_encodet.reset_index(drop=True))

    ########### Numerical Features ######################
    # CleanOutliers
    if self._clean_outliers:
        data = self._clean_outliers_enc.transform(data)

    # CleanNans
    if self._clean_nan_encoder:
        data = self._clean_nan_encoder.transform(data)
        logger.info(&#39;&gt; Clean Nans&#39;)

    # Generator interaction Num Features
    if self._num_generator_features:
        if len(self.num_features) &gt; 1:
            logger.info(&#39;&gt; Generate interaction Num Features&#39;)
            fe_df = self.num_generator.transform(data[self.num_features])
            
            if self._reduce_memory:
                fe_df = reduce_mem_usage(fe_df)
            data = pd.concat([
                    data.reset_index(drop=True), 
                    fe_df.reset_index(drop=True)
                    ], axis=1,)
            #data = data.join(fe_df.reset_index(drop=True))
            logger.info(f&#39; ADD features: {fe_df.shape[1]}&#39;)
    
    data.replace([np.inf, -np.inf], np.nan, inplace=True)
    data.fillna(self.data_median_dict, inplace=True)

    ########### Normalization ######################
    if self._normalization:
        logger.info(&#39;&gt; Normalization Features&#39;)
        data_tmp = self.scaler.transform(data[self.normalization_features])
        data_tmp = pd.DataFrame(data_tmp, columns=self.normalization_features)
        data.drop(self.normalization_features,axis=1,inplace=True)
        data = pd.concat([
                    data.reset_index(drop=True), 
                    data_tmp.reset_index(drop=True)], 
                    axis=1,)
        #data[self.normalization_features] = data_tmp[self.normalization_features]
        data_tmp=None

    ########### reduce_mem_usage ######################
    if self._reduce_memory:
        logger.info(&#39;&gt; Reduce_Memory&#39;)
        data = reduce_mem_usage(data, verbose=self.verbose)
    data.fillna(0, inplace=True)

    ########### Final ######################

    end_columns = len(data.columns)
    logger.info(&#39;#&#39;*50)
    logger.info(f&#39;Final data shape: {data.shape}&#39;)
    logger.info(f&#39;Total ADD columns: {end_columns-start_columns}&#39;)
    logger.info(&#39;#&#39;*50)
    return data</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.data_prepare.NumericInteractionFeatures"><code class="flex name class">
<span>class <span class="ident">NumericInteractionFeatures</span></span>
<span>(</span><span>operations=['/', '*', '-', '+'], verbose=0)</span>
</code></dt>
<dd>
<div class="desc"><p>Сlass for
Numerical interaction generator features: A/B, A*B, A-B,</p>
<p>Fill Nans and add column, that there were nans in this column</p>
<h2 id="args">Args</h2>
<p>method : ['median', 'mean',]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NumericInteractionFeatures(object):
    &#34;&#34;&#34;
    Сlass for  Numerical interaction generator features: A/B, A*B, A-B,
    &#34;&#34;&#34;
    _cols_combinations = None


    def __init__(self, operations=[&#39;/&#39;,&#39;*&#39;,&#39;-&#39;,&#39;+&#39;], verbose=0):
        &#34;&#34;&#34;
        Fill Nans and add column, that there were nans in this column
        
        Args:
            method : [&#39;median&#39;, &#39;mean&#39;,]
        &#34;&#34;&#34;
        self.operations = operations
        self.verbose = verbose
        self.columns = None


    def fit(self, columns,):
        &#34;&#34;&#34;
        Fit.

        Args:
            columns (list): num columns names
        Returns:
            self
        &#34;&#34;&#34;
        self.columns = columns
        self._cols_combinations = list(combinations(columns,2))
        return self


    def transform(self, df) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Transforms the dataset.
        Args:
            df (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
                Dataset with new features.
        &#34;&#34;&#34;
        if self._cols_combinations is None:
            raise Exception(&#34;No fit cols_combinations&#34;)

        fe_df = pd.DataFrame()

        for col1 in self.columns:
            for col2 in self.columns:
                if col1 == col2:
                    continue
                else:
                    if &#39;/&#39; in self.operations:
                        fe_df[&#39;{}_/_{}&#39;.format(col1, col2) ] = (df[col1]*1.) / df[col2]
                    if &#39;-&#39; in self.operations:
                        fe_df[&#39;{}_-_{}&#39;.format(col1, col2) ] = df[col1] - df[col2]

        for c in self._cols_combinations:
            if &#39;*&#39; in self.operations:
                fe_df[&#39;{}_*_{}&#39;.format(c[0], c[1]) ] = df[c[0]] * df[c[1]]
            if &#39;+&#39; in self.operations:
                fe_df[&#39;{}_+_{}&#39;.format(c[0], c[1]) ] = df[c[0]] + df[c[1]]
        return(fe_df)


    def fit_transform(self, data, cols) -&gt; pd.DataFrame:
        &#34;&#34;&#34;Fit and transforms the dataset.
        Args:
            data (pd.DataFrame, shape (n_samples, n_features)): the input data
            cols list() features: the input data
        Returns:
            pandas.Dataframe of shape = (n_train, n_features)
        &#34;&#34;&#34;
        self.fit(cols)

        return self.transform(data)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.data_prepare.NumericInteractionFeatures.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, columns)</span>
</code></dt>
<dd>
<div class="desc"><p>Fit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>columns</code></strong> :&ensp;<code>list</code></dt>
<dd>num columns names</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>self</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, columns,):
    &#34;&#34;&#34;
    Fit.

    Args:
        columns (list): num columns names
    Returns:
        self
    &#34;&#34;&#34;
    self.columns = columns
    self._cols_combinations = list(combinations(columns,2))
    return self</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.NumericInteractionFeatures.fit_transform"><code class="name flex">
<span>def <span class="ident">fit_transform</span></span>(<span>self, data, cols) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Fit and transforms the dataset.</p>
<h2 id="args">Args</h2>
<p>data (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>pandas.Dataframe of shape = (n_train, n_features)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_transform(self, data, cols) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Fit and transforms the dataset.
    Args:
        data (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        pandas.Dataframe of shape = (n_train, n_features)
    &#34;&#34;&#34;
    self.fit(cols)

    return self.transform(data)</code></pre>
</details>
</dd>
<dt id="automl_alex.data_prepare.NumericInteractionFeatures.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self, df) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Transforms the dataset.</p>
<h2 id="args">Args</h2>
<p>df (pd.DataFrame, shape (n_samples, n_features)): the input data
cols list() features: the input data</p>
<h2 id="returns">Returns</h2>
<p>pandas.Dataframe of shape = (n_train, n_features)
Dataset with new features.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, df) -&gt; pd.DataFrame:
    &#34;&#34;&#34;Transforms the dataset.
    Args:
        df (pd.DataFrame, shape (n_samples, n_features)): the input data
        cols list() features: the input data
    Returns:
        pandas.Dataframe of shape = (n_train, n_features)
            Dataset with new features.
    &#34;&#34;&#34;
    if self._cols_combinations is None:
        raise Exception(&#34;No fit cols_combinations&#34;)

    fe_df = pd.DataFrame()

    for col1 in self.columns:
        for col2 in self.columns:
            if col1 == col2:
                continue
            else:
                if &#39;/&#39; in self.operations:
                    fe_df[&#39;{}_/_{}&#39;.format(col1, col2) ] = (df[col1]*1.) / df[col2]
                if &#39;-&#39; in self.operations:
                    fe_df[&#39;{}_-_{}&#39;.format(col1, col2) ] = df[col1] - df[col2]

    for c in self._cols_combinations:
        if &#39;*&#39; in self.operations:
            fe_df[&#39;{}_*_{}&#39;.format(c[0], c[1]) ] = df[c[0]] * df[c[1]]
        if &#39;+&#39; in self.operations:
            fe_df[&#39;{}_+_{}&#39;.format(c[0], c[1]) ] = df[c[0]] + df[c[1]]
    return(fe_df)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="automl_alex" href="index.html">automl_alex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="automl_alex.data_prepare.reduce_mem_usage" href="#automl_alex.data_prepare.reduce_mem_usage">reduce_mem_usage</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="automl_alex.data_prepare.CleanNans" href="#automl_alex.data_prepare.CleanNans">CleanNans</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.data_prepare.CleanNans.fit" href="#automl_alex.data_prepare.CleanNans.fit">fit</a></code></li>
<li><code><a title="automl_alex.data_prepare.CleanNans.fit_transform" href="#automl_alex.data_prepare.CleanNans.fit_transform">fit_transform</a></code></li>
<li><code><a title="automl_alex.data_prepare.CleanNans.transform" href="#automl_alex.data_prepare.CleanNans.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.data_prepare.CleanOutliers" href="#automl_alex.data_prepare.CleanOutliers">CleanOutliers</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.data_prepare.CleanOutliers.fit" href="#automl_alex.data_prepare.CleanOutliers.fit">fit</a></code></li>
<li><code><a title="automl_alex.data_prepare.CleanOutliers.fit_transform" href="#automl_alex.data_prepare.CleanOutliers.fit_transform">fit_transform</a></code></li>
<li><code><a title="automl_alex.data_prepare.CleanOutliers.transform" href="#automl_alex.data_prepare.CleanOutliers.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.data_prepare.DataPrepare" href="#automl_alex.data_prepare.DataPrepare">DataPrepare</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.data_prepare.DataPrepare.auto_detect_cat_features" href="#automl_alex.data_prepare.DataPrepare.auto_detect_cat_features">auto_detect_cat_features</a></code></li>
<li><code><a title="automl_alex.data_prepare.DataPrepare.fit_transform" href="#automl_alex.data_prepare.DataPrepare.fit_transform">fit_transform</a></code></li>
<li><code><a title="automl_alex.data_prepare.DataPrepare.load" href="#automl_alex.data_prepare.DataPrepare.load">load</a></code></li>
<li><code><a title="automl_alex.data_prepare.DataPrepare.save" href="#automl_alex.data_prepare.DataPrepare.save">save</a></code></li>
<li><code><a title="automl_alex.data_prepare.DataPrepare.transform" href="#automl_alex.data_prepare.DataPrepare.transform">transform</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.data_prepare.NumericInteractionFeatures" href="#automl_alex.data_prepare.NumericInteractionFeatures">NumericInteractionFeatures</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.data_prepare.NumericInteractionFeatures.fit" href="#automl_alex.data_prepare.NumericInteractionFeatures.fit">fit</a></code></li>
<li><code><a title="automl_alex.data_prepare.NumericInteractionFeatures.fit_transform" href="#automl_alex.data_prepare.NumericInteractionFeatures.fit_transform">fit_transform</a></code></li>
<li><code><a title="automl_alex.data_prepare.NumericInteractionFeatures.transform" href="#automl_alex.data_prepare.NumericInteractionFeatures.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>