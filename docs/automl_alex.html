<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>automl_alex.automl_alex API documentation</title>
<meta name="description" content="AutoML and other Toolbox" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>automl_alex.automl_alex</code></h1>
</header>
<section id="section-intro">
<p>AutoML and other Toolbox</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;AutoML and other Toolbox&#39;&#39;&#39;

from typing import Any
from typing import Callable
from typing import Dict
from typing import List
from typing import Optional
from typing import Sequence
from typing import Type
from typing import Union
from sklearn.metrics import *
from tqdm import tqdm
import pandas as pd
import time
import joblib
import automl_alex
from .models import *
from .cross_validation import *
from .data_prepare import *
from ._encoders import *
from .optimizer import *
from pathlib import Path
from ._logger import *

TMP_FOLDER = &#39;.automl-alex_tmp/&#39;

##################################### ModelsReview ################################################


class ModelsReview(object):
    &#39;&#39;&#39;
    ModelsReview - allows you to see which models show good results on this data

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import ModelsReview, DataPrepare
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; # clean up data before use ModelsReview
    &gt;&gt;&gt; de = DataPrepare()
    &gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
    &gt;&gt;&gt; clean_X_test = de.transform(X_test)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = ModelsReview(type_of_estimator=&#39;classifier&#39;,
    &gt;&gt;&gt;                     metric = sklearn.metrics.roc_auc_score,)
    &gt;&gt;&gt; review = model.fit(X_train=X_train, 
    &gt;&gt;&gt;                     y_train=y_train, 
    &gt;&gt;&gt;                     X_test=X_test, 
    &gt;&gt;&gt;                     y_test=y_test,)

    &#39;&#39;&#39;    
    __name__ = &#39;ModelsReview&#39;

    def __init__(self,  
                    type_of_estimator: Optional[str] = None, # classifier or regression
                    metric: Optional[Callable] = None,
                    metric_round: int = 4,
                    gpu: bool = False, 
                    random_state: int = 42
                    ) -&gt; None:
        &#39;&#39;&#39;
        Parameters
        ----------
        type_of_estimator : Optional[str], optional
            [&#39;classifier&#39;, &#39;regression&#39;], by default None
        metric : Callable, optional
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.
        metric_round : int, optional
            round metric score., by default 4
        gpu : bool, optional
            Use GPU?, by default False
        random_state : int, optional
            Controls the generation of the random states for each repetition, by default 42
        &#39;&#39;&#39;    
        self._gpu = gpu
        self._random_state = random_state
        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is None:
            if self._type_of_estimator == &#39;classifier&#39;:
                self._metric = sklearn.metrics.roc_auc_score
            elif self._type_of_estimator == &#39;regression&#39;:
                self._metric = sklearn.metrics.mean_squared_error
        else:
            self._metric = metric
        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X_train: pd.DataFrame, 
        y_train: Union[list, np.array, pd.DataFrame],
        X_test: pd.DataFrame, 
        y_test: Union[list, np.array, pd.DataFrame],
        models_names: Optional[List[str]] = None,
        verbose: int = 3,
        ) -&gt; pd.DataFrame:
        &#39;&#39;&#39;
        Fit models from model_list and return scores

        Parameters
        ----------
        X_train : pd.DataFrame
            train data (pd.DataFrame, shape (n_samples, n_features))
        y_train : Union[list, np.array, pd.DataFrame]
            target
        X_test : pd.DataFrame
            test data (pd.DataFrame, shape (n_samples, n_features))
        y_test : Union[list, np.array, pd.DataFrame]
            test target
        models_names : Optional[List[str]], optional
            list of models from automl_alex.models.all_models, by default None
        verbose : int, optional
            print state, by default 3

        Returns
        -------
        pd.DataFrame
            results
        &#39;&#39;&#39;        
        logger_print_lvl(verbose)
        result = pd.DataFrame(columns=[&#39;Model_Name&#39;, &#39;Score&#39;, &#39;Time_Fit_Sec&#39;])
        score_ls = []
        time_ls = []
        if models_names is None:
            self.models_names = automl_alex.models.all_models.keys()
        else:
            self.models_names = models_names

        result[&#39;Model_Name&#39;] = self.models_names
        
        if verbose &gt; 0:
            disable_tqdm = False
        else: 
            disable_tqdm = True
        for model_name in tqdm(self.models_names, disable=disable_tqdm):
            # Model
            start_time = time.time()
            model_tmp = automl_alex.models.all_models[model_name](
                                            gpu=self._gpu, 
                                            random_state=self._random_state,
                                            type_of_estimator=self._type_of_estimator)
            # fit
            model_tmp.fit(X_train, y_train)
            # Predict
            if (self._metric.__name__ in predict_proba_metrics) and (model_tmp.is_possible_predict_proba()):
                y_pred = model_tmp.predict_proba(X_test)
            else:
                y_pred = model_tmp.predict(X_test)

            score_model = round(self._metric(y_test, y_pred), self._metric_round)
            score_ls.append(score_model)
            iter_time = round((time.time() - start_time),2)
            time_ls.append(iter_time)
            model_tmp = None

        result[&#39;Score&#39;] = score_ls
        result[&#39;Time_Fit_Sec&#39;] = time_ls
        self.result = result
        return(result)

class ModelsReviewClassifier(ModelsReview):
    &#39;&#39;&#39;
    ModelsReview - allows you to see which models show good results on this data

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import ModelsReviewClassifier, DataPrepare
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; # clean up data before use ModelsReview
    &gt;&gt;&gt; de = DataPrepare()
    &gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
    &gt;&gt;&gt; clean_X_test = de.transform(X_test)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = ModelsReviewClassifier(metric = sklearn.metrics.roc_auc_score,)
    &gt;&gt;&gt; review = model.fit(X_train=X_train, 
    &gt;&gt;&gt;                     y_train=y_train, 
    &gt;&gt;&gt;                     X_test=X_test, 
    &gt;&gt;&gt;                     y_test=y_test,)
    &#39;&#39;&#39;   
    _type_of_estimator=&#39;classifier&#39;


class ModelsReviewRegressor(ModelsReview):
    &#39;&#39;&#39;
    ModelsReview - allows you to see which models show good results on this data

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import ModelsReviewRegressor, DataPrepare
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(data_id=543, as_frame=True)
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.data), 
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.target), 
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; # clean up data before use ModelsReview
    &gt;&gt;&gt; de = DataPrepare()
    &gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
    &gt;&gt;&gt; clean_X_test = de.transform(X_test)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = ModelsReviewRegressor(metric = sklearn.metrics.mean_squared_error,)
    &gt;&gt;&gt; review = model.fit(X_train=X_train, 
    &gt;&gt;&gt;                     y_train=y_train, 
    &gt;&gt;&gt;                     X_test=X_test, 
    &gt;&gt;&gt;                     y_test=y_test,)
    &#39;&#39;&#39;   
    _type_of_estimator=&#39;regression&#39;



##################################### Stacking #########################################

# in progress...

##################################### AutoML #########################################

class AutoML(object):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import AutoML
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = AutoML(type_of_estimator=&#39;classifier&#39;)
    &gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
    &gt;&gt;&gt; predicts = model.predict(X_test)
    &gt;&gt;&gt; print(&#39;Test AUC: &#39;, round(sklearn.metrics.roc_auc_score(y_test, predicts),4))

    &#39;&#39;&#39;    
    __name__ = &#39;AutoML&#39;

    def __init__(self,  
                type_of_estimator: Optional[str] = None, # classifier or regression
                metric: Optional[Callable] = None,
                metric_round: int = 4,
                gpu: bool = False, 
                random_state: int = 42
                ) -&gt; None:
        &#39;&#39;&#39;
        Parameters
        ----------
        type_of_estimator : Optional[str], optional
            [&#39;classifier&#39;, &#39;regression&#39;], by default None
        metric : Callable, optional
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.
        metric_round : int, optional
            round metric score., by default 4
        gpu : bool, optional
            Use GPU?, by default False
        random_state : int, optional
            Controls the generation of the random states for each repetition, by default 42
        &#39;&#39;&#39;    
        self._gpu = gpu
        self._random_state = random_state

        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is not None:
            self.metric = metric
        else:
            if self._type_of_estimator == &#39;classifier&#39;:
                self.metric = sklearn.metrics.roc_auc_score
                self.direction = &#39;maximize&#39;
            elif self._type_of_estimator == &#39;regression&#39;:
                self.metric = sklearn.metrics.mean_squared_error
                self.direction = &#39;minimize&#39;

        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X: pd.DataFrame, 
        y: Union[list, np.array, pd.DataFrame], 
        timeout: int = 500, # optimization time in seconds
        auto_parameters: bool = True,
        folds: int = 7,
        score_folds: int = 2,
        opt_lvl: int = 2,
        early_stoping: int = 100,
        feature_selection: bool = True,
        verbose: int = 3,
        ) -&gt; None:
        &#39;&#39;&#39;
        Fit the model.

        Parameters
        ----------
        X : pd.DataFrame
            data (pd.DataFrame, shape (n_samples, n_features))
        y : Union[list, np.array, pd.DataFrame]
            target
        timeout : int, optional
            Optimization time in seconds.
        auto_parameters: bool, optional
            If we don&#39;t want to select anything, we just set auto_parameters=True. 
            Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
                -folds,
                -score_folds,
                -cold_start,
                -opt_lvl,
        folds : int, optional
            Number of folds for CrossValidation. Must be at least 2, by default 7
        score_folds : int, optional
            Number of score folds. Must be at least 1., by default 2
        opt_lvl : int, optional
            by limiting the optimization time, we will have to choose how deep we should optimize the parameters. 
            Perhaps some parameters are not so important and can only give a fraction of a percent. 
            By setting the opt_lvl parameter, you control the depth of optimization.
            in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration
            by default 2
        early_stoping : int, optional
            stop optimization if no better parameters are found through iterations
        feature_selection : bool, optional
            add feature_selection in optimization, by default True
        verbose : int, optional
            print state, by default 3

        Returns
        -------
        None
        &#39;&#39;&#39;
        logger_print_lvl(verbose)
        X_source = X.copy()
        ####################################################
        # STEP 0
        start_step_0 = time.time()
        logger.info(&#39;&gt; Start Fit Base Model&#39;)
        if timeout &lt; 400:
            logger.warning(&#34;! Not enough time to find the optimal parameters. \n \
                    Please, Increase the &#39;timeout&#39; parameter for normal optimization. (min 500 sec)&#34;)

        self.cat_features=X.columns[(X.dtypes == &#39;object&#39;) | (X.dtypes == &#39;category&#39;)]
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        self.model_1 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )
        self.model_1 = self.model_1.fit(X, y, cat_features=self.cat_features.tolist())
        X = None

        ####################################################
        # STEP 1
        start_step_1 = time.time()
        logger.info(&#39;&gt; DATA PREPROC&#39;)
        self.de_1 = automl_alex.DataPrepare(
            cat_encoder_names=[&#39;OneHotEncoder&#39;, &#39;CountEncoder&#39;],
            #outliers_threshold=3,
            normalization=True,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_1.fit_transform(X_source,)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        params = {
                &#39;metric&#39; : self.metric,
                &#39;metric_round&#39; :self._metric_round,
                &#39;auto_parameters&#39;:auto_parameters,
                &#39;folds&#39;:folds,
                &#39;score_folds&#39;:score_folds,
                &#39;opt_lvl&#39;:opt_lvl,
                &#39;early_stoping&#39;:early_stoping,
                &#39;type_of_estimator&#39;:self._type_of_estimator,
                &#39;random_state&#39;:self._random_state,
                &#39;gpu&#39;:self._gpu,
                #&#39;iteration_check&#39;: False,
                }

        # logger.info(50*&#39;#&#39;)
        # logger.info(&#39;&gt; Start Fit Models 2&#39;)
        # logger.info(50*&#39;#&#39;)
        # # Model 2
        # self.model_2 = automl_alex.BestSingleModel(
        #     models_names = [&#39;LinearModel&#39;,],
        #     feature_selection=False,
        #     **params,
        #     )

        # history = self.model_2.opt(X,y, timeout=100, verbose=verbose)
        # self.model_2.save(name=&#39;model_2&#39;, folder=TMP_FOLDER,)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 3&#39;)
        logger.info(50*&#39;#&#39;)
        # Model 3
        self.model_3 = automl_alex.MLP(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            verbose=verbose,
            )
        self.model_3 = self.model_3.fit(X, y)

        X = None

        total_step_1 = (time.time() - start_step_1)

        ####################################################
        # STEP 2
        start_step_2 = time.time()

        logger.info(&#39;&gt; DATA PREPROC&#39;)

        self.de_2 = DataPrepare(
            cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;,&#39;HashingEncoder&#39;],
            #outliers_threshold=3,
            normalization=False,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_2.fit_transform(X_source)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 4&#39;)

        self.model_4 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )

        self.model_4 = self.model_4.fit(X, y)

        total_step_2 = (time.time() - start_step_2)

        ####################################################
        # STEP 3
        # Model 2 - 3
        start_step_3 = time.time()

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 5&#39;)
        logger.info(50*&#39;#&#39;)

        time_to_opt = (timeout - (time.time()-start_step_0)) - 60
        time.sleep(0.1)

        self.model_5 = automl_alex.BestSingleModel(
            models_names = [&#39;LightGBM&#39;, &#39;ExtraTrees&#39;],
            feature_selection=feature_selection,
            **params,
            )

        history = self.model_5.opt(X,y, timeout=time_to_opt, verbose=verbose)
        self.model_5.save(name=&#39;model_5&#39;, folder=TMP_FOLDER,)

        total_step_4 = (time.time() - start_step_3)

        ####################################################
        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Finish!&#39;)


    @logger.catch
    def predict(self, X: pd.DataFrame, verbose: int = 0) -&gt; list:
        &#39;&#39;&#39;
        Predict the target for the input data

        Parameters
        ----------
        X : pd.DataFrame
            data (pd.DataFrame, shape (n_samples, n_features))
        verbose : int, optional
            print state, by default 0

        Returns
        -------
        list
            prediction

        Raises
        ------
        Exception
            If No fit models
        &#39;&#39;&#39;        
        if self.model_1 is None:
            raise Exception(&#34;No fit models&#34;)

        X_source = X.copy()
        ####################################################
        # STEP 0
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        # MODEL 1
        self.predict_model_1 = self.model_1.predict_or_predict_proba(X)

        ####################################################
        # STEP 1
        X = self.de_1.transform(X_source, verbose=verbose)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        # MODEL 2
        # self.model_2 = self.model_2.load(name=&#39;model_2&#39;, folder=TMP_FOLDER,)
        # self.predict_model_2 = self.model_2.predict(X)

        # MODEL 3
        self.predict_model_3 = self.model_3.predict_or_predict_proba(X)

        ####################################################
        # STEP 2
        X = self.de_2.transform(X_source, verbose=verbose)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        # MODEL 4
        self.predict_model_4 = self.model_4.predict_or_predict_proba(X)
        
        # MODEL 5
        self.model_5 = self.model_5.load(name=&#39;model_5&#39;, folder=TMP_FOLDER,)
        self.predict_model_5 = self.model_5.predict(X)
        
        ####################################################
        # STEP 4
        # Blend
        predicts = (
            self.predict_model_1
            #+self.predict_model_2
            +self.predict_model_3
            +self.predict_model_4
            +self.predict_model_5)/4
        return (predicts)


    @logger.catch
    def save(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;) -&gt; None:
        &#39;&#39;&#39;
        Save the model to disk

        Parameters
        ----------
        name : str, optional
            file name, by default &#39;AutoML_dump&#39;
        folder : str, optional
            target folder, by default &#39;./&#39;
        &#39;&#39;&#39;        
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        self.de_1.save(name=&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        self.de_2.save(name=&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        joblib.dump(self, dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        #self.model_2.save(name=&#39;model_2&#39;, folder=dir_tmp,)
        self.model_5.save(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Save AutoML&#39;)


    @logger.catch
    def load(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;) -&gt; Callable:
        &#39;&#39;&#39;
        Loads the model and creates a function that will load the model

        Parameters
        ----------
        name : str, optional
            file name, by default &#39;AutoML_dump&#39;
        folder : str, optional
            target folder, by default &#39;./&#39;

        Returns
        -------
        Callable
            AutoML
        &#39;&#39;&#39;        
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)
        model = joblib.load(dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        model.de_1 = DataPrepare()
        model.de_1 = model.de_1.load(&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        model.de_2 = DataPrepare()
        model.de_2 = model.de_2.load(&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        #model.model_2 = model.model_2.load(name=&#39;model_2&#39;, folder=dir_tmp,)
        model.model_5 = model.model_5.load(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Load AutoML&#39;)
        return(model)


class AutoMLClassifier(AutoML):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import AutoMLClassifier
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = AutoMLClassifier()
    &gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
    &gt;&gt;&gt; predicts = model.predict(X_test)
    &gt;&gt;&gt; print(&#39;Test AUC: &#39;, round(sklearn.metrics.roc_auc_score(y_test, predicts),4))

    &#39;&#39;&#39;    
    _type_of_estimator=&#39;classifier&#39;
    __name__ = &#39;AutoMLClassifier&#39;


class AutoMLRegressor(AutoML):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import AutoMLRegressor
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(data_id=543, as_frame=True)
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.data), 
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.target), 
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = AutoMLRegressor()
    &gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
    &gt;&gt;&gt; predicts = model.predict(X_test)
    &gt;&gt;&gt; print(&#39;Test MSE: &#39;, round(sklearn.metrics.mean_squared_error(y_test, predicts),4))

    &#39;&#39;&#39;   
    _type_of_estimator=&#39;regression&#39;
    __name__ = &#39;AutoMLRegressor&#39;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="automl_alex.automl_alex.AutoML"><code class="flex name class">
<span>class <span class="ident">AutoML</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[Callable, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>AutoML in the process of developing</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from automl_alex import AutoML
&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; # Get Dataset
&gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name='adult', version=1, as_frame=True)
&gt;&gt;&gt; dataset.target = dataset.target.astype('category').cat.codes
&gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
&gt;&gt;&gt;                                             dataset.data, 
&gt;&gt;&gt;                                             dataset.target,
&gt;&gt;&gt;                                             test_size=0.2,)
&gt;&gt;&gt; 
&gt;&gt;&gt; model = AutoML(type_of_estimator='classifier')
&gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
&gt;&gt;&gt; predicts = model.predict(X_test)
&gt;&gt;&gt; print('Test AUC: ', round(sklearn.metrics.roc_auc_score(y_test, predicts),4))
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type_of_estimator</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>['classifier', 'regression'], by default None</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>you can use standard metrics from sklearn.metrics or add custom metrics.
If None, the metric is selected from the type of estimator:
classifier: sklearn.metrics.roc_auc_score
regression: sklearn.metrics.mean_squared_error.</dd>
<dt><strong><code>metric_round</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>round metric score., by default 4</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use GPU?, by default False</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Controls the generation of the random states for each repetition, by default 42</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoML(object):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import AutoML
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = AutoML(type_of_estimator=&#39;classifier&#39;)
    &gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
    &gt;&gt;&gt; predicts = model.predict(X_test)
    &gt;&gt;&gt; print(&#39;Test AUC: &#39;, round(sklearn.metrics.roc_auc_score(y_test, predicts),4))

    &#39;&#39;&#39;    
    __name__ = &#39;AutoML&#39;

    def __init__(self,  
                type_of_estimator: Optional[str] = None, # classifier or regression
                metric: Optional[Callable] = None,
                metric_round: int = 4,
                gpu: bool = False, 
                random_state: int = 42
                ) -&gt; None:
        &#39;&#39;&#39;
        Parameters
        ----------
        type_of_estimator : Optional[str], optional
            [&#39;classifier&#39;, &#39;regression&#39;], by default None
        metric : Callable, optional
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.
        metric_round : int, optional
            round metric score., by default 4
        gpu : bool, optional
            Use GPU?, by default False
        random_state : int, optional
            Controls the generation of the random states for each repetition, by default 42
        &#39;&#39;&#39;    
        self._gpu = gpu
        self._random_state = random_state

        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is not None:
            self.metric = metric
        else:
            if self._type_of_estimator == &#39;classifier&#39;:
                self.metric = sklearn.metrics.roc_auc_score
                self.direction = &#39;maximize&#39;
            elif self._type_of_estimator == &#39;regression&#39;:
                self.metric = sklearn.metrics.mean_squared_error
                self.direction = &#39;minimize&#39;

        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X: pd.DataFrame, 
        y: Union[list, np.array, pd.DataFrame], 
        timeout: int = 500, # optimization time in seconds
        auto_parameters: bool = True,
        folds: int = 7,
        score_folds: int = 2,
        opt_lvl: int = 2,
        early_stoping: int = 100,
        feature_selection: bool = True,
        verbose: int = 3,
        ) -&gt; None:
        &#39;&#39;&#39;
        Fit the model.

        Parameters
        ----------
        X : pd.DataFrame
            data (pd.DataFrame, shape (n_samples, n_features))
        y : Union[list, np.array, pd.DataFrame]
            target
        timeout : int, optional
            Optimization time in seconds.
        auto_parameters: bool, optional
            If we don&#39;t want to select anything, we just set auto_parameters=True. 
            Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
                -folds,
                -score_folds,
                -cold_start,
                -opt_lvl,
        folds : int, optional
            Number of folds for CrossValidation. Must be at least 2, by default 7
        score_folds : int, optional
            Number of score folds. Must be at least 1., by default 2
        opt_lvl : int, optional
            by limiting the optimization time, we will have to choose how deep we should optimize the parameters. 
            Perhaps some parameters are not so important and can only give a fraction of a percent. 
            By setting the opt_lvl parameter, you control the depth of optimization.
            in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration
            by default 2
        early_stoping : int, optional
            stop optimization if no better parameters are found through iterations
        feature_selection : bool, optional
            add feature_selection in optimization, by default True
        verbose : int, optional
            print state, by default 3

        Returns
        -------
        None
        &#39;&#39;&#39;
        logger_print_lvl(verbose)
        X_source = X.copy()
        ####################################################
        # STEP 0
        start_step_0 = time.time()
        logger.info(&#39;&gt; Start Fit Base Model&#39;)
        if timeout &lt; 400:
            logger.warning(&#34;! Not enough time to find the optimal parameters. \n \
                    Please, Increase the &#39;timeout&#39; parameter for normal optimization. (min 500 sec)&#34;)

        self.cat_features=X.columns[(X.dtypes == &#39;object&#39;) | (X.dtypes == &#39;category&#39;)]
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        self.model_1 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )
        self.model_1 = self.model_1.fit(X, y, cat_features=self.cat_features.tolist())
        X = None

        ####################################################
        # STEP 1
        start_step_1 = time.time()
        logger.info(&#39;&gt; DATA PREPROC&#39;)
        self.de_1 = automl_alex.DataPrepare(
            cat_encoder_names=[&#39;OneHotEncoder&#39;, &#39;CountEncoder&#39;],
            #outliers_threshold=3,
            normalization=True,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_1.fit_transform(X_source,)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        params = {
                &#39;metric&#39; : self.metric,
                &#39;metric_round&#39; :self._metric_round,
                &#39;auto_parameters&#39;:auto_parameters,
                &#39;folds&#39;:folds,
                &#39;score_folds&#39;:score_folds,
                &#39;opt_lvl&#39;:opt_lvl,
                &#39;early_stoping&#39;:early_stoping,
                &#39;type_of_estimator&#39;:self._type_of_estimator,
                &#39;random_state&#39;:self._random_state,
                &#39;gpu&#39;:self._gpu,
                #&#39;iteration_check&#39;: False,
                }

        # logger.info(50*&#39;#&#39;)
        # logger.info(&#39;&gt; Start Fit Models 2&#39;)
        # logger.info(50*&#39;#&#39;)
        # # Model 2
        # self.model_2 = automl_alex.BestSingleModel(
        #     models_names = [&#39;LinearModel&#39;,],
        #     feature_selection=False,
        #     **params,
        #     )

        # history = self.model_2.opt(X,y, timeout=100, verbose=verbose)
        # self.model_2.save(name=&#39;model_2&#39;, folder=TMP_FOLDER,)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 3&#39;)
        logger.info(50*&#39;#&#39;)
        # Model 3
        self.model_3 = automl_alex.MLP(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            verbose=verbose,
            )
        self.model_3 = self.model_3.fit(X, y)

        X = None

        total_step_1 = (time.time() - start_step_1)

        ####################################################
        # STEP 2
        start_step_2 = time.time()

        logger.info(&#39;&gt; DATA PREPROC&#39;)

        self.de_2 = DataPrepare(
            cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;,&#39;HashingEncoder&#39;],
            #outliers_threshold=3,
            normalization=False,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_2.fit_transform(X_source)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 4&#39;)

        self.model_4 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )

        self.model_4 = self.model_4.fit(X, y)

        total_step_2 = (time.time() - start_step_2)

        ####################################################
        # STEP 3
        # Model 2 - 3
        start_step_3 = time.time()

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 5&#39;)
        logger.info(50*&#39;#&#39;)

        time_to_opt = (timeout - (time.time()-start_step_0)) - 60
        time.sleep(0.1)

        self.model_5 = automl_alex.BestSingleModel(
            models_names = [&#39;LightGBM&#39;, &#39;ExtraTrees&#39;],
            feature_selection=feature_selection,
            **params,
            )

        history = self.model_5.opt(X,y, timeout=time_to_opt, verbose=verbose)
        self.model_5.save(name=&#39;model_5&#39;, folder=TMP_FOLDER,)

        total_step_4 = (time.time() - start_step_3)

        ####################################################
        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Finish!&#39;)


    @logger.catch
    def predict(self, X: pd.DataFrame, verbose: int = 0) -&gt; list:
        &#39;&#39;&#39;
        Predict the target for the input data

        Parameters
        ----------
        X : pd.DataFrame
            data (pd.DataFrame, shape (n_samples, n_features))
        verbose : int, optional
            print state, by default 0

        Returns
        -------
        list
            prediction

        Raises
        ------
        Exception
            If No fit models
        &#39;&#39;&#39;        
        if self.model_1 is None:
            raise Exception(&#34;No fit models&#34;)

        X_source = X.copy()
        ####################################################
        # STEP 0
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        # MODEL 1
        self.predict_model_1 = self.model_1.predict_or_predict_proba(X)

        ####################################################
        # STEP 1
        X = self.de_1.transform(X_source, verbose=verbose)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        # MODEL 2
        # self.model_2 = self.model_2.load(name=&#39;model_2&#39;, folder=TMP_FOLDER,)
        # self.predict_model_2 = self.model_2.predict(X)

        # MODEL 3
        self.predict_model_3 = self.model_3.predict_or_predict_proba(X)

        ####################################################
        # STEP 2
        X = self.de_2.transform(X_source, verbose=verbose)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        # MODEL 4
        self.predict_model_4 = self.model_4.predict_or_predict_proba(X)
        
        # MODEL 5
        self.model_5 = self.model_5.load(name=&#39;model_5&#39;, folder=TMP_FOLDER,)
        self.predict_model_5 = self.model_5.predict(X)
        
        ####################################################
        # STEP 4
        # Blend
        predicts = (
            self.predict_model_1
            #+self.predict_model_2
            +self.predict_model_3
            +self.predict_model_4
            +self.predict_model_5)/4
        return (predicts)


    @logger.catch
    def save(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;) -&gt; None:
        &#39;&#39;&#39;
        Save the model to disk

        Parameters
        ----------
        name : str, optional
            file name, by default &#39;AutoML_dump&#39;
        folder : str, optional
            target folder, by default &#39;./&#39;
        &#39;&#39;&#39;        
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        self.de_1.save(name=&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        self.de_2.save(name=&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        joblib.dump(self, dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        #self.model_2.save(name=&#39;model_2&#39;, folder=dir_tmp,)
        self.model_5.save(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Save AutoML&#39;)


    @logger.catch
    def load(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;) -&gt; Callable:
        &#39;&#39;&#39;
        Loads the model and creates a function that will load the model

        Parameters
        ----------
        name : str, optional
            file name, by default &#39;AutoML_dump&#39;
        folder : str, optional
            target folder, by default &#39;./&#39;

        Returns
        -------
        Callable
            AutoML
        &#39;&#39;&#39;        
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)
        model = joblib.load(dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        model.de_1 = DataPrepare()
        model.de_1 = model.de_1.load(&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        model.de_2 = DataPrepare()
        model.de_2 = model.de_2.load(&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        #model.model_2 = model.model_2.load(name=&#39;model_2&#39;, folder=dir_tmp,)
        model.model_5 = model.model_5.load(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Load AutoML&#39;)
        return(model)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.AutoMLClassifier" href="#automl_alex.automl_alex.AutoMLClassifier">AutoMLClassifier</a></li>
<li><a title="automl_alex.automl_alex.AutoMLRegressor" href="#automl_alex.automl_alex.AutoMLRegressor">AutoMLRegressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.automl_alex.AutoML.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X: pandas.core.frame.DataFrame, y: Union[list, <built-in function array>, pandas.core.frame.DataFrame], timeout: int = 500, auto_parameters: bool = True, folds: int = 7, score_folds: int = 2, opt_lvl: int = 2, early_stoping: int = 100, feature_selection: bool = True, verbose: int = 3) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Fit the model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>data (pd.DataFrame, shape (n_samples, n_features))</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>Union[list, np.array, pd.DataFrame]</code></dt>
<dd>target</dd>
<dt><strong><code>timeout</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Optimization time in seconds.</dd>
<dt><strong><code>auto_parameters</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If we don't want to select anything, we just set auto_parameters=True.
Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
-folds,
-score_folds,
-cold_start,
-opt_lvl,</dd>
<dt><strong><code>folds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of folds for CrossValidation. Must be at least 2, by default 7</dd>
<dt><strong><code>score_folds</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of score folds. Must be at least 1., by default 2</dd>
<dt><strong><code>opt_lvl</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>by limiting the optimization time, we will have to choose how deep we should optimize the parameters.
Perhaps some parameters are not so important and can only give a fraction of a percent.
By setting the opt_lvl parameter, you control the depth of optimization.
in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration
by default 2</dd>
<dt><strong><code>early_stoping</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>stop optimization if no better parameters are found through iterations</dd>
<dt><strong><code>feature_selection</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>add feature_selection in optimization, by default True</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>print state, by default 3</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit(self,
    X: pd.DataFrame, 
    y: Union[list, np.array, pd.DataFrame], 
    timeout: int = 500, # optimization time in seconds
    auto_parameters: bool = True,
    folds: int = 7,
    score_folds: int = 2,
    opt_lvl: int = 2,
    early_stoping: int = 100,
    feature_selection: bool = True,
    verbose: int = 3,
    ) -&gt; None:
    &#39;&#39;&#39;
    Fit the model.

    Parameters
    ----------
    X : pd.DataFrame
        data (pd.DataFrame, shape (n_samples, n_features))
    y : Union[list, np.array, pd.DataFrame]
        target
    timeout : int, optional
        Optimization time in seconds.
    auto_parameters: bool, optional
        If we don&#39;t want to select anything, we just set auto_parameters=True. 
        Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
            -folds,
            -score_folds,
            -cold_start,
            -opt_lvl,
    folds : int, optional
        Number of folds for CrossValidation. Must be at least 2, by default 7
    score_folds : int, optional
        Number of score folds. Must be at least 1., by default 2
    opt_lvl : int, optional
        by limiting the optimization time, we will have to choose how deep we should optimize the parameters. 
        Perhaps some parameters are not so important and can only give a fraction of a percent. 
        By setting the opt_lvl parameter, you control the depth of optimization.
        in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration
        by default 2
    early_stoping : int, optional
        stop optimization if no better parameters are found through iterations
    feature_selection : bool, optional
        add feature_selection in optimization, by default True
    verbose : int, optional
        print state, by default 3

    Returns
    -------
    None
    &#39;&#39;&#39;
    logger_print_lvl(verbose)
    X_source = X.copy()
    ####################################################
    # STEP 0
    start_step_0 = time.time()
    logger.info(&#39;&gt; Start Fit Base Model&#39;)
    if timeout &lt; 400:
        logger.warning(&#34;! Not enough time to find the optimal parameters. \n \
                Please, Increase the &#39;timeout&#39; parameter for normal optimization. (min 500 sec)&#34;)

    self.cat_features=X.columns[(X.dtypes == &#39;object&#39;) | (X.dtypes == &#39;category&#39;)]
    X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
    X.fillna(0, inplace=True)
    X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

    self.model_1 = automl_alex.CatBoost(
        type_of_estimator=self._type_of_estimator, 
        random_state=self._random_state,
        gpu=self._gpu,
        verbose=verbose,
        )
    self.model_1 = self.model_1.fit(X, y, cat_features=self.cat_features.tolist())
    X = None

    ####################################################
    # STEP 1
    start_step_1 = time.time()
    logger.info(&#39;&gt; DATA PREPROC&#39;)
    self.de_1 = automl_alex.DataPrepare(
        cat_encoder_names=[&#39;OneHotEncoder&#39;, &#39;CountEncoder&#39;],
        #outliers_threshold=3,
        normalization=True,
        random_state=self._random_state, 
        verbose=verbose
        )
    X = self.de_1.fit_transform(X_source,)
    if self.de_1.cat_features is not None:
        X = X.drop(self.de_1.cat_features, axis = 1)

    params = {
            &#39;metric&#39; : self.metric,
            &#39;metric_round&#39; :self._metric_round,
            &#39;auto_parameters&#39;:auto_parameters,
            &#39;folds&#39;:folds,
            &#39;score_folds&#39;:score_folds,
            &#39;opt_lvl&#39;:opt_lvl,
            &#39;early_stoping&#39;:early_stoping,
            &#39;type_of_estimator&#39;:self._type_of_estimator,
            &#39;random_state&#39;:self._random_state,
            &#39;gpu&#39;:self._gpu,
            #&#39;iteration_check&#39;: False,
            }

    # logger.info(50*&#39;#&#39;)
    # logger.info(&#39;&gt; Start Fit Models 2&#39;)
    # logger.info(50*&#39;#&#39;)
    # # Model 2
    # self.model_2 = automl_alex.BestSingleModel(
    #     models_names = [&#39;LinearModel&#39;,],
    #     feature_selection=False,
    #     **params,
    #     )

    # history = self.model_2.opt(X,y, timeout=100, verbose=verbose)
    # self.model_2.save(name=&#39;model_2&#39;, folder=TMP_FOLDER,)

    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Start Fit Models 3&#39;)
    logger.info(50*&#39;#&#39;)
    # Model 3
    self.model_3 = automl_alex.MLP(
        type_of_estimator=self._type_of_estimator, 
        random_state=self._random_state,
        verbose=verbose,
        )
    self.model_3 = self.model_3.fit(X, y)

    X = None

    total_step_1 = (time.time() - start_step_1)

    ####################################################
    # STEP 2
    start_step_2 = time.time()

    logger.info(&#39;&gt; DATA PREPROC&#39;)

    self.de_2 = DataPrepare(
        cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;,&#39;HashingEncoder&#39;],
        #outliers_threshold=3,
        normalization=False,
        random_state=self._random_state, 
        verbose=verbose
        )
    X = self.de_2.fit_transform(X_source)
    #X = X.drop(self.de_2.cat_features, axis = 1)

    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Start Fit Models 4&#39;)

    self.model_4 = automl_alex.CatBoost(
        type_of_estimator=self._type_of_estimator, 
        random_state=self._random_state,
        gpu=self._gpu,
        verbose=verbose,
        )

    self.model_4 = self.model_4.fit(X, y)

    total_step_2 = (time.time() - start_step_2)

    ####################################################
    # STEP 3
    # Model 2 - 3
    start_step_3 = time.time()

    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Start Fit Models 5&#39;)
    logger.info(50*&#39;#&#39;)

    time_to_opt = (timeout - (time.time()-start_step_0)) - 60
    time.sleep(0.1)

    self.model_5 = automl_alex.BestSingleModel(
        models_names = [&#39;LightGBM&#39;, &#39;ExtraTrees&#39;],
        feature_selection=feature_selection,
        **params,
        )

    history = self.model_5.opt(X,y, timeout=time_to_opt, verbose=verbose)
    self.model_5.save(name=&#39;model_5&#39;, folder=TMP_FOLDER,)

    total_step_4 = (time.time() - start_step_3)

    ####################################################
    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Finish!&#39;)</code></pre>
</details>
</dd>
<dt id="automl_alex.automl_alex.AutoML.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, name: str = 'AutoML_dump', folder: str = './') ‑> Callable</span>
</code></dt>
<dd>
<div class="desc"><p>Loads the model and creates a function that will load the model</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>file name, by default 'AutoML_dump'</dd>
<dt><strong><code>folder</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>target folder, by default './'</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Callable</code></dt>
<dd>AutoML</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def load(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;) -&gt; Callable:
    &#39;&#39;&#39;
    Loads the model and creates a function that will load the model

    Parameters
    ----------
    name : str, optional
        file name, by default &#39;AutoML_dump&#39;
    folder : str, optional
        target folder, by default &#39;./&#39;

    Returns
    -------
    Callable
        AutoML
    &#39;&#39;&#39;        
    dir_tmp = folder+&#34;AutoML_tmp/&#34;
    Path(dir_tmp).mkdir(parents=True, exist_ok=True)
    shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)
    model = joblib.load(dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
    model.de_1 = DataPrepare()
    model.de_1 = model.de_1.load(&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
    model.de_2 = DataPrepare()
    model.de_2 = model.de_2.load(&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
    #model.model_2 = model.model_2.load(name=&#39;model_2&#39;, folder=dir_tmp,)
    model.model_5 = model.model_5.load(name=&#39;model_5&#39;, folder=dir_tmp,)
    shutil.rmtree(dir_tmp)
    logger.info(&#39;Load AutoML&#39;)
    return(model)</code></pre>
</details>
</dd>
<dt id="automl_alex.automl_alex.AutoML.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X: pandas.core.frame.DataFrame, verbose: int = 0) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Predict the target for the input data</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>data (pd.DataFrame, shape (n_samples, n_features))</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>print state, by default 0</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>prediction</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If No fit models</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def predict(self, X: pd.DataFrame, verbose: int = 0) -&gt; list:
    &#39;&#39;&#39;
    Predict the target for the input data

    Parameters
    ----------
    X : pd.DataFrame
        data (pd.DataFrame, shape (n_samples, n_features))
    verbose : int, optional
        print state, by default 0

    Returns
    -------
    list
        prediction

    Raises
    ------
    Exception
        If No fit models
    &#39;&#39;&#39;        
    if self.model_1 is None:
        raise Exception(&#34;No fit models&#34;)

    X_source = X.copy()
    ####################################################
    # STEP 0
    X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
    X.fillna(0, inplace=True)
    X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

    # MODEL 1
    self.predict_model_1 = self.model_1.predict_or_predict_proba(X)

    ####################################################
    # STEP 1
    X = self.de_1.transform(X_source, verbose=verbose)
    if self.de_1.cat_features is not None:
        X = X.drop(self.de_1.cat_features, axis = 1)

    # MODEL 2
    # self.model_2 = self.model_2.load(name=&#39;model_2&#39;, folder=TMP_FOLDER,)
    # self.predict_model_2 = self.model_2.predict(X)

    # MODEL 3
    self.predict_model_3 = self.model_3.predict_or_predict_proba(X)

    ####################################################
    # STEP 2
    X = self.de_2.transform(X_source, verbose=verbose)
    #X = X.drop(self.de_2.cat_features, axis = 1)

    # MODEL 4
    self.predict_model_4 = self.model_4.predict_or_predict_proba(X)
    
    # MODEL 5
    self.model_5 = self.model_5.load(name=&#39;model_5&#39;, folder=TMP_FOLDER,)
    self.predict_model_5 = self.model_5.predict(X)
    
    ####################################################
    # STEP 4
    # Blend
    predicts = (
        self.predict_model_1
        #+self.predict_model_2
        +self.predict_model_3
        +self.predict_model_4
        +self.predict_model_5)/4
    return (predicts)</code></pre>
</details>
</dd>
<dt id="automl_alex.automl_alex.AutoML.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, name: str = 'AutoML_dump', folder: str = './') ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Save the model to disk</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>file name, by default 'AutoML_dump'</dd>
<dt><strong><code>folder</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>target folder, by default './'</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def save(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;) -&gt; None:
    &#39;&#39;&#39;
    Save the model to disk

    Parameters
    ----------
    name : str, optional
        file name, by default &#39;AutoML_dump&#39;
    folder : str, optional
        target folder, by default &#39;./&#39;
    &#39;&#39;&#39;        
    dir_tmp = folder+&#34;AutoML_tmp/&#34;
    Path(dir_tmp).mkdir(parents=True, exist_ok=True)
    self.de_1.save(name=&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
    self.de_2.save(name=&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
    joblib.dump(self, dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
    #self.model_2.save(name=&#39;model_2&#39;, folder=dir_tmp,)
    self.model_5.save(name=&#39;model_5&#39;, folder=dir_tmp,)
    shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)
    shutil.rmtree(dir_tmp)
    logger.info(&#39;Save AutoML&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.automl_alex.AutoMLClassifier"><code class="flex name class">
<span>class <span class="ident">AutoMLClassifier</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[Callable, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>AutoML in the process of developing</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from automl_alex import AutoMLClassifier
&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; # Get Dataset
&gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name='adult', version=1, as_frame=True)
&gt;&gt;&gt; dataset.target = dataset.target.astype('category').cat.codes
&gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
&gt;&gt;&gt;                                             dataset.data, 
&gt;&gt;&gt;                                             dataset.target,
&gt;&gt;&gt;                                             test_size=0.2,)
&gt;&gt;&gt; 
&gt;&gt;&gt; model = AutoMLClassifier()
&gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
&gt;&gt;&gt; predicts = model.predict(X_test)
&gt;&gt;&gt; print('Test AUC: ', round(sklearn.metrics.roc_auc_score(y_test, predicts),4))
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type_of_estimator</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>['classifier', 'regression'], by default None</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>you can use standard metrics from sklearn.metrics or add custom metrics.
If None, the metric is selected from the type of estimator:
classifier: sklearn.metrics.roc_auc_score
regression: sklearn.metrics.mean_squared_error.</dd>
<dt><strong><code>metric_round</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>round metric score., by default 4</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use GPU?, by default False</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Controls the generation of the random states for each repetition, by default 42</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoMLClassifier(AutoML):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import AutoMLClassifier
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = AutoMLClassifier()
    &gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
    &gt;&gt;&gt; predicts = model.predict(X_test)
    &gt;&gt;&gt; print(&#39;Test AUC: &#39;, round(sklearn.metrics.roc_auc_score(y_test, predicts),4))

    &#39;&#39;&#39;    
    _type_of_estimator=&#39;classifier&#39;
    __name__ = &#39;AutoMLClassifier&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.AutoML.fit" href="#automl_alex.automl_alex.AutoML.fit">fit</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.load" href="#automl_alex.automl_alex.AutoML.load">load</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.predict" href="#automl_alex.automl_alex.AutoML.predict">predict</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.save" href="#automl_alex.automl_alex.AutoML.save">save</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="automl_alex.automl_alex.AutoMLRegressor"><code class="flex name class">
<span>class <span class="ident">AutoMLRegressor</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[Callable, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>AutoML in the process of developing</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from automl_alex import AutoMLRegressor
&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; # Get Dataset
&gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(data_id=543, as_frame=True)
&gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
&gt;&gt;&gt;                                             pd.DataFrame(dataset.data), 
&gt;&gt;&gt;                                             pd.DataFrame(dataset.target), 
&gt;&gt;&gt;                                             test_size=0.2,)
&gt;&gt;&gt; 
&gt;&gt;&gt; model = AutoMLRegressor()
&gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
&gt;&gt;&gt; predicts = model.predict(X_test)
&gt;&gt;&gt; print('Test MSE: ', round(sklearn.metrics.mean_squared_error(y_test, predicts),4))
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type_of_estimator</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>['classifier', 'regression'], by default None</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>you can use standard metrics from sklearn.metrics or add custom metrics.
If None, the metric is selected from the type of estimator:
classifier: sklearn.metrics.roc_auc_score
regression: sklearn.metrics.mean_squared_error.</dd>
<dt><strong><code>metric_round</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>round metric score., by default 4</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use GPU?, by default False</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Controls the generation of the random states for each repetition, by default 42</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoMLRegressor(AutoML):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import AutoMLRegressor
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(data_id=543, as_frame=True)
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.data), 
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.target), 
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = AutoMLRegressor()
    &gt;&gt;&gt; model.fit(X_train, y_train, timeout=600)
    &gt;&gt;&gt; predicts = model.predict(X_test)
    &gt;&gt;&gt; print(&#39;Test MSE: &#39;, round(sklearn.metrics.mean_squared_error(y_test, predicts),4))

    &#39;&#39;&#39;   
    _type_of_estimator=&#39;regression&#39;
    __name__ = &#39;AutoMLRegressor&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.AutoML.fit" href="#automl_alex.automl_alex.AutoML.fit">fit</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.load" href="#automl_alex.automl_alex.AutoML.load">load</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.predict" href="#automl_alex.automl_alex.AutoML.predict">predict</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.save" href="#automl_alex.automl_alex.AutoML.save">save</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="automl_alex.automl_alex.ModelsReview"><code class="flex name class">
<span>class <span class="ident">ModelsReview</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[Callable, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>ModelsReview - allows you to see which models show good results on this data</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from automl_alex import ModelsReview, DataPrepare
&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; # Get Dataset
&gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name='adult', version=1, as_frame=True)
&gt;&gt;&gt; dataset.target = dataset.target.astype('category').cat.codes
&gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
&gt;&gt;&gt;                                             dataset.data, 
&gt;&gt;&gt;                                             dataset.target,
&gt;&gt;&gt;                                             test_size=0.2,)
&gt;&gt;&gt; # clean up data before use ModelsReview
&gt;&gt;&gt; de = DataPrepare()
&gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
&gt;&gt;&gt; clean_X_test = de.transform(X_test)
&gt;&gt;&gt; 
&gt;&gt;&gt; model = ModelsReview(type_of_estimator='classifier',
&gt;&gt;&gt;                     metric = sklearn.metrics.roc_auc_score,)
&gt;&gt;&gt; review = model.fit(X_train=X_train, 
&gt;&gt;&gt;                     y_train=y_train, 
&gt;&gt;&gt;                     X_test=X_test, 
&gt;&gt;&gt;                     y_test=y_test,)
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type_of_estimator</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>['classifier', 'regression'], by default None</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>you can use standard metrics from sklearn.metrics or add custom metrics.
If None, the metric is selected from the type of estimator:
classifier: sklearn.metrics.roc_auc_score
regression: sklearn.metrics.mean_squared_error.</dd>
<dt><strong><code>metric_round</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>round metric score., by default 4</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use GPU?, by default False</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Controls the generation of the random states for each repetition, by default 42</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelsReview(object):
    &#39;&#39;&#39;
    ModelsReview - allows you to see which models show good results on this data

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import ModelsReview, DataPrepare
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; # clean up data before use ModelsReview
    &gt;&gt;&gt; de = DataPrepare()
    &gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
    &gt;&gt;&gt; clean_X_test = de.transform(X_test)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = ModelsReview(type_of_estimator=&#39;classifier&#39;,
    &gt;&gt;&gt;                     metric = sklearn.metrics.roc_auc_score,)
    &gt;&gt;&gt; review = model.fit(X_train=X_train, 
    &gt;&gt;&gt;                     y_train=y_train, 
    &gt;&gt;&gt;                     X_test=X_test, 
    &gt;&gt;&gt;                     y_test=y_test,)

    &#39;&#39;&#39;    
    __name__ = &#39;ModelsReview&#39;

    def __init__(self,  
                    type_of_estimator: Optional[str] = None, # classifier or regression
                    metric: Optional[Callable] = None,
                    metric_round: int = 4,
                    gpu: bool = False, 
                    random_state: int = 42
                    ) -&gt; None:
        &#39;&#39;&#39;
        Parameters
        ----------
        type_of_estimator : Optional[str], optional
            [&#39;classifier&#39;, &#39;regression&#39;], by default None
        metric : Callable, optional
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.
        metric_round : int, optional
            round metric score., by default 4
        gpu : bool, optional
            Use GPU?, by default False
        random_state : int, optional
            Controls the generation of the random states for each repetition, by default 42
        &#39;&#39;&#39;    
        self._gpu = gpu
        self._random_state = random_state
        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is None:
            if self._type_of_estimator == &#39;classifier&#39;:
                self._metric = sklearn.metrics.roc_auc_score
            elif self._type_of_estimator == &#39;regression&#39;:
                self._metric = sklearn.metrics.mean_squared_error
        else:
            self._metric = metric
        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X_train: pd.DataFrame, 
        y_train: Union[list, np.array, pd.DataFrame],
        X_test: pd.DataFrame, 
        y_test: Union[list, np.array, pd.DataFrame],
        models_names: Optional[List[str]] = None,
        verbose: int = 3,
        ) -&gt; pd.DataFrame:
        &#39;&#39;&#39;
        Fit models from model_list and return scores

        Parameters
        ----------
        X_train : pd.DataFrame
            train data (pd.DataFrame, shape (n_samples, n_features))
        y_train : Union[list, np.array, pd.DataFrame]
            target
        X_test : pd.DataFrame
            test data (pd.DataFrame, shape (n_samples, n_features))
        y_test : Union[list, np.array, pd.DataFrame]
            test target
        models_names : Optional[List[str]], optional
            list of models from automl_alex.models.all_models, by default None
        verbose : int, optional
            print state, by default 3

        Returns
        -------
        pd.DataFrame
            results
        &#39;&#39;&#39;        
        logger_print_lvl(verbose)
        result = pd.DataFrame(columns=[&#39;Model_Name&#39;, &#39;Score&#39;, &#39;Time_Fit_Sec&#39;])
        score_ls = []
        time_ls = []
        if models_names is None:
            self.models_names = automl_alex.models.all_models.keys()
        else:
            self.models_names = models_names

        result[&#39;Model_Name&#39;] = self.models_names
        
        if verbose &gt; 0:
            disable_tqdm = False
        else: 
            disable_tqdm = True
        for model_name in tqdm(self.models_names, disable=disable_tqdm):
            # Model
            start_time = time.time()
            model_tmp = automl_alex.models.all_models[model_name](
                                            gpu=self._gpu, 
                                            random_state=self._random_state,
                                            type_of_estimator=self._type_of_estimator)
            # fit
            model_tmp.fit(X_train, y_train)
            # Predict
            if (self._metric.__name__ in predict_proba_metrics) and (model_tmp.is_possible_predict_proba()):
                y_pred = model_tmp.predict_proba(X_test)
            else:
                y_pred = model_tmp.predict(X_test)

            score_model = round(self._metric(y_test, y_pred), self._metric_round)
            score_ls.append(score_model)
            iter_time = round((time.time() - start_time),2)
            time_ls.append(iter_time)
            model_tmp = None

        result[&#39;Score&#39;] = score_ls
        result[&#39;Time_Fit_Sec&#39;] = time_ls
        self.result = result
        return(result)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.ModelsReviewClassifier" href="#automl_alex.automl_alex.ModelsReviewClassifier">ModelsReviewClassifier</a></li>
<li><a title="automl_alex.automl_alex.ModelsReviewRegressor" href="#automl_alex.automl_alex.ModelsReviewRegressor">ModelsReviewRegressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.automl_alex.ModelsReview.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X_train: pandas.core.frame.DataFrame, y_train: Union[list, <built-in function array>, pandas.core.frame.DataFrame], X_test: pandas.core.frame.DataFrame, y_test: Union[list, <built-in function array>, pandas.core.frame.DataFrame], models_names: Union[List[str], NoneType] = None, verbose: int = 3) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Fit models from model_list and return scores</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X_train</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>train data (pd.DataFrame, shape (n_samples, n_features))</dd>
<dt><strong><code>y_train</code></strong> :&ensp;<code>Union[list, np.array, pd.DataFrame]</code></dt>
<dd>target</dd>
<dt><strong><code>X_test</code></strong> :&ensp;<code>pd.DataFrame</code></dt>
<dd>test data (pd.DataFrame, shape (n_samples, n_features))</dd>
<dt><strong><code>y_test</code></strong> :&ensp;<code>Union[list, np.array, pd.DataFrame]</code></dt>
<dd>test target</dd>
<dt><strong><code>models_names</code></strong> :&ensp;<code>Optional[List[str]]</code>, optional</dt>
<dd>list of models from automl_alex.models.all_models, by default None</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>print state, by default 3</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pd.DataFrame</code></dt>
<dd>results</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit(self,
    X_train: pd.DataFrame, 
    y_train: Union[list, np.array, pd.DataFrame],
    X_test: pd.DataFrame, 
    y_test: Union[list, np.array, pd.DataFrame],
    models_names: Optional[List[str]] = None,
    verbose: int = 3,
    ) -&gt; pd.DataFrame:
    &#39;&#39;&#39;
    Fit models from model_list and return scores

    Parameters
    ----------
    X_train : pd.DataFrame
        train data (pd.DataFrame, shape (n_samples, n_features))
    y_train : Union[list, np.array, pd.DataFrame]
        target
    X_test : pd.DataFrame
        test data (pd.DataFrame, shape (n_samples, n_features))
    y_test : Union[list, np.array, pd.DataFrame]
        test target
    models_names : Optional[List[str]], optional
        list of models from automl_alex.models.all_models, by default None
    verbose : int, optional
        print state, by default 3

    Returns
    -------
    pd.DataFrame
        results
    &#39;&#39;&#39;        
    logger_print_lvl(verbose)
    result = pd.DataFrame(columns=[&#39;Model_Name&#39;, &#39;Score&#39;, &#39;Time_Fit_Sec&#39;])
    score_ls = []
    time_ls = []
    if models_names is None:
        self.models_names = automl_alex.models.all_models.keys()
    else:
        self.models_names = models_names

    result[&#39;Model_Name&#39;] = self.models_names
    
    if verbose &gt; 0:
        disable_tqdm = False
    else: 
        disable_tqdm = True
    for model_name in tqdm(self.models_names, disable=disable_tqdm):
        # Model
        start_time = time.time()
        model_tmp = automl_alex.models.all_models[model_name](
                                        gpu=self._gpu, 
                                        random_state=self._random_state,
                                        type_of_estimator=self._type_of_estimator)
        # fit
        model_tmp.fit(X_train, y_train)
        # Predict
        if (self._metric.__name__ in predict_proba_metrics) and (model_tmp.is_possible_predict_proba()):
            y_pred = model_tmp.predict_proba(X_test)
        else:
            y_pred = model_tmp.predict(X_test)

        score_model = round(self._metric(y_test, y_pred), self._metric_round)
        score_ls.append(score_model)
        iter_time = round((time.time() - start_time),2)
        time_ls.append(iter_time)
        model_tmp = None

    result[&#39;Score&#39;] = score_ls
    result[&#39;Time_Fit_Sec&#39;] = time_ls
    self.result = result
    return(result)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.automl_alex.ModelsReviewClassifier"><code class="flex name class">
<span>class <span class="ident">ModelsReviewClassifier</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[Callable, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>ModelsReview - allows you to see which models show good results on this data</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from automl_alex import ModelsReviewClassifier, DataPrepare
&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; # Get Dataset
&gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name='adult', version=1, as_frame=True)
&gt;&gt;&gt; dataset.target = dataset.target.astype('category').cat.codes
&gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
&gt;&gt;&gt;                                             dataset.data, 
&gt;&gt;&gt;                                             dataset.target,
&gt;&gt;&gt;                                             test_size=0.2,)
&gt;&gt;&gt; # clean up data before use ModelsReview
&gt;&gt;&gt; de = DataPrepare()
&gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
&gt;&gt;&gt; clean_X_test = de.transform(X_test)
&gt;&gt;&gt; 
&gt;&gt;&gt; model = ModelsReviewClassifier(metric = sklearn.metrics.roc_auc_score,)
&gt;&gt;&gt; review = model.fit(X_train=X_train, 
&gt;&gt;&gt;                     y_train=y_train, 
&gt;&gt;&gt;                     X_test=X_test, 
&gt;&gt;&gt;                     y_test=y_test,)
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type_of_estimator</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>['classifier', 'regression'], by default None</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>you can use standard metrics from sklearn.metrics or add custom metrics.
If None, the metric is selected from the type of estimator:
classifier: sklearn.metrics.roc_auc_score
regression: sklearn.metrics.mean_squared_error.</dd>
<dt><strong><code>metric_round</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>round metric score., by default 4</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use GPU?, by default False</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Controls the generation of the random states for each repetition, by default 42</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelsReviewClassifier(ModelsReview):
    &#39;&#39;&#39;
    ModelsReview - allows you to see which models show good results on this data

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import ModelsReviewClassifier, DataPrepare
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(name=&#39;adult&#39;, version=1, as_frame=True)
    &gt;&gt;&gt; dataset.target = dataset.target.astype(&#39;category&#39;).cat.codes
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             dataset.data, 
    &gt;&gt;&gt;                                             dataset.target,
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; # clean up data before use ModelsReview
    &gt;&gt;&gt; de = DataPrepare()
    &gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
    &gt;&gt;&gt; clean_X_test = de.transform(X_test)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = ModelsReviewClassifier(metric = sklearn.metrics.roc_auc_score,)
    &gt;&gt;&gt; review = model.fit(X_train=X_train, 
    &gt;&gt;&gt;                     y_train=y_train, 
    &gt;&gt;&gt;                     X_test=X_test, 
    &gt;&gt;&gt;                     y_test=y_test,)
    &#39;&#39;&#39;   
    _type_of_estimator=&#39;classifier&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.ModelsReview.fit" href="#automl_alex.automl_alex.ModelsReview.fit">fit</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="automl_alex.automl_alex.ModelsReviewRegressor"><code class="flex name class">
<span>class <span class="ident">ModelsReviewRegressor</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[Callable, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>ModelsReview - allows you to see which models show good results on this data</p>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; from automl_alex import ModelsReviewRegressor, DataPrepare
&gt;&gt;&gt; import sklearn
&gt;&gt;&gt; # Get Dataset
&gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(data_id=543, as_frame=True)
&gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
&gt;&gt;&gt;                                             pd.DataFrame(dataset.data), 
&gt;&gt;&gt;                                             pd.DataFrame(dataset.target), 
&gt;&gt;&gt;                                             test_size=0.2,)
&gt;&gt;&gt; # clean up data before use ModelsReview
&gt;&gt;&gt; de = DataPrepare()
&gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
&gt;&gt;&gt; clean_X_test = de.transform(X_test)
&gt;&gt;&gt; 
&gt;&gt;&gt; model = ModelsReviewRegressor(metric = sklearn.metrics.mean_squared_error,)
&gt;&gt;&gt; review = model.fit(X_train=X_train, 
&gt;&gt;&gt;                     y_train=y_train, 
&gt;&gt;&gt;                     X_test=X_test, 
&gt;&gt;&gt;                     y_test=y_test,)
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>type_of_estimator</code></strong> :&ensp;<code>Optional[str]</code>, optional</dt>
<dd>['classifier', 'regression'], by default None</dd>
<dt><strong><code>metric</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>you can use standard metrics from sklearn.metrics or add custom metrics.
If None, the metric is selected from the type of estimator:
classifier: sklearn.metrics.roc_auc_score
regression: sklearn.metrics.mean_squared_error.</dd>
<dt><strong><code>metric_round</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>round metric score., by default 4</dd>
<dt><strong><code>gpu</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use GPU?, by default False</dd>
<dt><strong><code>random_state</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Controls the generation of the random states for each repetition, by default 42</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelsReviewRegressor(ModelsReview):
    &#39;&#39;&#39;
    ModelsReview - allows you to see which models show good results on this data

    Examples
    --------
    &gt;&gt;&gt; from automl_alex import ModelsReviewRegressor, DataPrepare
    &gt;&gt;&gt; import sklearn
    &gt;&gt;&gt; # Get Dataset
    &gt;&gt;&gt; dataset = sklearn.datasets.fetch_openml(data_id=543, as_frame=True)
    &gt;&gt;&gt; X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.data), 
    &gt;&gt;&gt;                                             pd.DataFrame(dataset.target), 
    &gt;&gt;&gt;                                             test_size=0.2,)
    &gt;&gt;&gt; # clean up data before use ModelsReview
    &gt;&gt;&gt; de = DataPrepare()
    &gt;&gt;&gt; clean_X_train = de.fit_transform(X_train)
    &gt;&gt;&gt; clean_X_test = de.transform(X_test)
    &gt;&gt;&gt; 
    &gt;&gt;&gt; model = ModelsReviewRegressor(metric = sklearn.metrics.mean_squared_error,)
    &gt;&gt;&gt; review = model.fit(X_train=X_train, 
    &gt;&gt;&gt;                     y_train=y_train, 
    &gt;&gt;&gt;                     X_test=X_test, 
    &gt;&gt;&gt;                     y_test=y_test,)
    &#39;&#39;&#39;   
    _type_of_estimator=&#39;regression&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.ModelsReview.fit" href="#automl_alex.automl_alex.ModelsReview.fit">fit</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="automl_alex" href="index.html">automl_alex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.automl_alex.AutoML.fit" href="#automl_alex.automl_alex.AutoML.fit">fit</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.load" href="#automl_alex.automl_alex.AutoML.load">load</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.predict" href="#automl_alex.automl_alex.AutoML.predict">predict</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.save" href="#automl_alex.automl_alex.AutoML.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.AutoMLClassifier" href="#automl_alex.automl_alex.AutoMLClassifier">AutoMLClassifier</a></code></h4>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.AutoMLRegressor" href="#automl_alex.automl_alex.AutoMLRegressor">AutoMLRegressor</a></code></h4>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.automl_alex.ModelsReview.fit" href="#automl_alex.automl_alex.ModelsReview.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.ModelsReviewClassifier" href="#automl_alex.automl_alex.ModelsReviewClassifier">ModelsReviewClassifier</a></code></h4>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.ModelsReviewRegressor" href="#automl_alex.automl_alex.ModelsReviewRegressor">ModelsReviewRegressor</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>