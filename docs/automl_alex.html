<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>automl_alex.automl_alex API documentation</title>
<meta name="description" content="AutoML and other Toolbox" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>automl_alex.automl_alex</code></h1>
</header>
<section id="section-intro">
<p>AutoML and other Toolbox</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;AutoML and other Toolbox&#39;&#39;&#39;

from typing import Any
from typing import Callable
from typing import Dict
from typing import List
from typing import Optional
from typing import Sequence
from typing import Type
from sklearn.metrics import *
from tqdm import tqdm
import pandas as pd
import time
import joblib
import automl_alex
from .models import *
from .cross_validation import *
from .data_prepare import *
from ._encoders import *
from .optimizer import *
from pathlib import Path
from ._logger import *

TMP_FOLDER = &#39;.automl-alex_tmp/&#39;

##################################### BestSingleModel ################################################

# in progress...

##################################### ModelsReview ################################################


class ModelsReview(object):
    &#34;&#34;&#34;
    ModelsReview - allows you to see which models show good results on this data
    &#34;&#34;&#34;
    __name__ = &#39;ModelsReview&#39;

    def __init__(self,  
                    type_of_estimator: Optional[str] = None, # classifier or regression
                    metric: Optional[type] = None,
                    metric_round: int = 4,
                    gpu: bool = False, 
                    random_state: int = 42
                    ) -&gt; None:
        self._gpu = gpu
        self._random_state = random_state
        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is None:
            if self._type_of_estimator == &#39;classifier&#39;:
                self._metric = sklearn.metrics.roc_auc_score
            elif self._type_of_estimator == &#39;regression&#39;:
                self._metric = sklearn.metrics.mean_squared_error
        else:
            self._metric = metric
        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X_train: pd.DataFrame, 
        y_train: Any, 
        X_test: pd.DataFrame, 
        y_test: Any,
        models_names: Optional[Sequence[str]] = None,
        verbose: int = 3,
        ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Fit models (in list models_names) whis default params

        Args:
            X_train: Train Dataset (pd.DataFrame)
            y_train: Target list [list, np.array, pd.DataFrame]
            X_test: Test Dataset (pd.DataFrame)
            y_test: Target list [list, np.array, pd.DataFrame]
            models_names: list of models

        Returns:
            pd.DataFrame with resuls
        &#34;&#34;&#34;
        logger_print_lvl(verbose)
        result = pd.DataFrame(columns=[&#39;Model_Name&#39;, &#39;Score&#39;, &#39;Time_Fit_Sec&#39;])
        score_ls = []
        time_ls = []
        if models_names is None:
            self.models_names = automl_alex.models.all_models.keys()
        else:
            self.models_names = models_names

        result[&#39;Model_Name&#39;] = self.models_names
        
        if verbose &gt; 0:
            disable_tqdm = False
        else: 
            disable_tqdm = True
        for model_name in tqdm(self.models_names, disable=disable_tqdm):
            # Model
            start_time = time.time()
            model_tmp = automl_alex.models.all_models[model_name](
                                            gpu=self._gpu, 
                                            random_state=self._random_state,
                                            type_of_estimator=self._type_of_estimator)
            # fit
            model_tmp.fit(X_train, y_train)
            # Predict
            if (self._metric.__name__ in predict_proba_metrics) and (model_tmp.is_possible_predict_proba()):
                y_pred = model_tmp.predict_proba(X_test)
            else:
                y_pred = model_tmp.predict(X_test)

            score_model = round(self._metric(y_test, y_pred), self._metric_round)
            score_ls.append(score_model)
            iter_time = round((time.time() - start_time),2)
            time_ls.append(iter_time)
            model_tmp = None

        result[&#39;Score&#39;] = score_ls
        result[&#39;Time_Fit_Sec&#39;] = time_ls
        self.result = result
        return(result)

class ModelsReviewClassifier(ModelsReview):
    _type_of_estimator=&#39;classifier&#39;


class ModelsReviewRegressor(ModelsReview):
    _type_of_estimator=&#39;regression&#39;



##################################### Stacking #########################################

# in progress...

##################################### AutoML #########################################

class AutoML(object):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Parameters
    ----------
        type_of_estimator : str
            classifier or regression.

        metric : type
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.

        metric_round : int
            round metric score.

        gpu : bool
            Use GPU?.

        random_state : int
            RandomState instance. 
            Controls the generation of the random states for each repetition.

    Methods
    -------
        fit():
            Fit AutoML

        predict():
            predict on new data

        save():
            save model

        load():
            load model
    &#39;&#39;&#39;
    __name__ = &#39;AutoML&#39;

    def __init__(self,  
                type_of_estimator: Optional[str] = None, # classifier or regression
                metric: Optional[type] = None,
                metric_round: int = 4,
                gpu: bool = False, 
                random_state: int = 42
                ) -&gt; None:
        self._gpu = gpu
        self._random_state = random_state

        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is not None:
            self.metric = metric
        else:
            if self._type_of_estimator == &#39;classifier&#39;:
                self.metric = sklearn.metrics.roc_auc_score
                self.direction = &#39;maximize&#39;
            elif self._type_of_estimator == &#39;regression&#39;:
                self.metric = sklearn.metrics.mean_squared_error
                self.direction = &#39;minimize&#39;

        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X, 
        y, 
        timeout: int = 500, # optimization time in seconds
        auto_parameters: bool = True,
        folds: int = 7,
        score_folds: int = 2,
        opt_lvl: int = 2,
        early_stoping: int = 100,
        feature_selection: bool = True,
        verbose: int = 3,
        ) -&gt; None:
        &#39;&#39;&#39;
        Fit AutoML

        Parameters
        ----------
            X : pd.DataFrame
                Train Dataset

            y : list, np.array, pd.DataFrame
                Target list

            timeout :  int
                Optimization time in seconds.

            opt_lvl : int
                by limiting the optimization time, we will have to choose how deep we should optimize the parameters. 
                Perhaps some parameters are not so important and can only give a fraction of a percent. 
                By setting the opt_lvl parameter, you control the depth of optimization.
                in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration

            early_stoping : int
                stop optimization if no better parameters are found through iterations

            auto_parameters : bool
                If we don&#39;t want to select anything, we just set auto_parameters=True. 
                Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
                    *folds
                    *score_folds
                    *cold_start
                    *opt_lvl

            feature_selection : bool
                add feature_selection in optimization


        Returns
        -------
        None
        &#39;&#39;&#39;
        logger_print_lvl(verbose)
        X_source = X.copy()
        ####################################################
        # STEP 0
        start_step_0 = time.time()
        logger.info(&#39;&gt; Start Fit Base Model&#39;)
        if timeout &lt; 400:
            logger.warning(&#34;! Not enough time to find the optimal parameters. \n \
                    Please, Increase the &#39;timeout&#39; parameter for normal optimization. (min 500 sec)&#34;)

        self.cat_features=X.columns[(X.dtypes == &#39;object&#39;) | (X.dtypes == &#39;category&#39;)]
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        self.model_1 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )
        self.model_1 = self.model_1.fit(X, y, cat_features=self.cat_features.tolist())
        X = None

        ####################################################
        # STEP 1
        start_step_1 = time.time()
        logger.info(&#39;&gt; DATA PREPROC&#39;)
        self.de_1 = automl_alex.DataPrepare(
            cat_encoder_names=[&#39;OneHotEncoder&#39;, &#39;CountEncoder&#39;],
            #outliers_threshold=3,
            normalization=True,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_1.fit_transform(X_source,)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        params = {
                &#39;metric&#39; : self.metric,
                &#39;metric_round&#39; :self._metric_round,
                &#39;auto_parameters&#39;:auto_parameters,
                &#39;folds&#39;:folds,
                &#39;score_folds&#39;:score_folds,
                &#39;opt_lvl&#39;:opt_lvl,
                &#39;early_stoping&#39;:early_stoping,
                &#39;type_of_estimator&#39;:self._type_of_estimator,
                &#39;random_state&#39;:self._random_state,
                &#39;gpu&#39;:self._gpu,
                #&#39;iteration_check&#39;: False,
                }

        # logger.info(50*&#39;#&#39;)
        # logger.info(&#39;&gt; Start Fit Models 2&#39;)
        # logger.info(50*&#39;#&#39;)
        # # Model 2
        # self.model_2 = automl_alex.BestSingleModel(
        #     models_names = [&#39;LinearModel&#39;,],
        #     feature_selection=False,
        #     **params,
        #     )

        # history = self.model_2.opt(X,y, timeout=100, verbose=verbose)
        # self.model_2.save(name=&#39;model_2&#39;, folder=TMP_FOLDER,)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 3&#39;)
        logger.info(50*&#39;#&#39;)
        # Model 3
        self.model_3 = automl_alex.MLP(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            verbose=verbose,
            )
        self.model_3 = self.model_3.fit(X, y)

        X = None

        total_step_1 = (time.time() - start_step_1)

        ####################################################
        # STEP 2
        start_step_2 = time.time()

        logger.info(&#39;&gt; DATA PREPROC&#39;)

        self.de_2 = DataPrepare(
            cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;,&#39;HashingEncoder&#39;],
            #outliers_threshold=3,
            normalization=False,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_2.fit_transform(X_source)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 4&#39;)

        self.model_4 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )

        self.model_4 = self.model_4.fit(X, y)

        total_step_2 = (time.time() - start_step_2)

        ####################################################
        # STEP 3
        # Model 2 - 3
        start_step_3 = time.time()

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 5&#39;)
        logger.info(50*&#39;#&#39;)

        time_to_opt = (timeout - (time.time()-start_step_0)) - 60
        time.sleep(0.1)

        self.model_5 = automl_alex.BestSingleModel(
            models_names = [&#39;LightGBM&#39;, &#39;ExtraTrees&#39;],
            feature_selection=feature_selection,
            **params,
            )

        history = self.model_5.opt(X,y, timeout=time_to_opt, verbose=verbose)
        self.model_5.save(name=&#39;model_5&#39;, folder=TMP_FOLDER,)

        total_step_4 = (time.time() - start_step_3)

        ####################################################
        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Finish!&#39;)


    @logger.catch
    def predict(self, X=None, verbose: int = 0):
        &#34;&#34;&#34;
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        &#34;&#34;&#34;
        if self.model_1 is None:
            raise Exception(&#34;No fit models&#34;)

        X_source = X.copy()
        ####################################################
        # STEP 0
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        # MODEL 1
        self.predict_model_1 = self.model_1.predict_or_predict_proba(X)

        ####################################################
        # STEP 1
        X = self.de_1.transform(X_source, verbose=verbose)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        # MODEL 2
        # self.model_2 = self.model_2.load(name=&#39;model_2&#39;, folder=TMP_FOLDER,)
        # self.predict_model_2 = self.model_2.predict(X)

        # MODEL 3
        self.predict_model_3 = self.model_3.predict_or_predict_proba(X)

        ####################################################
        # STEP 2
        X = self.de_2.transform(X_source, verbose=verbose)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        # MODEL 4
        self.predict_model_4 = self.model_4.predict_or_predict_proba(X)
        
        # MODEL 5
        self.model_5 = self.model_5.load(name=&#39;model_5&#39;, folder=TMP_FOLDER,)
        self.predict_model_5 = self.model_5.predict(X)
        
        ####################################################
        # STEP 4
        # Blend
        predicts = (
            self.predict_model_1
            #+self.predict_model_2
            +self.predict_model_3
            +self.predict_model_4
            +self.predict_model_5)/4
        return (predicts)


    @logger.catch
    def save(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;):
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        self.de_1.save(name=&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        self.de_2.save(name=&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        joblib.dump(self, dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        #self.model_2.save(name=&#39;model_2&#39;, folder=dir_tmp,)
        self.model_5.save(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Save AutoML&#39;)


    @logger.catch
    def load(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;):
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)
        model = joblib.load(dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        model.de_1 = DataPrepare()
        model.de_1 = model.de_1.load(&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        model.de_2 = DataPrepare()
        model.de_2 = model.de_2.load(&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        #model.model_2 = model.model_2.load(name=&#39;model_2&#39;, folder=dir_tmp,)
        model.model_5 = model.model_5.load(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Load AutoML&#39;)
        return(model)


class AutoMLClassifier(AutoML):
    _type_of_estimator=&#39;classifier&#39;
    __name__ = &#39;AutoMLClassifier&#39;

class AutoMLRegressor(AutoML):
    _type_of_estimator=&#39;regression&#39;
    __name__ = &#39;AutoMLRegressor&#39;</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="automl_alex.automl_alex.AutoML"><code class="flex name class">
<span>class <span class="ident">AutoML</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[type, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>AutoML in the process of developing</p>
<h2 id="parameters">Parameters</h2>
<pre><code>type_of_estimator : str
    classifier or regression.

metric : type
    you can use standard metrics from sklearn.metrics or add custom metrics.
    If None, the metric is selected from the type of estimator:
    classifier: sklearn.metrics.roc_auc_score
    regression: sklearn.metrics.mean_squared_error.

metric_round : int
    round metric score.

gpu : bool
    Use GPU?.

random_state : int
    RandomState instance. 
    Controls the generation of the random states for each repetition.
</code></pre>
<h2 id="methods">Methods</h2>
<pre><code>fit():
    Fit AutoML

predict():
    predict on new data

save():
    save model

load():
    load model
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoML(object):
    &#39;&#39;&#39;
    AutoML in the process of developing

    Parameters
    ----------
        type_of_estimator : str
            classifier or regression.

        metric : type
            you can use standard metrics from sklearn.metrics or add custom metrics.
            If None, the metric is selected from the type of estimator:
            classifier: sklearn.metrics.roc_auc_score
            regression: sklearn.metrics.mean_squared_error.

        metric_round : int
            round metric score.

        gpu : bool
            Use GPU?.

        random_state : int
            RandomState instance. 
            Controls the generation of the random states for each repetition.

    Methods
    -------
        fit():
            Fit AutoML

        predict():
            predict on new data

        save():
            save model

        load():
            load model
    &#39;&#39;&#39;
    __name__ = &#39;AutoML&#39;

    def __init__(self,  
                type_of_estimator: Optional[str] = None, # classifier or regression
                metric: Optional[type] = None,
                metric_round: int = 4,
                gpu: bool = False, 
                random_state: int = 42
                ) -&gt; None:
        self._gpu = gpu
        self._random_state = random_state

        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is not None:
            self.metric = metric
        else:
            if self._type_of_estimator == &#39;classifier&#39;:
                self.metric = sklearn.metrics.roc_auc_score
                self.direction = &#39;maximize&#39;
            elif self._type_of_estimator == &#39;regression&#39;:
                self.metric = sklearn.metrics.mean_squared_error
                self.direction = &#39;minimize&#39;

        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X, 
        y, 
        timeout: int = 500, # optimization time in seconds
        auto_parameters: bool = True,
        folds: int = 7,
        score_folds: int = 2,
        opt_lvl: int = 2,
        early_stoping: int = 100,
        feature_selection: bool = True,
        verbose: int = 3,
        ) -&gt; None:
        &#39;&#39;&#39;
        Fit AutoML

        Parameters
        ----------
            X : pd.DataFrame
                Train Dataset

            y : list, np.array, pd.DataFrame
                Target list

            timeout :  int
                Optimization time in seconds.

            opt_lvl : int
                by limiting the optimization time, we will have to choose how deep we should optimize the parameters. 
                Perhaps some parameters are not so important and can only give a fraction of a percent. 
                By setting the opt_lvl parameter, you control the depth of optimization.
                in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration

            early_stoping : int
                stop optimization if no better parameters are found through iterations

            auto_parameters : bool
                If we don&#39;t want to select anything, we just set auto_parameters=True. 
                Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
                    *folds
                    *score_folds
                    *cold_start
                    *opt_lvl

            feature_selection : bool
                add feature_selection in optimization


        Returns
        -------
        None
        &#39;&#39;&#39;
        logger_print_lvl(verbose)
        X_source = X.copy()
        ####################################################
        # STEP 0
        start_step_0 = time.time()
        logger.info(&#39;&gt; Start Fit Base Model&#39;)
        if timeout &lt; 400:
            logger.warning(&#34;! Not enough time to find the optimal parameters. \n \
                    Please, Increase the &#39;timeout&#39; parameter for normal optimization. (min 500 sec)&#34;)

        self.cat_features=X.columns[(X.dtypes == &#39;object&#39;) | (X.dtypes == &#39;category&#39;)]
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        self.model_1 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )
        self.model_1 = self.model_1.fit(X, y, cat_features=self.cat_features.tolist())
        X = None

        ####################################################
        # STEP 1
        start_step_1 = time.time()
        logger.info(&#39;&gt; DATA PREPROC&#39;)
        self.de_1 = automl_alex.DataPrepare(
            cat_encoder_names=[&#39;OneHotEncoder&#39;, &#39;CountEncoder&#39;],
            #outliers_threshold=3,
            normalization=True,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_1.fit_transform(X_source,)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        params = {
                &#39;metric&#39; : self.metric,
                &#39;metric_round&#39; :self._metric_round,
                &#39;auto_parameters&#39;:auto_parameters,
                &#39;folds&#39;:folds,
                &#39;score_folds&#39;:score_folds,
                &#39;opt_lvl&#39;:opt_lvl,
                &#39;early_stoping&#39;:early_stoping,
                &#39;type_of_estimator&#39;:self._type_of_estimator,
                &#39;random_state&#39;:self._random_state,
                &#39;gpu&#39;:self._gpu,
                #&#39;iteration_check&#39;: False,
                }

        # logger.info(50*&#39;#&#39;)
        # logger.info(&#39;&gt; Start Fit Models 2&#39;)
        # logger.info(50*&#39;#&#39;)
        # # Model 2
        # self.model_2 = automl_alex.BestSingleModel(
        #     models_names = [&#39;LinearModel&#39;,],
        #     feature_selection=False,
        #     **params,
        #     )

        # history = self.model_2.opt(X,y, timeout=100, verbose=verbose)
        # self.model_2.save(name=&#39;model_2&#39;, folder=TMP_FOLDER,)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 3&#39;)
        logger.info(50*&#39;#&#39;)
        # Model 3
        self.model_3 = automl_alex.MLP(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            verbose=verbose,
            )
        self.model_3 = self.model_3.fit(X, y)

        X = None

        total_step_1 = (time.time() - start_step_1)

        ####################################################
        # STEP 2
        start_step_2 = time.time()

        logger.info(&#39;&gt; DATA PREPROC&#39;)

        self.de_2 = DataPrepare(
            cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;,&#39;HashingEncoder&#39;],
            #outliers_threshold=3,
            normalization=False,
            random_state=self._random_state, 
            verbose=verbose
            )
        X = self.de_2.fit_transform(X_source)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 4&#39;)

        self.model_4 = automl_alex.CatBoost(
            type_of_estimator=self._type_of_estimator, 
            random_state=self._random_state,
            gpu=self._gpu,
            verbose=verbose,
            )

        self.model_4 = self.model_4.fit(X, y)

        total_step_2 = (time.time() - start_step_2)

        ####################################################
        # STEP 3
        # Model 2 - 3
        start_step_3 = time.time()

        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Start Fit Models 5&#39;)
        logger.info(50*&#39;#&#39;)

        time_to_opt = (timeout - (time.time()-start_step_0)) - 60
        time.sleep(0.1)

        self.model_5 = automl_alex.BestSingleModel(
            models_names = [&#39;LightGBM&#39;, &#39;ExtraTrees&#39;],
            feature_selection=feature_selection,
            **params,
            )

        history = self.model_5.opt(X,y, timeout=time_to_opt, verbose=verbose)
        self.model_5.save(name=&#39;model_5&#39;, folder=TMP_FOLDER,)

        total_step_4 = (time.time() - start_step_3)

        ####################################################
        logger.info(50*&#39;#&#39;)
        logger.info(&#39;&gt; Finish!&#39;)


    @logger.catch
    def predict(self, X=None, verbose: int = 0):
        &#34;&#34;&#34;
        Args:
            X (np.array, shape (n_samples, n_features)): the input data
        Return:
            np.array, shape (n_samples, n_classes)
        &#34;&#34;&#34;
        if self.model_1 is None:
            raise Exception(&#34;No fit models&#34;)

        X_source = X.copy()
        ####################################################
        # STEP 0
        X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
        X.fillna(0, inplace=True)
        X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

        # MODEL 1
        self.predict_model_1 = self.model_1.predict_or_predict_proba(X)

        ####################################################
        # STEP 1
        X = self.de_1.transform(X_source, verbose=verbose)
        if self.de_1.cat_features is not None:
            X = X.drop(self.de_1.cat_features, axis = 1)

        # MODEL 2
        # self.model_2 = self.model_2.load(name=&#39;model_2&#39;, folder=TMP_FOLDER,)
        # self.predict_model_2 = self.model_2.predict(X)

        # MODEL 3
        self.predict_model_3 = self.model_3.predict_or_predict_proba(X)

        ####################################################
        # STEP 2
        X = self.de_2.transform(X_source, verbose=verbose)
        #X = X.drop(self.de_2.cat_features, axis = 1)

        # MODEL 4
        self.predict_model_4 = self.model_4.predict_or_predict_proba(X)
        
        # MODEL 5
        self.model_5 = self.model_5.load(name=&#39;model_5&#39;, folder=TMP_FOLDER,)
        self.predict_model_5 = self.model_5.predict(X)
        
        ####################################################
        # STEP 4
        # Blend
        predicts = (
            self.predict_model_1
            #+self.predict_model_2
            +self.predict_model_3
            +self.predict_model_4
            +self.predict_model_5)/4
        return (predicts)


    @logger.catch
    def save(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;):
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        self.de_1.save(name=&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        self.de_2.save(name=&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        joblib.dump(self, dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        #self.model_2.save(name=&#39;model_2&#39;, folder=dir_tmp,)
        self.model_5.save(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Save AutoML&#39;)


    @logger.catch
    def load(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;):
        dir_tmp = folder+&#34;AutoML_tmp/&#34;
        Path(dir_tmp).mkdir(parents=True, exist_ok=True)
        shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)
        model = joblib.load(dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
        model.de_1 = DataPrepare()
        model.de_1 = model.de_1.load(&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
        model.de_2 = DataPrepare()
        model.de_2 = model.de_2.load(&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
        #model.model_2 = model.model_2.load(name=&#39;model_2&#39;, folder=dir_tmp,)
        model.model_5 = model.model_5.load(name=&#39;model_5&#39;, folder=dir_tmp,)
        shutil.rmtree(dir_tmp)
        logger.info(&#39;Load AutoML&#39;)
        return(model)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.AutoMLClassifier" href="#automl_alex.automl_alex.AutoMLClassifier">AutoMLClassifier</a></li>
<li><a title="automl_alex.automl_alex.AutoMLRegressor" href="#automl_alex.automl_alex.AutoMLRegressor">AutoMLRegressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.automl_alex.AutoML.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X, y, timeout: int = 500, auto_parameters: bool = True, folds: int = 7, score_folds: int = 2, opt_lvl: int = 2, early_stoping: int = 100, feature_selection: bool = True, verbose: int = 3) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Fit AutoML</p>
<h2 id="parameters">Parameters</h2>
<pre><code>X : pd.DataFrame
    Train Dataset

y : list, np.array, pd.DataFrame
    Target list

timeout :  int
    Optimization time in seconds.

opt_lvl : int
    by limiting the optimization time, we will have to choose how deep we should optimize the parameters. 
    Perhaps some parameters are not so important and can only give a fraction of a percent. 
    By setting the opt_lvl parameter, you control the depth of optimization.
    in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration

early_stoping : int
    stop optimization if no better parameters are found through iterations

auto_parameters : bool
    If we don't want to select anything, we just set auto_parameters=True. 
    Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
        *folds
        *score_folds
        *cold_start
        *opt_lvl

feature_selection : bool
    add feature_selection in optimization
</code></pre>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit(self,
    X, 
    y, 
    timeout: int = 500, # optimization time in seconds
    auto_parameters: bool = True,
    folds: int = 7,
    score_folds: int = 2,
    opt_lvl: int = 2,
    early_stoping: int = 100,
    feature_selection: bool = True,
    verbose: int = 3,
    ) -&gt; None:
    &#39;&#39;&#39;
    Fit AutoML

    Parameters
    ----------
        X : pd.DataFrame
            Train Dataset

        y : list, np.array, pd.DataFrame
            Target list

        timeout :  int
            Optimization time in seconds.

        opt_lvl : int
            by limiting the optimization time, we will have to choose how deep we should optimize the parameters. 
            Perhaps some parameters are not so important and can only give a fraction of a percent. 
            By setting the opt_lvl parameter, you control the depth of optimization.
            in the code automl_alex.models.model_lightgbm.LightGBM you can find how parameters are substituted for iteration

        early_stoping : int
            stop optimization if no better parameters are found through iterations

        auto_parameters : bool
            If we don&#39;t want to select anything, we just set auto_parameters=True. 
            Then the algorithm itself will select, depending on the time allotted to it, the optimal values for:
                *folds
                *score_folds
                *cold_start
                *opt_lvl

        feature_selection : bool
            add feature_selection in optimization


    Returns
    -------
    None
    &#39;&#39;&#39;
    logger_print_lvl(verbose)
    X_source = X.copy()
    ####################################################
    # STEP 0
    start_step_0 = time.time()
    logger.info(&#39;&gt; Start Fit Base Model&#39;)
    if timeout &lt; 400:
        logger.warning(&#34;! Not enough time to find the optimal parameters. \n \
                Please, Increase the &#39;timeout&#39; parameter for normal optimization. (min 500 sec)&#34;)

    self.cat_features=X.columns[(X.dtypes == &#39;object&#39;) | (X.dtypes == &#39;category&#39;)]
    X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
    X.fillna(0, inplace=True)
    X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

    self.model_1 = automl_alex.CatBoost(
        type_of_estimator=self._type_of_estimator, 
        random_state=self._random_state,
        gpu=self._gpu,
        verbose=verbose,
        )
    self.model_1 = self.model_1.fit(X, y, cat_features=self.cat_features.tolist())
    X = None

    ####################################################
    # STEP 1
    start_step_1 = time.time()
    logger.info(&#39;&gt; DATA PREPROC&#39;)
    self.de_1 = automl_alex.DataPrepare(
        cat_encoder_names=[&#39;OneHotEncoder&#39;, &#39;CountEncoder&#39;],
        #outliers_threshold=3,
        normalization=True,
        random_state=self._random_state, 
        verbose=verbose
        )
    X = self.de_1.fit_transform(X_source,)
    if self.de_1.cat_features is not None:
        X = X.drop(self.de_1.cat_features, axis = 1)

    params = {
            &#39;metric&#39; : self.metric,
            &#39;metric_round&#39; :self._metric_round,
            &#39;auto_parameters&#39;:auto_parameters,
            &#39;folds&#39;:folds,
            &#39;score_folds&#39;:score_folds,
            &#39;opt_lvl&#39;:opt_lvl,
            &#39;early_stoping&#39;:early_stoping,
            &#39;type_of_estimator&#39;:self._type_of_estimator,
            &#39;random_state&#39;:self._random_state,
            &#39;gpu&#39;:self._gpu,
            #&#39;iteration_check&#39;: False,
            }

    # logger.info(50*&#39;#&#39;)
    # logger.info(&#39;&gt; Start Fit Models 2&#39;)
    # logger.info(50*&#39;#&#39;)
    # # Model 2
    # self.model_2 = automl_alex.BestSingleModel(
    #     models_names = [&#39;LinearModel&#39;,],
    #     feature_selection=False,
    #     **params,
    #     )

    # history = self.model_2.opt(X,y, timeout=100, verbose=verbose)
    # self.model_2.save(name=&#39;model_2&#39;, folder=TMP_FOLDER,)

    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Start Fit Models 3&#39;)
    logger.info(50*&#39;#&#39;)
    # Model 3
    self.model_3 = automl_alex.MLP(
        type_of_estimator=self._type_of_estimator, 
        random_state=self._random_state,
        verbose=verbose,
        )
    self.model_3 = self.model_3.fit(X, y)

    X = None

    total_step_1 = (time.time() - start_step_1)

    ####################################################
    # STEP 2
    start_step_2 = time.time()

    logger.info(&#39;&gt; DATA PREPROC&#39;)

    self.de_2 = DataPrepare(
        cat_encoder_names=[&#39;HelmertEncoder&#39;,&#39;CountEncoder&#39;,&#39;HashingEncoder&#39;],
        #outliers_threshold=3,
        normalization=False,
        random_state=self._random_state, 
        verbose=verbose
        )
    X = self.de_2.fit_transform(X_source)
    #X = X.drop(self.de_2.cat_features, axis = 1)

    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Start Fit Models 4&#39;)

    self.model_4 = automl_alex.CatBoost(
        type_of_estimator=self._type_of_estimator, 
        random_state=self._random_state,
        gpu=self._gpu,
        verbose=verbose,
        )

    self.model_4 = self.model_4.fit(X, y)

    total_step_2 = (time.time() - start_step_2)

    ####################################################
    # STEP 3
    # Model 2 - 3
    start_step_3 = time.time()

    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Start Fit Models 5&#39;)
    logger.info(50*&#39;#&#39;)

    time_to_opt = (timeout - (time.time()-start_step_0)) - 60
    time.sleep(0.1)

    self.model_5 = automl_alex.BestSingleModel(
        models_names = [&#39;LightGBM&#39;, &#39;ExtraTrees&#39;],
        feature_selection=feature_selection,
        **params,
        )

    history = self.model_5.opt(X,y, timeout=time_to_opt, verbose=verbose)
    self.model_5.save(name=&#39;model_5&#39;, folder=TMP_FOLDER,)

    total_step_4 = (time.time() - start_step_3)

    ####################################################
    logger.info(50*&#39;#&#39;)
    logger.info(&#39;&gt; Finish!&#39;)</code></pre>
</details>
</dd>
<dt id="automl_alex.automl_alex.AutoML.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, name: str = 'AutoML_dump', folder: str = './')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def load(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;):
    dir_tmp = folder+&#34;AutoML_tmp/&#34;
    Path(dir_tmp).mkdir(parents=True, exist_ok=True)
    shutil.unpack_archive(folder+name+&#39;.zip&#39;, dir_tmp)
    model = joblib.load(dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
    model.de_1 = DataPrepare()
    model.de_1 = model.de_1.load(&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
    model.de_2 = DataPrepare()
    model.de_2 = model.de_2.load(&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
    #model.model_2 = model.model_2.load(name=&#39;model_2&#39;, folder=dir_tmp,)
    model.model_5 = model.model_5.load(name=&#39;model_5&#39;, folder=dir_tmp,)
    shutil.rmtree(dir_tmp)
    logger.info(&#39;Load AutoML&#39;)
    return(model)</code></pre>
</details>
</dd>
<dt id="automl_alex.automl_alex.AutoML.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self, X=None, verbose: int = 0)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<p>X (np.array, shape (n_samples, n_features)): the input data</p>
<h2 id="return">Return</h2>
<p>np.array, shape (n_samples, n_classes)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def predict(self, X=None, verbose: int = 0):
    &#34;&#34;&#34;
    Args:
        X (np.array, shape (n_samples, n_features)): the input data
    Return:
        np.array, shape (n_samples, n_classes)
    &#34;&#34;&#34;
    if self.model_1 is None:
        raise Exception(&#34;No fit models&#34;)

    X_source = X.copy()
    ####################################################
    # STEP 0
    X[self.cat_features] = X[self.cat_features].astype(&#39;str&#39;)
    X.fillna(0, inplace=True)
    X[self.cat_features] = X[self.cat_features].astype(&#39;category&#39;)

    # MODEL 1
    self.predict_model_1 = self.model_1.predict_or_predict_proba(X)

    ####################################################
    # STEP 1
    X = self.de_1.transform(X_source, verbose=verbose)
    if self.de_1.cat_features is not None:
        X = X.drop(self.de_1.cat_features, axis = 1)

    # MODEL 2
    # self.model_2 = self.model_2.load(name=&#39;model_2&#39;, folder=TMP_FOLDER,)
    # self.predict_model_2 = self.model_2.predict(X)

    # MODEL 3
    self.predict_model_3 = self.model_3.predict_or_predict_proba(X)

    ####################################################
    # STEP 2
    X = self.de_2.transform(X_source, verbose=verbose)
    #X = X.drop(self.de_2.cat_features, axis = 1)

    # MODEL 4
    self.predict_model_4 = self.model_4.predict_or_predict_proba(X)
    
    # MODEL 5
    self.model_5 = self.model_5.load(name=&#39;model_5&#39;, folder=TMP_FOLDER,)
    self.predict_model_5 = self.model_5.predict(X)
    
    ####################################################
    # STEP 4
    # Blend
    predicts = (
        self.predict_model_1
        #+self.predict_model_2
        +self.predict_model_3
        +self.predict_model_4
        +self.predict_model_5)/4
    return (predicts)</code></pre>
</details>
</dd>
<dt id="automl_alex.automl_alex.AutoML.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self, name: str = 'AutoML_dump', folder: str = './')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def save(self, name: str = &#39;AutoML_dump&#39;, folder: str = &#39;./&#39;):
    dir_tmp = folder+&#34;AutoML_tmp/&#34;
    Path(dir_tmp).mkdir(parents=True, exist_ok=True)
    self.de_1.save(name=&#39;DataPrepare_1_dump&#39;, folder=dir_tmp)
    self.de_2.save(name=&#39;DataPrepare_2_dump&#39;, folder=dir_tmp)
    joblib.dump(self, dir_tmp+&#39;AutoML&#39;+&#39;.pkl&#39;)
    #self.model_2.save(name=&#39;model_2&#39;, folder=dir_tmp,)
    self.model_5.save(name=&#39;model_5&#39;, folder=dir_tmp,)
    shutil.make_archive(folder+name, &#39;zip&#39;, dir_tmp)
    shutil.rmtree(dir_tmp)
    logger.info(&#39;Save AutoML&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.automl_alex.AutoMLClassifier"><code class="flex name class">
<span>class <span class="ident">AutoMLClassifier</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[type, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>AutoML in the process of developing</p>
<h2 id="parameters">Parameters</h2>
<pre><code>type_of_estimator : str
    classifier or regression.

metric : type
    you can use standard metrics from sklearn.metrics or add custom metrics.
    If None, the metric is selected from the type of estimator:
    classifier: sklearn.metrics.roc_auc_score
    regression: sklearn.metrics.mean_squared_error.

metric_round : int
    round metric score.

gpu : bool
    Use GPU?.

random_state : int
    RandomState instance. 
    Controls the generation of the random states for each repetition.
</code></pre>
<h2 id="methods">Methods</h2>
<pre><code>fit():
    Fit AutoML

predict():
    predict on new data

save():
    save model

load():
    load model
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoMLClassifier(AutoML):
    _type_of_estimator=&#39;classifier&#39;
    __name__ = &#39;AutoMLClassifier&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.AutoML.fit" href="#automl_alex.automl_alex.AutoML.fit">fit</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.predict" href="#automl_alex.automl_alex.AutoML.predict">predict</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="automl_alex.automl_alex.AutoMLRegressor"><code class="flex name class">
<span>class <span class="ident">AutoMLRegressor</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[type, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>AutoML in the process of developing</p>
<h2 id="parameters">Parameters</h2>
<pre><code>type_of_estimator : str
    classifier or regression.

metric : type
    you can use standard metrics from sklearn.metrics or add custom metrics.
    If None, the metric is selected from the type of estimator:
    classifier: sklearn.metrics.roc_auc_score
    regression: sklearn.metrics.mean_squared_error.

metric_round : int
    round metric score.

gpu : bool
    Use GPU?.

random_state : int
    RandomState instance. 
    Controls the generation of the random states for each repetition.
</code></pre>
<h2 id="methods">Methods</h2>
<pre><code>fit():
    Fit AutoML

predict():
    predict on new data

save():
    save model

load():
    load model
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoMLRegressor(AutoML):
    _type_of_estimator=&#39;regression&#39;
    __name__ = &#39;AutoMLRegressor&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.AutoML.fit" href="#automl_alex.automl_alex.AutoML.fit">fit</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.predict" href="#automl_alex.automl_alex.AutoML.predict">predict</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="automl_alex.automl_alex.ModelsReview"><code class="flex name class">
<span>class <span class="ident">ModelsReview</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[type, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>ModelsReview - allows you to see which models show good results on this data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelsReview(object):
    &#34;&#34;&#34;
    ModelsReview - allows you to see which models show good results on this data
    &#34;&#34;&#34;
    __name__ = &#39;ModelsReview&#39;

    def __init__(self,  
                    type_of_estimator: Optional[str] = None, # classifier or regression
                    metric: Optional[type] = None,
                    metric_round: int = 4,
                    gpu: bool = False, 
                    random_state: int = 42
                    ) -&gt; None:
        self._gpu = gpu
        self._random_state = random_state
        if type_of_estimator is not None:
            self._type_of_estimator = type_of_estimator

        if metric is None:
            if self._type_of_estimator == &#39;classifier&#39;:
                self._metric = sklearn.metrics.roc_auc_score
            elif self._type_of_estimator == &#39;regression&#39;:
                self._metric = sklearn.metrics.mean_squared_error
        else:
            self._metric = metric
        self._metric_round = metric_round


    @logger.catch
    def fit(self,
        X_train: pd.DataFrame, 
        y_train: Any, 
        X_test: pd.DataFrame, 
        y_test: Any,
        models_names: Optional[Sequence[str]] = None,
        verbose: int = 3,
        ) -&gt; pd.DataFrame:
        &#34;&#34;&#34;
        Fit models (in list models_names) whis default params

        Args:
            X_train: Train Dataset (pd.DataFrame)
            y_train: Target list [list, np.array, pd.DataFrame]
            X_test: Test Dataset (pd.DataFrame)
            y_test: Target list [list, np.array, pd.DataFrame]
            models_names: list of models

        Returns:
            pd.DataFrame with resuls
        &#34;&#34;&#34;
        logger_print_lvl(verbose)
        result = pd.DataFrame(columns=[&#39;Model_Name&#39;, &#39;Score&#39;, &#39;Time_Fit_Sec&#39;])
        score_ls = []
        time_ls = []
        if models_names is None:
            self.models_names = automl_alex.models.all_models.keys()
        else:
            self.models_names = models_names

        result[&#39;Model_Name&#39;] = self.models_names
        
        if verbose &gt; 0:
            disable_tqdm = False
        else: 
            disable_tqdm = True
        for model_name in tqdm(self.models_names, disable=disable_tqdm):
            # Model
            start_time = time.time()
            model_tmp = automl_alex.models.all_models[model_name](
                                            gpu=self._gpu, 
                                            random_state=self._random_state,
                                            type_of_estimator=self._type_of_estimator)
            # fit
            model_tmp.fit(X_train, y_train)
            # Predict
            if (self._metric.__name__ in predict_proba_metrics) and (model_tmp.is_possible_predict_proba()):
                y_pred = model_tmp.predict_proba(X_test)
            else:
                y_pred = model_tmp.predict(X_test)

            score_model = round(self._metric(y_test, y_pred), self._metric_round)
            score_ls.append(score_model)
            iter_time = round((time.time() - start_time),2)
            time_ls.append(iter_time)
            model_tmp = None

        result[&#39;Score&#39;] = score_ls
        result[&#39;Time_Fit_Sec&#39;] = time_ls
        self.result = result
        return(result)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.ModelsReviewClassifier" href="#automl_alex.automl_alex.ModelsReviewClassifier">ModelsReviewClassifier</a></li>
<li><a title="automl_alex.automl_alex.ModelsReviewRegressor" href="#automl_alex.automl_alex.ModelsReviewRegressor">ModelsReviewRegressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="automl_alex.automl_alex.ModelsReview.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, X_train: pandas.core.frame.DataFrame, y_train: Any, X_test: pandas.core.frame.DataFrame, y_test: Any, models_names: Union[Sequence[str], NoneType] = None, verbose: int = 3) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"><p>Fit models (in list models_names) whis default params</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X_train</code></strong></dt>
<dd>Train Dataset (pd.DataFrame)</dd>
<dt><strong><code>y_train</code></strong></dt>
<dd>Target list [list, np.array, pd.DataFrame]</dd>
<dt><strong><code>X_test</code></strong></dt>
<dd>Test Dataset (pd.DataFrame)</dd>
<dt><strong><code>y_test</code></strong></dt>
<dd>Target list [list, np.array, pd.DataFrame]</dd>
<dt><strong><code>models_names</code></strong></dt>
<dd>list of models</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>pd.DataFrame with resuls</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@logger.catch
def fit(self,
    X_train: pd.DataFrame, 
    y_train: Any, 
    X_test: pd.DataFrame, 
    y_test: Any,
    models_names: Optional[Sequence[str]] = None,
    verbose: int = 3,
    ) -&gt; pd.DataFrame:
    &#34;&#34;&#34;
    Fit models (in list models_names) whis default params

    Args:
        X_train: Train Dataset (pd.DataFrame)
        y_train: Target list [list, np.array, pd.DataFrame]
        X_test: Test Dataset (pd.DataFrame)
        y_test: Target list [list, np.array, pd.DataFrame]
        models_names: list of models

    Returns:
        pd.DataFrame with resuls
    &#34;&#34;&#34;
    logger_print_lvl(verbose)
    result = pd.DataFrame(columns=[&#39;Model_Name&#39;, &#39;Score&#39;, &#39;Time_Fit_Sec&#39;])
    score_ls = []
    time_ls = []
    if models_names is None:
        self.models_names = automl_alex.models.all_models.keys()
    else:
        self.models_names = models_names

    result[&#39;Model_Name&#39;] = self.models_names
    
    if verbose &gt; 0:
        disable_tqdm = False
    else: 
        disable_tqdm = True
    for model_name in tqdm(self.models_names, disable=disable_tqdm):
        # Model
        start_time = time.time()
        model_tmp = automl_alex.models.all_models[model_name](
                                        gpu=self._gpu, 
                                        random_state=self._random_state,
                                        type_of_estimator=self._type_of_estimator)
        # fit
        model_tmp.fit(X_train, y_train)
        # Predict
        if (self._metric.__name__ in predict_proba_metrics) and (model_tmp.is_possible_predict_proba()):
            y_pred = model_tmp.predict_proba(X_test)
        else:
            y_pred = model_tmp.predict(X_test)

        score_model = round(self._metric(y_test, y_pred), self._metric_round)
        score_ls.append(score_model)
        iter_time = round((time.time() - start_time),2)
        time_ls.append(iter_time)
        model_tmp = None

    result[&#39;Score&#39;] = score_ls
    result[&#39;Time_Fit_Sec&#39;] = time_ls
    self.result = result
    return(result)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="automl_alex.automl_alex.ModelsReviewClassifier"><code class="flex name class">
<span>class <span class="ident">ModelsReviewClassifier</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[type, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>ModelsReview - allows you to see which models show good results on this data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelsReviewClassifier(ModelsReview):
    _type_of_estimator=&#39;classifier&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.ModelsReview.fit" href="#automl_alex.automl_alex.ModelsReview.fit">fit</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="automl_alex.automl_alex.ModelsReviewRegressor"><code class="flex name class">
<span>class <span class="ident">ModelsReviewRegressor</span></span>
<span>(</span><span>type_of_estimator: Union[str, NoneType] = None, metric: Union[type, NoneType] = None, metric_round: int = 4, gpu: bool = False, random_state: int = 42)</span>
</code></dt>
<dd>
<div class="desc"><p>ModelsReview - allows you to see which models show good results on this data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelsReviewRegressor(ModelsReview):
    _type_of_estimator=&#39;regression&#39;</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></b></code>:
<ul class="hlist">
<li><code><a title="automl_alex.automl_alex.ModelsReview.fit" href="#automl_alex.automl_alex.ModelsReview.fit">fit</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="automl_alex" href="index.html">automl_alex</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="automl_alex.automl_alex.AutoML" href="#automl_alex.automl_alex.AutoML">AutoML</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.automl_alex.AutoML.fit" href="#automl_alex.automl_alex.AutoML.fit">fit</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.load" href="#automl_alex.automl_alex.AutoML.load">load</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.predict" href="#automl_alex.automl_alex.AutoML.predict">predict</a></code></li>
<li><code><a title="automl_alex.automl_alex.AutoML.save" href="#automl_alex.automl_alex.AutoML.save">save</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.AutoMLClassifier" href="#automl_alex.automl_alex.AutoMLClassifier">AutoMLClassifier</a></code></h4>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.AutoMLRegressor" href="#automl_alex.automl_alex.AutoMLRegressor">AutoMLRegressor</a></code></h4>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.ModelsReview" href="#automl_alex.automl_alex.ModelsReview">ModelsReview</a></code></h4>
<ul class="">
<li><code><a title="automl_alex.automl_alex.ModelsReview.fit" href="#automl_alex.automl_alex.ModelsReview.fit">fit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.ModelsReviewClassifier" href="#automl_alex.automl_alex.ModelsReviewClassifier">ModelsReviewClassifier</a></code></h4>
</li>
<li>
<h4><code><a title="automl_alex.automl_alex.ModelsReviewRegressor" href="#automl_alex.automl_alex.ModelsReviewRegressor">ModelsReviewRegressor</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>